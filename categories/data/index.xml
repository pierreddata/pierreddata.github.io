<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Data on Le D de Data</title>
        <link>https://blog.ddata.fr/categories/data/</link>
        <description>Recent content in Data on Le D de Data</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>fr-fr</language>
        <lastBuildDate>Sun, 09 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.ddata.fr/categories/data/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Projet data : les principales transformations des données</title>
        <link>https://blog.ddata.fr/p/datalake-transformation/</link>
        <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-transformation/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-transformation/katrin-hauf-sD-WFeC8WEs-unsplash.jpg" alt="Featured image of post Projet data : les principales transformations des données" /&gt;&lt;p&gt;Dans cet article, je vous présente les principales transformations des données que vous ferez dans un projet data.&lt;/p&gt;
&lt;h2 id=&#34;contexte&#34;&gt;Contexte&lt;/h2&gt;
&lt;p&gt;Lorsque vous traitez les données d&amp;rsquo;un projet data vous allez faire une succession de transformation pour obtenir votre résultat final.&lt;br&gt;
Par exemple la chaine de traitement dans le cadre d&amp;rsquo;un projet datawarehouse est la suivante :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/001-datalake-transformation.png&#34;
	width=&#34;741&#34;
	height=&#34;388&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/001-datalake-transformation_hu95166b86ce920fe6fdef2e75a6837833_25021_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/001-datalake-transformation_hu95166b86ce920fe6fdef2e75a6837833_25021_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Chaine de transformation typique d&amp;rsquo;un datalakehouse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;458px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;On retrouve un ensemble de traitement permettant la transformation des données d&amp;rsquo;une zone à l&amp;rsquo;autre :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dans la zone bronze, un maelstrom de données brut.&lt;/li&gt;
&lt;li&gt;Dans la zone silver, des données préparées sous forme de table structurée.&lt;/li&gt;
&lt;li&gt;Dans la zone gold, des données à forte valeur ajoutée comme par exemple des tables modéliser en étoile pour de reporting avec potentiellement des données pré agrégé.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mais, quels que soient les outils que vous utiliserez pour votre projet, vous utiliserez presque toujours les mêmes transformations de données.&lt;br&gt;
Je vous propose de passer en revue ces transformations.&lt;/p&gt;
&lt;h2 id=&#34;structuration-des-données&#34;&gt;Structuration des données&lt;/h2&gt;
&lt;p&gt;La phase de structuration des données va vous permettre de transformer vos données en données structurées sous forme de table.&lt;/p&gt;
&lt;h3 id=&#34;la-table&#34;&gt;La table&lt;/h3&gt;
&lt;p&gt;La table est la structure de données que vous allez utiliser pour toutes vos transformations de données.&lt;br&gt;
Une table a les caractéristiques suivantes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les données sont stockées en ligne&lt;/li&gt;
&lt;li&gt;Chaque colonne d&amp;rsquo;une table contient des valeurs cohérentes
&lt;ul&gt;
&lt;li&gt;De nature : une colonne contient des données métier de même nature. Par exemple dans une colonne contenant le nom d&amp;rsquo;une personne on ne met pas sa date de naissance.&lt;/li&gt;
&lt;li&gt;De type : une colonne ne contient que des valeurs ayant le même type ou pas de valeur. Par exemple une colonne de date ne contient que des dates.&lt;/li&gt;
&lt;li&gt;De format : une colonne de date ne contient que des dates au même format, par exemple on ne stocke pas le 31 décembre 2022 sous ces 2 formes dans la même colonne :
&lt;ul&gt;
&lt;li&gt;31/12/2022&lt;/li&gt;
&lt;li&gt;2022-12-31&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/002-table.png&#34;
	width=&#34;952&#34;
	height=&#34;265&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/002-table_hud391b07d078a580a6feb81061bc5f572_17212_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/002-table_hud391b07d078a580a6feb81061bc5f572_17212_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Structure d&amp;rsquo;une table&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;359&#34;
		data-flex-basis=&#34;862px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;nommage-des-colonnes&#34;&gt;Nommage des colonnes&lt;/h3&gt;
&lt;p&gt;Le nommage des colonnes d&amp;rsquo;une table est une phase très importante dans un projet BI. En général vous avez des données dont le nom n&amp;rsquo;est pas adapté à la création d&amp;rsquo;un modèle métier. La phase de renommage des colonnes d&amp;rsquo;une table vous permet d&amp;rsquo;apporter cette cohérence métier.&lt;br&gt;
Prenez grand soin à avoir un nommage cohérent et d&amp;rsquo;utiliser le bon vocabulaire du modèle métier cible, en effet si votre modèle de données doit être utilisé par des personnes d&amp;rsquo;un métier parlé leur langage et masquer tous les éléments techniques inutiles.&lt;/p&gt;
&lt;h3 id=&#34;typage-de-données&#34;&gt;Typage de données&lt;/h3&gt;
&lt;p&gt;Le typage des données permet de conserver la cohérence d&amp;rsquo;une table.  Parmi les types de données disponibles dans votre outil, choisissez toujours des types de données vous apportant une forte garantie sur le contenu de la colonne.&lt;br&gt;
Par exemple, si vous stockez vos dates dans une colonne stockant du texte, vous n&amp;rsquo;aurez pas la garantie d&amp;rsquo;avoir des dates dedans.&lt;br&gt;
Les principaux types de données que l&amp;rsquo;on utilise sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le texte ou chaine de caractères&lt;/li&gt;
&lt;li&gt;Les nombres
&lt;ul&gt;
&lt;li&gt;Entier&lt;/li&gt;
&lt;li&gt;Décimaux&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Le temps
&lt;ul&gt;
&lt;li&gt;Date&lt;/li&gt;
&lt;li&gt;Date/Heure&lt;/li&gt;
&lt;li&gt;Date/Heure/Fuseau horaire&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Les booléens (vrai/faux)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Attention, chaque type de données a ses propres pièges et on rencontre souvent les mêmes problèmes. Par exemple :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les formats de date ou de nombre décimaux différents entre les systèmes français et américain&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;encodage du texte, si les termes ASCII, UTF-8 ou UNICODE ne vous sont pas familiers vous risquez des ennuis lors du traitement de données avec des caractères spéciaux.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sélection-des-colonnes&#34;&gt;Sélection des colonnes&lt;/h3&gt;
&lt;p&gt;Ne sélectionnez que les colonnes utiles d&amp;rsquo;un point de vue métier ou technique à votre modèle de données. Il est en général plus simple d&amp;rsquo;ajouter une nouvelle colonne à un modèle existant pour répondre à un nouveau besoin que de créer un modèle cohérent avec un grand nombre de colonnes inutile.&lt;/p&gt;
&lt;h2 id=&#34;préparation-des-données&#34;&gt;Préparation des données&lt;/h2&gt;
&lt;p&gt;Nous allons maintenant passer en revue les principales transformations que vous ferez sur vos données lors d&amp;rsquo;un projet data.&lt;/p&gt;
&lt;h3 id=&#34;filtrage-des-données&#34;&gt;Filtrage des données&lt;/h3&gt;
&lt;p&gt;Un filtre permet de sélectionner les lignes que l&amp;rsquo;on souhaite conserver en fonction de critères prédéfinis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/003-filter.png&#34;
	width=&#34;1043&#34;
	height=&#34;254&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/003-filter_hu295087bdfaebbb443fd5fd45d8b80859_25711_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/003-filter_hu295087bdfaebbb443fd5fd45d8b80859_25711_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple d&amp;rsquo;un filtre&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;410&#34;
		data-flex-basis=&#34;985px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;remplacer-des-données&#34;&gt;Remplacer des données&lt;/h3&gt;
&lt;p&gt;Le remplacement de données permet de remplacer une valeur par une autre valeur.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/004-replace.png&#34;
	width=&#34;1048&#34;
	height=&#34;258&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/004-replace_hub2f149565f883ffa61f4566d7d21573f_28594_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/004-replace_hub2f149565f883ffa61f4566d7d21573f_28594_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple d&amp;rsquo;un remplacement&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;406&#34;
		data-flex-basis=&#34;974px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Attention, certains outils sont sensibles à la casse, il différencie minuscules et majuscules. Par exemple, &lt;em&gt;rouge&lt;/em&gt; sera différent de &lt;em&gt;Rouge&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;tri-des-données&#34;&gt;Tri des données&lt;/h3&gt;
&lt;p&gt;Le tri des données permet d&amp;rsquo;imposer l&amp;rsquo;ordre des lignes les unes par rapport aux autres. Le tri peut être ascendant ou descendant et peu porter sur plusieurs colonnes afin de forcer le tri des lignes sur des sous-critères.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/005-sort.png&#34;
	width=&#34;1040&#34;
	height=&#34;248&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/005-sort_hue1dd40990003dd15ac4aa7fd74bbc98c_26701_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/005-sort_hue1dd40990003dd15ac4aa7fd74bbc98c_26701_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de tri&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;419&#34;
		data-flex-basis=&#34;1006px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Attention, tant que vous n&amp;rsquo;avez pas trié vos données ne préjugé pas du tri de celle-ci.&lt;/p&gt;
&lt;h3 id=&#34;partitionnement-des-données&#34;&gt;Partitionnement des données&lt;/h3&gt;
&lt;p&gt;Le partitionnement des données permet de séparer physiquement vos données. On utilise cette technique sur les gros volumes de données en général pour améliorer les performances.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/006-partition.png&#34;
	width=&#34;1061&#34;
	height=&#34;339&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/006-partition_hue74bb6e30d0e32723710751280f8827c_34954_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/006-partition_hue74bb6e30d0e32723710751280f8827c_34954_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de partitionnement&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;312&#34;
		data-flex-basis=&#34;751px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pivoter-dépivoter-et-transposer&#34;&gt;Pivoter Dépivoter et Transposer&lt;/h3&gt;
&lt;p&gt;Ces 3 opérations permettent de faire des transformations sur la structure de la table. Elles sont souvent utilisées pour transformer des données formatées pour la saisie utilisateur dans Excel en données sous la forme d&amp;rsquo;une table utilisable.&lt;/p&gt;
&lt;h4 id=&#34;pivoter&#34;&gt;Pivoter&lt;/h4&gt;
&lt;p&gt;Permets de basculer des valeurs stockées dans les lignes d&amp;rsquo;une colonne en valeur stockée en colonne. Une opération d&amp;rsquo;agrégation de données peut aussi être réalisée lors du pivot pour regrouper les valeurs sources identiques en une seule valeur cible.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/007-pivot.png&#34;
	width=&#34;1166&#34;
	height=&#34;365&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/007-pivot_hu57a42e3ab360df31f6b3c7ec0a581137_43302_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/007-pivot_hu57a42e3ab360df31f6b3c7ec0a581137_43302_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de pivot&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;319&#34;
		data-flex-basis=&#34;766px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;dépivoter&#34;&gt;Dépivoter&lt;/h4&gt;
&lt;p&gt;Permets de basculer des valeurs stockées dans les noms de colonne d&amp;rsquo;une table en valeur stockée en ligne. C&amp;rsquo;est l&amp;rsquo;opération inverse de pivoter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/008-unpivot.png&#34;
	width=&#34;957&#34;
	height=&#34;359&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/008-unpivot_hu9741ec229875aad0855941f7d9632802_30130_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/008-unpivot_hu9741ec229875aad0855941f7d9632802_30130_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de dépivot&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;266&#34;
		data-flex-basis=&#34;639px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;transposer&#34;&gt;Transposer&lt;/h4&gt;
&lt;p&gt;Permets de réaliser une rotation à 90° de votre table.&lt;br&gt;
Si vous transposez une table déjà transposée vous retrouvez la table initiale.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/009-transpose.png&#34;
	width=&#34;1030&#34;
	height=&#34;172&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/009-transpose_hu1e912de131ba385d75e1589577f9ed95_20258_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/009-transpose_hu1e912de131ba385d75e1589577f9ed95_20258_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de transpose&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;598&#34;
		data-flex-basis=&#34;1437px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;créer-des-colonnes&#34;&gt;Créer des colonnes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Par fractionnement, vous divisez une colonne en plusieurs autres colonnes à l&amp;rsquo;aide d&amp;rsquo;un séparateur.&lt;/li&gt;
&lt;li&gt;Par calcul, vous créer une colonne en vous basant sur des informations en provenance d&amp;rsquo;autres colonnes de la même ligne.&lt;/li&gt;
&lt;li&gt;Par extraction d&amp;rsquo;un terme, vous créer une colonne en extrayant un sous-ensemble d&amp;rsquo;information d&amp;rsquo;une colonne.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;les-jointures&#34;&gt;Les jointures&lt;/h3&gt;
&lt;p&gt;Les jointures sont une transformation de données très puissantes. Elles peuvent avoir plusieurs usages, notamment :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L&amp;rsquo;enrichissement d&amp;rsquo;une table via une autre table.&lt;/li&gt;
&lt;li&gt;La suppression de ligne d&amp;rsquo;une table par une autre table.&lt;/li&gt;
&lt;li&gt;La création de ligne dans une table.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;jointure-externe-gauche&#34;&gt;Jointure externe gauche&lt;/h4&gt;
&lt;p&gt;La jointure externe gauche permet d&amp;rsquo;intégrer dans la table de gauche toutes les valeurs de la table de droite correspondant à la relation de jointure.&lt;br&gt;
Avec ce tpe de jointure on à la garantie de conserver TOUTES les lignes de la table de gauche.&lt;br&gt;
Par contre s&amp;rsquo;il existe plusieurs occurrences d&amp;rsquo;une même valeur dans la table de droite pour la relation, cela va créer des lignes dans la table de gauche.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/010-left-join.png&#34;
	width=&#34;902&#34;
	height=&#34;754&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/010-left-join_hu0d32a9d40bfd9a804900732601d4a70a_60783_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/010-left-join_hu0d32a9d40bfd9a804900732601d4a70a_60783_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de jointure externe gauche&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;287px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;La valeur &lt;em&gt;null&lt;/em&gt; représente par convention une absence de valeur dans la table de droite.&lt;/p&gt;
&lt;h4 id=&#34;jointure-externe-droite&#34;&gt;Jointure externe droite&lt;/h4&gt;
&lt;p&gt;La jointure externe droite permet de filtrer les lignes de table de gauche à l&amp;rsquo;aide des données de la table de droite.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/011-right-join.png&#34;
	width=&#34;877&#34;
	height=&#34;618&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/011-right-join_hue6467861ea40fd427e5cbb792ee7e795_47893_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/011-right-join_hue6467861ea40fd427e5cbb792ee7e795_47893_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de jointure externe droite&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;jointure-externe-entière&#34;&gt;Jointure externe entière&lt;/h4&gt;
&lt;p&gt;La jointure externe entière permet de conserver toutes les lignes des 2 tables.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/012-outer-join.png&#34;
	width=&#34;899&#34;
	height=&#34;640&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/012-outer-join_hucea240b1dd0705a3ca2860825525de0a_51336_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/012-outer-join_hucea240b1dd0705a3ca2860825525de0a_51336_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de jointure externe entière&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;337px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;jointure-interne&#34;&gt;Jointure interne&lt;/h4&gt;
&lt;p&gt;La jointure interne permet de ne conserver que les lignes appartenant aux 2 tables.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/013-inner-join.png&#34;
	width=&#34;842&#34;
	height=&#34;704&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/013-inner-join_hubb93a21d63a59400ebb47ad774e75d44_55204_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/013-inner-join_hubb93a21d63a59400ebb47ad774e75d44_55204_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de jointure interne&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;287px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;lunion&#34;&gt;L&amp;rsquo;union&lt;/h3&gt;
&lt;p&gt;L&amp;rsquo;union permet de cumuler les lignes de 2 tables ou plus en une seule table. Suivant les outils les tables doivent avoir la même structure de colonnes ou non.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/014-union.png&#34;
	width=&#34;1230&#34;
	height=&#34;387&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/014-union_hu08b8b2936bcec6aec09cf50f2e96f724_39688_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/014-union_hu08b8b2936bcec6aec09cf50f2e96f724_39688_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple d&amp;rsquo;union de 2 tables&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;317&#34;
		data-flex-basis=&#34;762px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;le-dédoublonnage&#34;&gt;Le dédoublonnage&lt;/h3&gt;
&lt;p&gt;Le dédoublonnage est une opération qui permet de supprimer les lignes identiques. On à deux types d&amp;rsquo;opération de dédoublonnage :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dédoublonage pur pour les lignes complètement identiques.&lt;/li&gt;
&lt;li&gt;Dédoublonage partiel, sur un critère choisi d&amp;rsquo;une ou plusieurs colonnes, dans ce cas il faut faire attention à quelle valeur est conservé.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/015-remove-duplicate.png&#34;
	width=&#34;961&#34;
	height=&#34;290&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/015-remove-duplicate_hu242f1629717d92cf66906aeffe83af4e_27840_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/015-remove-duplicate_hu242f1629717d92cf66906aeffe83af4e_27840_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de dédoublonnage&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;331&#34;
		data-flex-basis=&#34;795px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;agrégation&#34;&gt;Agrégation&lt;/h3&gt;
&lt;p&gt;L&amp;rsquo;agrégation est une opération qui permet de réduire le nombre de lignes d&amp;rsquo;une table. On supprime certaines colonnes, et en perdant ce détail, on peut regrouper les lignes en faisant une opération mathématique sur les colonnes numériques, en général une somme.&lt;br&gt;
Les opérations d&amp;rsquo;agrégations classiques sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Somme&lt;/li&gt;
&lt;li&gt;Minimum&lt;/li&gt;
&lt;li&gt;Maximum&lt;/li&gt;
&lt;li&gt;Moyenne&lt;/li&gt;
&lt;li&gt;Écart type&lt;/li&gt;
&lt;li&gt;Compter le nombre de lignes agrégé&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-transformation/img/016-aggregate.png&#34;
	width=&#34;948&#34;
	height=&#34;276&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-transformation/img/016-aggregate_hu6266293f44db3af5f37240e3d8650a9a_23713_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-transformation/img/016-aggregate_hu6266293f44db3af5f37240e3d8650a9a_23713_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple d&amp;rsquo;agrégation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;343&#34;
		data-flex-basis=&#34;824px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;opération d&amp;rsquo;agrégation est une opération qui modifie la granularité de la table. La table résultante a un niveau de détail moins important que la table source, on perd donc de l&amp;rsquo;information.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datalake : les principaux formats de fichiers</title>
        <link>https://blog.ddata.fr/p/datalake-file-format/</link>
        <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-file-format/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-file-format/david-bruno-silva-Z19vToWBDIc-unsplash.jpg" alt="Featured image of post Datalake : les principaux formats de fichiers" /&gt;&lt;p&gt;Dans cet article je vous présente les principaux formats de fichier que l&amp;rsquo;on trouve dans un datalake.&lt;/p&gt;
&lt;h2 id=&#34;contexte&#34;&gt;Contexte&lt;/h2&gt;
&lt;p&gt;Lorsque vous travaillez avec un datalake vous allez manipuler un certain nombre de formats de fichier. Je vous présente ici ceux que vous rencontrerez le plus souvent.&lt;/p&gt;
&lt;p&gt;Vous trouverez dans mes articles précédents les explications concernant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les architectures de type datalakehouse : &lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/&#34; &gt;Datawarehouse ou datalakehouse, quelle source pour vos reportings ?&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Les datalakes :&lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/datalake-la-base/&#34; &gt;Datalake : la base d&amp;rsquo;un projet data moderne&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quest-ce-quun-fichier-&#34;&gt;Qu&amp;rsquo;est-ce qu&amp;rsquo;un fichier ?&lt;/h2&gt;
&lt;p&gt;La question peut sembler triviale pour certain j&amp;rsquo;ai rencontré suffisamment de personnes pour qui cela n&amp;rsquo;était pas clair pour prendre le temps de l&amp;rsquo;expliquer.&lt;br&gt;
En informatique on utilise des fichiers pour stocker de l&amp;rsquo;information et des dossiers pour ranger nos fichiers dans des structures hiérarchiques.&lt;br&gt;
Un fichier peut contenir tous types d&amp;rsquo;informations, du code binaire de vos programmes aux données des documents que vous réalisez avec ces programmes.&lt;br&gt;
On appelle un format de fichier l&amp;rsquo;extension du fichier.&lt;br&gt;
L&amp;rsquo;extension du fichier est simplement la suite de caractères se trouvant après le dernier point du nom complet du fichier. Les extensions sont souvent masquées dans l&amp;rsquo;explorateur de fichier de Windows.&lt;br&gt;
Par exemple, &lt;em&gt;MonFichier.json&lt;/em&gt; est un fichier de type json.
Attention l&amp;rsquo;extension du fichier ne garantit en aucun cas que le contenu du fichier est bien celui prétendu par l&amp;rsquo;extension. Il s&amp;rsquo;agit simplement d&amp;rsquo;une convention, pas d&amp;rsquo;une contrainte.
Enfin on peut distinguer 2 grands types de contenus dans nos fichiers :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;les contenus texte, les fichiers sont lisibles et éditables avec un éditeur de texte.&lt;/li&gt;
&lt;li&gt;les contenus binaires, les fichiers sont utilisables avec des programmes particuliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;json&#34;&gt;JSON&lt;/h2&gt;
&lt;p&gt;Les fichiers JSON :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.json&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;ils contiennent du texte&lt;/li&gt;
&lt;li&gt;ils sont produits par des systèmes automatisés comme des API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Un fichier JSON permet de stocker de l&amp;rsquo;information hiérarchique, en effet ce format provient de la programmation est est utilisé pour stockée des objets complexes.&lt;br&gt;
Pour faire simple, dans un JSON vous stockez des couples clé/valeur sous la forme &lt;em&gt;&amp;ldquo;MaClé&amp;rdquo;: &amp;ldquo;MaValeur&amp;rdquo;&lt;/em&gt; chaque couple est séparé par une virgule.&lt;br&gt;
La valeur peut être :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Une chaine de caractères, dans ce cas elle est entre quote &lt;em&gt;&amp;quot;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;une valeur numérique, dans ce cas elle est saisie directement après les 2 points. On utilise le point comme séparateur décimal.&lt;/li&gt;
&lt;li&gt;un valeur booléenne, dans ce cas on saisie directement &lt;em&gt;true&lt;/em&gt; ou &lt;em&gt;false&lt;/em&gt; sans quote.&lt;/li&gt;
&lt;li&gt;un objet, qui n&amp;rsquo;est défini comme un ensemble de clés/valeur correspondant à une clé. Dans ce cas la valeur se trouve entre accolades.&lt;/li&gt;
&lt;li&gt;un tableau, qui peut contenir plusieurs valeurs de n&amp;rsquo;importe quel type.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Voici un exemple de fichier JSON :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;MonObjet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;Nom&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Nom de mon objet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;&amp;#34;Quantite&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;MonTableauDeNombre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;MonTableauDObjet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;Titre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Mon livre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;&amp;#34;Auteur&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Max&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;Titre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Mon second livre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;&amp;#34;Auteur&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Max&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;csv&#34;&gt;CSV&lt;/h2&gt;
&lt;p&gt;Les fichiers CSV :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;ils contiennent du texte&lt;/li&gt;
&lt;li&gt;ils sont produits par des systèmes automatisés comme des exports de logiciel métier.&lt;/li&gt;
&lt;li&gt;le format peut différer en fonction des informations régionales utilisées pour le produire.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Un fichier CSV permet de stocker de l&amp;rsquo;information sous la forme d&amp;rsquo;un tableau de données.&lt;br&gt;
On retrouve une ligne dans le fichier par ligne dans la table, toutes les lignes du fichier doivent avoir la même structure.&lt;/p&gt;
&lt;p&gt;Pour chaque fichier CSV on doit connaitre les informations le constituant afin de pouvoir le lire sans erreur, notamment :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quel est le séparateur de colonne, le point-virgule en français ou la virgule en américain.&lt;/li&gt;
&lt;li&gt;Quel est le séparateur décimal, la virgule en français ou le point en américain.&lt;/li&gt;
&lt;li&gt;Qu’elle est de délimiteur de colonne s’il existe, en général la double quote &lt;em&gt;&amp;quot;&lt;/em&gt; qui permet d&amp;rsquo;utiliser les séparateurs de colonnes dans du texte sans qu&amp;rsquo;il soit interprété.&lt;/li&gt;
&lt;li&gt;Si la première ligne est l&amp;rsquo;en tête des colonnes ou une ligne de données.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Voici un exemple de fichier CSV américain :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Titre,Auteur,Quantité
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Mon livre, Max, 10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Mon second livre, Max, 25
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;excel&#34;&gt;Excel&lt;/h2&gt;
&lt;p&gt;Les fichiers Excel :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.xls&lt;/em&gt; ou &lt;em&gt;.xlsx&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;ils contiennent des données binaires.&lt;/li&gt;
&lt;li&gt;ils sont produits par Microsoft Excel ou des systèmes d&amp;rsquo;export de données.&lt;/li&gt;
&lt;li&gt;le nombre de lignes d&amp;rsquo;une feuille de calcul est limité (1 048 576 lignes pour les fichiers xlsx)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Les fichiers Excel sont en général une source de problème, car étant à la main des utilisateurs, ils ont tendance à modifier la structure de leur fichier sans connaitre les conséquences sur les traitements de données utilisant leurs fichiers.&lt;br&gt;
Attention on trouve encore beaucoup de fichier Excel 97-2003, ceux avec l&amp;rsquo;extension &lt;em&gt;xls&lt;/em&gt;, ce format de fichier est vieillissant. Évitez-les si vous le pouvez.&lt;/p&gt;
&lt;h2 id=&#34;parquet&#34;&gt;Parquet&lt;/h2&gt;
&lt;p&gt;Les fichiers Parquet :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.parquet&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;ils contiennent des données binaires.&lt;/li&gt;
&lt;li&gt;ils sont inmutables, une fois le fichier créer vous ne pouvez pas modifier sont contenu.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Les fichiers parquet sont le format roi des datalakes. Ils stockent des tables et apportent grand nombre de services :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Présence des métadonnées de la table
&lt;ul&gt;
&lt;li&gt;nom de colonne&lt;/li&gt;
&lt;li&gt;type de données&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stockage colonne optimisée pour les traitements analytiques.&lt;/li&gt;
&lt;li&gt;Compression des données.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Étant inmutable, l&amp;rsquo;utilisation de ces fichiers demande une bonne stratégie d&amp;rsquo;alimentation pour optimiser vos traitements. En général on trouve les scénarii suivant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remplacement intégral des fichiers avec la nouvelle version des données.&lt;/li&gt;
&lt;li&gt;Remplacement partiel des fichiers avec un sous-ensemble des données, on ne remplace que certains fichiers, chaque fichier correspond à une partition de données connue.&lt;/li&gt;
&lt;li&gt;Ajout uniquement des nouvelles données dans de nouveaux fichiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le format parquet est un format open source géré par la &lt;a class=&#34;link&#34; href=&#34;https://parquet.apache.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;fondation Apache&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;delta-lake&#34;&gt;Delta lake&lt;/h3&gt;
&lt;p&gt;La gestion des suppression et modification de données dans le système source n&amp;rsquo;est pas simple à gérer avec les fichiers parquets.&lt;br&gt;
Afin de simplifier cela, il existe le format Delta lake qui permet de gérer ces problèmes. Le résultat est un ensemble de fichiers parquet et json qui représente l&amp;rsquo;état des données. Chaque modification créée de nouveaux fichiers.
Vous trouverez plus d&amp;rsquo;information sur le &lt;a class=&#34;link&#34; href=&#34;https://delta.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;site de delta lake&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datalake : comment traiter les données d&#39;un datalake dans Azure</title>
        <link>https://blog.ddata.fr/p/datalake-compute/</link>
        <pubDate>Sun, 05 Mar 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-compute/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-compute/deepmind-ZJKE4XVlKIA-unsplash.jpg" alt="Featured image of post Datalake : comment traiter les données d&#39;un datalake dans Azure" /&gt;&lt;p&gt;Dans cet article je vous présente les principaux services permettant de traiter les données d&amp;rsquo;un datalake dans Azure.&lt;/p&gt;
&lt;h2 id=&#34;rappel&#34;&gt;Rappel&lt;/h2&gt;
&lt;p&gt;Vous trouverez dans mes articles précédents les explications concernant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les architectures de type datalakehouse : &lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/&#34; &gt;Datawarehouse ou datalakehouse, quelle source pour vos reportings ?&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Les datalakes :&lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/datalake-la-base/&#34; &gt;Datalake : la base d&amp;rsquo;un projet data moderne&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;les-services-de-données-spécialisés&#34;&gt;Les services de données spécialisés&lt;/h2&gt;
&lt;h3 id=&#34;img10126-icon-service-data-factorypng-azure-data-factory&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10126-icon-service-Data-Factory.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10126-icon-service-Data-Factory_hu8e9af3290ac174282009dd85995d7eea_896_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10126-icon-service-Data-Factory_hu8e9af3290ac174282009dd85995d7eea_896_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Data Factory&lt;/h3&gt;
&lt;p&gt;Azure Data Factory est un outil de type ETL/ELT (Extract Transform Load / Extract Load Transform), il permet de réaliser des déplacements de données d&amp;rsquo;un point à un autre en réalisant des transformations sur les données. Parmi les usages classiques, on retrouve :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alimentation de la zone Bronze d&amp;rsquo;un datalake avec des données en provenance de service cloud et de vos réseaux d&amp;rsquo;entreprise (via un programme installé sur vos serveurs)&lt;/li&gt;
&lt;li&gt;Transformation et structuration des données :
&lt;ul&gt;
&lt;li&gt;Création de fichiers structurés de type Parquet&lt;/li&gt;
&lt;li&gt;Re modélisation des données applicative vers un modèle en étoile&lt;/li&gt;
&lt;li&gt;Transformation de données semi-structurées de type JSON en données structuré de type Parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suivi de l&amp;rsquo;exécution des traitements de transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/data-factory/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img00606-icon-service-azure-synapse-analyticspng-azure-synapse-analytics&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/00606-icon-service-Azure-Synapse-Analytics.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/00606-icon-service-Azure-Synapse-Analytics_hu8e9af3290ac174282009dd85995d7eea_1467_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/00606-icon-service-Azure-Synapse-Analytics_hu8e9af3290ac174282009dd85995d7eea_1467_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Synapse Analytics&lt;/h3&gt;
&lt;p&gt;Azure Synapse Analytics est avant tout une suite d&amp;rsquo;outils comprenant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;un outil de type ETL/ELT, qui est simplement une version d&amp;rsquo;Azure Data Factory&lt;/li&gt;
&lt;li&gt;des moteurs de traitements de données :
&lt;ul&gt;
&lt;li&gt;SQL
&lt;ul&gt;
&lt;li&gt;Pools provisionnés : Vous louez des capacités de traitements SQL dédiés.&lt;/li&gt;
&lt;li&gt;Serverless : Vous payez vos traitements SQL à la requête.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apache Spark : le moteur de traitement de données Apache Spark qui permet de créer des traitements de données fortement scalable. Vous pouvez écrire des traitements dans différents langages (Python/R/Scala/Java) et les exécuter sur la plate-forme Spark.&lt;/li&gt;
&lt;li&gt;Data Explorer : un moteur de traitement spécialisé dans les traitements des logs et des séries temporelles.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;un outil de monitoring&lt;/li&gt;
&lt;li&gt;un espace de création regroupant tous les services ci-dessus.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Azure Synapse Analytics est le service phare de la suite Azure pour les traitements de données.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/synapse-analytics/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10787-icon-service-azure-databrickspng-azure-databricks&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10787-icon-service-Azure-Databricks.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10787-icon-service-Azure-Databricks_hu8e9af3290ac174282009dd85995d7eea_1519_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10787-icon-service-Azure-Databricks_hu8e9af3290ac174282009dd85995d7eea_1519_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Databricks&lt;/h3&gt;
&lt;p&gt;Azure Databricks est l&amp;rsquo;implémentation de Databricks dans Azure.&lt;br&gt;
Databricks est un outil basé sur Apache Spark tout en incluant des services complémentaires. Au même titre qu&amp;rsquo;Azure Synapse Analytics, il permet de créer des traitements d&amp;rsquo;ingestion et de préparation de données.&lt;/p&gt;
&lt;p&gt;Contrairement à Azure Synapse Analytics, Databricks est prévu pour être multicloud, vous retrouverez ce service dans AWS (Amazon) ou GCP (Google).&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/databricks/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10145-icon-service-azure-data-explorer-clusterspng-azure-data-explorer&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10145-icon-service-Azure-Data-Explorer-Clusters.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10145-icon-service-Azure-Data-Explorer-Clusters_hu8e9af3290ac174282009dd85995d7eea_1395_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10145-icon-service-Azure-Data-Explorer-Clusters_hu8e9af3290ac174282009dd85995d7eea_1395_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Data Explorer&lt;/h3&gt;
&lt;p&gt;Azure Data Explorer est un moteur de traitement spécialisé dans les traitements des logs et des séries temporelles.&lt;br&gt;
Microsoft a créé un langage de requête dédié nommé KQL (Kusto Query Language) permettant de plonger dans les puits de logs comme le commandant Cousteau dans la mer.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/data-explorer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;les-services-cognitifs&#34;&gt;Les services cognitifs&lt;/h2&gt;
&lt;h3 id=&#34;img10162-icon-service-cognitive-servicespng-azure-cognitive-services&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10162-icon-service-Cognitive-Services.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10162-icon-service-Cognitive-Services_hu8e9af3290ac174282009dd85995d7eea_1235_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10162-icon-service-Cognitive-Services_hu8e9af3290ac174282009dd85995d7eea_1235_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Cognitive Services&lt;/h3&gt;
&lt;p&gt;Azure Cognitive Services est un ensemble de service de type IA préentrainé que vous pouvez consommer directement sous forme d&amp;rsquo;API.&lt;br&gt;
Parmi les principaux services proposés, on retrouve :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La reconnaissance de texte&lt;/li&gt;
&lt;li&gt;La reconnaissance faciale&lt;/li&gt;
&lt;li&gt;La reconnaissance d&amp;rsquo;entité sur des images ou vidéos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vous pouvez intégrer ces services dans vos traitements de données pour enrichir ces dernières.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/cognitive-services/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10167-icon-service-machine-learning-studio-workspacespng-azure-machine-learning&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10167-icon-service-Machine-Learning-Studio-Workspaces.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10167-icon-service-Machine-Learning-Studio-Workspaces_hu8e9af3290ac174282009dd85995d7eea_868_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10167-icon-service-Machine-Learning-Studio-Workspaces_hu8e9af3290ac174282009dd85995d7eea_868_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Machine Learning&lt;/h3&gt;
&lt;p&gt;Azure Machine Learning est un outil de création d&amp;rsquo;expérience de Machine Learning, il prend en charge de bout en bout ce type de projet de la création du modèle à son exploitation en production.&lt;br&gt;
Les modèles ainsi créés pourront enrichir vos traitements de données.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;les-services-de-calcul-utilisable-pour-traiter-les-données&#34;&gt;Les services de calcul utilisable pour traiter les données&lt;/h2&gt;
&lt;h3 id=&#34;img10029-icon-service-function-appspng-azure-functions&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10029-icon-service-Function-Apps.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10029-icon-service-Function-Apps_hu8e9af3290ac174282009dd85995d7eea_1188_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10029-icon-service-Function-Apps_hu8e9af3290ac174282009dd85995d7eea_1188_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Functions&lt;/h3&gt;
&lt;p&gt;Azure Functions permet d&amp;rsquo;exécuter du code à la demande dans Azure, sans louer de ressources en permanence. Vous êtes facturé à l&amp;rsquo;usage.&lt;br&gt;
Vous pouvez créer des traitements s&amp;rsquo;exécutant à la demande, sur des évènements Azure ou de manière planifiée.&lt;br&gt;
Les principaux langages disponibles sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;li&gt;Java&lt;/li&gt;
&lt;li&gt;PowerShell&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;TypeScript&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Par défaut les Azure Functions sont limitées à 5 minutes d&amp;rsquo;exécutions, 10 maximum. Si vous avez besoin de plus regarder &lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=csharp-inproc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;les fonctions durables.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/functions/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10201-icon-service-logic-appspng-azure-logic-apps&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10201-icon-service-Logic-Apps.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10201-icon-service-Logic-Apps_hu8e9af3290ac174282009dd85995d7eea_949_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10201-icon-service-Logic-Apps_hu8e9af3290ac174282009dd85995d7eea_949_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Logic Apps&lt;/h3&gt;
&lt;p&gt;Azure Logic Apps est un outil de création de traitement low code. Il est très proche en termes d&amp;rsquo;interface et d&amp;rsquo;utilisation de Power Automate, mais contrairement à ce dernier vous payez les traitements à l&amp;rsquo;exécution, quels que soient les connecteurs utilisés.&lt;br&gt;
Vous pouvez créer des traitements s&amp;rsquo;exécutant à la demande, sur des évènements Azure ou de manière planifiée.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/logic-apps/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;autres-services-azure-à-intégrer&#34;&gt;Autres services Azure à intégrer&lt;/h2&gt;
&lt;h3 id=&#34;img10245-icon-service-key-vaultspng-azure-key-vault&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10245-icon-service-Key-Vaults.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10245-icon-service-Key-Vaults_hu7a6ed2925b8411b46ca180016908f0d9_1806_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10245-icon-service-Key-Vaults_hu7a6ed2925b8411b46ca180016908f0d9_1806_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Key Vault&lt;/h3&gt;
&lt;p&gt;Azure Key Vault est un coffre-fort pour vos mots de passe, est-il besoin de rappeler qu&amp;rsquo;un mot de passe n&amp;rsquo;a rien à faire en clair dans un code ?&lt;br&gt;
Azure Key Vault vous permettra de sécuriser vos mots de passe et autres secrets et le mettra à disposition des applications ayant le droit de les utiliser. Vos différentes briques de services Azure ou autre pourront ainsi utiliser des secrets en toute discrétion.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/key-vault/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;imgpurviewpng-microsoft-purview&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/purview.png&#34;
	width=&#34;30&#34;
	height=&#34;19&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/purview_huebae1058d42e27fb2cdbb3964620300c_1234_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/purview_huebae1058d42e27fb2cdbb3964620300c_1234_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt; Microsoft Purview&lt;/h3&gt;
&lt;p&gt;Microsoft Purview est en ensemble d&amp;rsquo;outils permettant de mettre en place une gouvernance de données globale. Il vous permet aussi de mettre en place un catalogue de données d&amp;rsquo;entreprise.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/purview/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture-type&#34;&gt;Architecture type&lt;/h2&gt;
&lt;p&gt;Tous les outils présentés ci-dessus peuvent travailler ensemble dans des architectures data moderne, ma préférée étant celle autour de Synapse pour un usage BI:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/enterprise-bi-scoped-architecture.png&#34;
	width=&#34;1200&#34;
	height=&#34;846&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/enterprise-bi-scoped-architecture_hu1f3b34477e815894fea8669db0f97484_250544_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/enterprise-bi-scoped-architecture_hu1f3b34477e815894fea8669db0f97484_250544_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Décisionnel d’entreprise&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Vous trouverez les références de cette architecture &lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/fr-fr/azure/architecture/example-scenario/analytics/enterprise-bi-synapse&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt; et une version plus globale &lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/fr-fr/azure/architecture/example-scenario/dataplate2e/data-platform-end-to-end&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datalake : la base d&#39;un projet data moderne</title>
        <link>https://blog.ddata.fr/p/datalake-la-base/</link>
        <pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-la-base/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-la-base/ticka-kao-o87vASD6Ksk-unsplash.jpg" alt="Featured image of post Datalake : la base d&#39;un projet data moderne" /&gt;&lt;p&gt;Dans cet article je vous présente le concept de Datalake ou lac de données en bon français. Un datalake est la fondation d&amp;rsquo;un projet data moderne, vous le retrouverez dans la plupart de vos futurs projets.&lt;/p&gt;
&lt;h2 id=&#34;contexte&#34;&gt;Contexte&lt;/h2&gt;
&lt;p&gt;Comme je vous l&amp;rsquo;ai présenté &lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/&#34; &gt;dans ce précédent article&lt;/a&gt;, le datalake est un élément clé des projets data moderne. Je me propose maintenant de vous présenter plus en détail ce qu&amp;rsquo;est un datalake, comment on le structure et comment on peut s&amp;rsquo;en servir.&lt;br&gt;
Contrairement à ce que l&amp;rsquo;on pourrait croire, les datalakes ne sont pas réservés aux gros projets de data et leur philosophie d&amp;rsquo;usage peut-être reprise dans tout projet data.&lt;/p&gt;
&lt;h2 id=&#34;le-stockage&#34;&gt;Le stockage&lt;/h2&gt;
&lt;p&gt;Lorsque l&amp;rsquo;on parle datalake on parle avant tout d&amp;rsquo;un stockage de données pouvant accepter tout type de format de données.&lt;br&gt;
En général lorsque l&amp;rsquo;on parle de datalake on pense stockage cloud, l&amp;rsquo;avantage de ce type de stockage est sa capacité à être étendue en fonction de vos besoins et de vos finances. Bien que cette solution est la plus souple, on peut très bien créer un datalake en dehors du cloud.&lt;br&gt;
Les principaux types de stockages pour vos datalakes peuvent être :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En cloud dans l&amp;rsquo;écosystème Microsoft
&lt;ul&gt;
&lt;li&gt;Azure Datalake Gen 2&lt;/li&gt;
&lt;li&gt;Sharepoint&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sur vos infrastructures locales
&lt;ul&gt;
&lt;li&gt;Un partage réseau&lt;/li&gt;
&lt;li&gt;Un disque dur&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lorganisation&#34;&gt;L&amp;rsquo;organisation&lt;/h2&gt;
&lt;p&gt;Une fois la solution de stockage choisie, il est indispensable de réfléchir à l&amp;rsquo;organisation de votre datalake.&lt;br&gt;
Sans rigueur votre lac de données deviendra un marais de données (dataswamp).&lt;/p&gt;
&lt;h3 id=&#34;les-zones&#34;&gt;Les zones&lt;/h3&gt;
&lt;p&gt;Bien que chacun puisse créer l&amp;rsquo;organisation qu&amp;rsquo;il souhaite, on retrouve en général les 3 zones suivantes :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-la-base/img/001-datalake-zone.png&#34;
	width=&#34;799&#34;
	height=&#34;230&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-la-base/img/001-datalake-zone_hu727fac40be1e93915e54196ea9c46c88_7320_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-la-base/img/001-datalake-zone_hu727fac40be1e93915e54196ea9c46c88_7320_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Les zones d&amp;rsquo;un datalake&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;347&#34;
		data-flex-basis=&#34;833px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bronze : Cette zone sert de landing zone pour les données, elle permet d&amp;rsquo;écrire les données brutes reçues dans le datalake. Aucune transformation n&amp;rsquo;est réalisée sur les données.&lt;/li&gt;
&lt;li&gt;Silver : cette zone permet de raffiner les données de la zone Bronze et d&amp;rsquo;effectuer les traitements de préparation de données parmi lesquels on retrouve :
&lt;ul&gt;
&lt;li&gt;La transformation de données en un format de table avec
&lt;ul&gt;
&lt;li&gt;Le nommage des colonnes&lt;/li&gt;
&lt;li&gt;Le typage des colonnes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;La détection des anomalies&lt;/li&gt;
&lt;li&gt;La détection des problèmes de qualité&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gold : cette zone contient les données avec le plus haut niveau de raffinage, on retrouve par exemple
&lt;ul&gt;
&lt;li&gt;Des données modélisées pour le reporting (modèle en étoile).&lt;/li&gt;
&lt;li&gt;Des données préaggrégées.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le nom de ces zones peut varier, mais leur usage reste. D&amp;rsquo;autres zones peuvent être créées en fonction de vos besoins spécifiques, par exemple une zone sandbox pour des expérimentations.&lt;/p&gt;
&lt;p&gt;Les coûts de stockage étant relativement bas, en général, les traitements permettant le raffinage des données d&amp;rsquo;une zone à l&amp;rsquo;autre ne suppriment pas les données traitées. Ainsi si vous traitez des données sources en utilisant 10 champs sur 50 dans le fichier brut et que vos besoins évoluent, vous aurez toujours la possibilité de retraiter l&amp;rsquo;ensemble des données brut pour répondre à vos nouveaux besoins.&lt;/p&gt;
&lt;h3 id=&#34;les-partitions&#34;&gt;Les partitions&lt;/h3&gt;
&lt;p&gt;Le partitionnement des données va vous permettre de ranger vos données de manière précise. Quand on parle de partitionnement, il s&amp;rsquo;agit simplement de mettre vos fichiers dans des dossiers et sous dossier différent. L&amp;rsquo;avantage du partitionnement est notamment de pouvoir accéder rapidement à des données en ne lisant que les fichiers présents dans les partitions qui nous intéressent.&lt;br&gt;
Vous pouvez mixer les partitions comme vous le souhaitez.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-la-base/img/002-datalake-partition.png&#34;
	width=&#34;774&#34;
	height=&#34;436&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-la-base/img/002-datalake-partition_hu0016a9332b0f3ada4911fd5db105c496_15833_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-la-base/img/002-datalake-partition_hu0016a9332b0f3ada4911fd5db105c496_15833_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de partitions&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;En général, on crée des partitions en créant des  dossiers sous la forme &lt;em&gt;NomPartition=ValeurPartition&lt;/em&gt;.&lt;br&gt;
Les principaux partitionnements que vous utiliserez sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Par source de données avec par exemple des dossiers sous la forme &lt;em&gt;/service=rh/application=monapplication&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Par version de traitement ou de format de fichier avec par exemple des dossiers sous la forme &lt;em&gt;/version=2.0&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Par date avec par exemple des dossiers sous la forme &lt;em&gt;/year=2023/month=02/day=26&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;les-principaux-formats-de-données&#34;&gt;Les principaux formats de données&lt;/h2&gt;
&lt;p&gt;Votre datalake va contenir des fichiers, voici quelque format classique que vous y trouverez :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parquet : C&amp;rsquo;est le format roi des datalakes, il s&amp;rsquo;agit d&amp;rsquo;un format de fichier dédier aux données analytiques, il conserve le nom et le type de données des colonnes et compresse les données contenues dans le fichier.&lt;/li&gt;
&lt;li&gt;CSV : Format classique d&amp;rsquo;extraction de données, il contient le nom des colonnes, mais les séparateurs utilisés peuvent varier en fonction de l&amp;rsquo;origine régionale du fichier (les séparateurs US et FR sont différents).&lt;/li&gt;
&lt;li&gt;Excel : Est-ce besoin de le présenter ? Un incontournable dans tous projets data.&lt;/li&gt;
&lt;li&gt;json, yaml, xml : Il s&amp;rsquo;agit en général de fichiers issus de traitement automatisé tel des API. Ces fichiers sont semi-structuré.&lt;/li&gt;
&lt;li&gt;log : les fichiers de log pourront être stockés longtemps et conserver une possibilité de traitement dans un datalake. Ces fichiers sont en général semi-structuré.&lt;/li&gt;
&lt;li&gt;Binaire : les vidéos et images sont des formats de données binaires, ils peuvent contenir certaines métadonnées exploitables directement (coordonnée GPS pour une photo par exemple), mais en général un traitement de type IA et nécessaire pour qualifier le contenu du fichier.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;la-sécurité&#34;&gt;La sécurité&lt;/h2&gt;
&lt;p&gt;La sécurité est un point clé de votre datalake, elle est portée par les possibilités offertes par la solution de stockage que vous avez choisie.&lt;br&gt;
Dans tous les cas, penser à créer plusieurs datalakes si vous avez des besoins forts de sécurité, une séparation des données peut prémunir contre des niveaux de droits inadéquats accordés à certain utilisateur.&lt;/p&gt;
&lt;h2 id=&#34;traitement-des-données&#34;&gt;Traitement des données&lt;/h2&gt;
&lt;p&gt;Le traitement des données d&amp;rsquo;un datalake fera l&amp;rsquo;objet d&amp;rsquo;un prochain article.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datawarehouse ou datalakehouse, quelle source pour vos reportings ?</title>
        <link>https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/</link>
        <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/antoine-petitteville-RIdYHUISNuM-unsplash.jpg" alt="Featured image of post Datawarehouse ou datalakehouse, quelle source pour vos reportings ?" /&gt;&lt;p&gt;Dans cet article, je vous présente les différentes sources possibles pour vos reportings. Les solutions natives de vos applications, un datawarehouse ou un datalakehouse ? Je vous propose de passer en revue ces solutions.&lt;/p&gt;
&lt;h2 id=&#34;le-reporting-de-données-sans-la-bi&#34;&gt;Le reporting de données sans la BI&lt;/h2&gt;
&lt;p&gt;Vos applications sont en général fournies avec des possibilités de reporting dédié fournies par l&amp;rsquo;éditeur de la solution.&lt;/p&gt;
&lt;p&gt;Les avantages de ces solutions de reporting natives sont principalement :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le coût normalement inclut dans la solution.&lt;/li&gt;
&lt;li&gt;Des rapports en lien très fort avec les données de la solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Par contre les inconvénients sont en général :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Une faible capacité de customisation et d&amp;rsquo;intégration de règle spécifique à votre usage.&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;impossibilité de croiser avec d&amp;rsquo;autres sources de données extérieures à la solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si l’on faisait une analogie avec la cuisine, nous avons affaire à un plat industriel prêt à être consommé.&lt;/p&gt;
&lt;p&gt;On peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui délivre des rapports aux utilisateurs via des outils de reporting différents les uns des autres.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/001-data-sans-bi.png&#34;
	width=&#34;601&#34;
	height=&#34;706&#34;
	srcset=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/001-data-sans-bi_hu8a19905e549f9e1eaa5a52d8f8030dd9_33837_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/001-data-sans-bi_hu8a19905e549f9e1eaa5a52d8f8030dd9_33837_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Le reporting sans BI&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;85&#34;
		data-flex-basis=&#34;204px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;la-bi-à-la-rescousse-&#34;&gt;La BI à la rescousse !&lt;/h2&gt;
&lt;p&gt;Comment faire mieux que cette solution de données en silo ? La réponse est simplement l&amp;rsquo;informatique décisionnelle souvent appelée BI acronyme de son nom anglais Business Inteligence.&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;objectif de la BI est de vous permettre d&amp;rsquo;utiliser un seul outil pour tous vos reportings, basé sur une source de données consolidées permettant le croisement des données de vos différents silos de données grâce à la mise en relation des données communes.&lt;/p&gt;
&lt;h2 id=&#34;les-services-attendus-dune-couche-data&#34;&gt;Les services attendus d&amp;rsquo;une couche data&lt;/h2&gt;
&lt;p&gt;Pour réaliser cet objectif, les solutions proposées par la BI ont évoluée dans le temps, mais les services attendus par cette couche intermédiaire reste les mêmes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Capacité de traitement et de stockage dédié distinct de l&amp;rsquo;infrastructure de l&amp;rsquo;application&lt;/li&gt;
&lt;li&gt;Centralisation des données&lt;/li&gt;
&lt;li&gt;Modélisation des données optimisée pour le reporting (modèle en étoile)&lt;/li&gt;
&lt;li&gt;Historisation des données
&lt;ul&gt;
&lt;li&gt;Pour certaine dimension, il est nécessaire de conserver la trace des changements, on parle en général de dimension à variation lente (Slow changing dimension).&lt;/li&gt;
&lt;li&gt;Pour certaines tables de faits, comme les stocks, on souhaite conserver des photos à un instant T des données afin de les assembler en un film de données permettant de suivre l&amp;rsquo;évolution des changements dans le temps.&lt;/li&gt;
&lt;li&gt;Réaliser des transformations des données sources comme par exemple passées de données cumulées (non agrégable) en données journalière (agrégable).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cette liste n&amp;rsquo;est pas exhaustive des services attendus par les datatrucs.&lt;/p&gt;
&lt;h2 id=&#34;les-datawarehouses&#34;&gt;Les datawarehouses&lt;/h2&gt;
&lt;p&gt;La solution que l&amp;rsquo;on retrouve le plus fréquemment dans les solutions BI historiques des entreprises est le datawarehouse ou entrepôt de données en bon français.&lt;/p&gt;
&lt;p&gt;Un datawarehouse est basiquement une base de données dédiée au reporting. Cette base de données est chargée via un ETL (acronyme d&amp;rsquo;Extract Transform Load). Le chargement est en général réalisé par des traitements en mode batch, en opposition à un traitement en temps réel, qui charge les données à une fréquence prédéfinies, classiquement une fois par jour.&lt;/p&gt;
&lt;p&gt;Les données subissent des transformations lors du chargement afin que les tables résultantes soient optimisées pour le reporting, on passe en général d&amp;rsquo;une modélisation OLTP (celle de l&amp;rsquo;application) à une modélisation en étoile.&lt;/p&gt;
&lt;p&gt;Les datawarehouses sont des projets techniques en général réalisés par l&amp;rsquo;IT de l&amp;rsquo;entreprise.&lt;/p&gt;
&lt;p&gt;Si l’on faisait une analogie avec la cuisine, nous avons affaire à un plat fait maison, mais l&amp;rsquo;œuf utilisé pour la tarte ne peut plus servir à faire des crêpes.&lt;/p&gt;
&lt;p&gt;On peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui sont chargées dans une base intermédiaire dédié au reporting, le datawarehouse.&lt;br&gt;
Le reporting peut être réalisé par un outil unique  ou pas en fonction des besoins des utilisateurs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/002-datawarehouse.png&#34;
	width=&#34;578&#34;
	height=&#34;675&#34;
	srcset=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/002-datawarehouse_hu65ec1f30ab54919c049d7a2243557acb_25785_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/002-datawarehouse_hu65ec1f30ab54919c049d7a2243557acb_25785_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Le reporting avec un datawarehouse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;85&#34;
		data-flex-basis=&#34;205px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;les-datalakehouses&#34;&gt;Les datalakehouses&lt;/h2&gt;
&lt;p&gt;Les datawarehouses ont certains inconvénients et ne sont pas optimisés pour répondre à certains besoins modernes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les données étant stockées dans une base de données, ils supportent très mal les données non structurées et les changements de structure des données dans le temps.&lt;/li&gt;
&lt;li&gt;Les données de type streaming (données en temps réel comme un capteur ou flux de données twitter) sont difficiles à intégrer dans un datawarehouse.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pour répondre à ces besoins modernes tout en gardant les services fournis par les datawarehouses, la BI moderne propose comme réponse les datalakehouses.&lt;/p&gt;
&lt;p&gt;Un datalakehouse est la fusion des datalakes et des datawarehouses. J&amp;rsquo;ai expliqué ce qu&amp;rsquo;est un datawarehouse ci-dessus, je vais donc expliqué le concept de datalake.&lt;/p&gt;
&lt;p&gt;Les datalakes sont la réponse au stockage de données répondant aux critères suivants :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Il accepte les formats multiples
&lt;ul&gt;
&lt;li&gt;Données structurées en provenance des bases de données&lt;/li&gt;
&lt;li&gt;Données semis structuré comme des fichiers Excel, JSON, CSV&lt;/li&gt;
&lt;li&gt;Données non structurées comme des images ou vidéos, qui pourront subir des traitements de type IA pour en extraire de l&amp;rsquo;information exploitable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Il supporte des fréquences de chargement variables allant du mode batch de datawarehouse au temps réel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ce stockage permettant tous ces cas d&amp;rsquo;usages n&amp;rsquo;est ni plus ni moins qu&amp;rsquo;un stockage avec les mêmes services qu&amp;rsquo;un disque dur, mais disponible en général dans le cloud.&lt;/p&gt;
&lt;p&gt;Le plus important avec un datalake est la rigueur avec laquelle on range ces données afin de ne pas finir avec un dataswamp ou marais de données.&lt;/p&gt;
&lt;p&gt;Un datalakehouse est donc une architecture de raffinage de données composé d&amp;rsquo;un datalake et de divers outils permettant le raffinage des données. Différents outils peuvent être utilisés aux différentes phases de raffinage en fonction des besoins.&lt;/p&gt;
&lt;p&gt;Les principales étapes de raffinage des données sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L&amp;rsquo;alimentation du datalake avec les données brutes, sans transformation, dans une zone que l&amp;rsquo;on nomme couramment &lt;em&gt;Bronze&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Le raffinage des données brut à l&amp;rsquo;aide de divers traitements en fonction des besoins, on stocke le résultat dans une zone que l&amp;rsquo;on nomme couramment &lt;em&gt;Silver&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;La mise à disposition des données de reporting dans une zone que l&amp;rsquo;on nomme couramment &lt;em&gt;Gold&lt;/em&gt;. Les données peuvent par exemple être préaggrégées pour améliorer le reporting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On conserve les données de chaque étape ainsi on peut, si besoin, ajouter de nouveaux traitements sur d&amp;rsquo;anciennes données.&lt;br&gt;
Les données du datalake du dalakehouse seront donc disponibles pour d&amp;rsquo;autres usages tels que :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Du machine learning.&lt;/li&gt;
&lt;li&gt;Un référentiel d&amp;rsquo;entreprise mis à disposition d&amp;rsquo;autres applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si l’on faisait une analogie avec la cuisine, nous avons affaire à un garde-manger où on en prend les éléments que l&amp;rsquo;on souhaite cuisiner en fonction des besoins.&lt;/p&gt;
&lt;p&gt;On peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui sont chargées dans un stockage intermédiaire sans usage spécifique puis préparé en fonction des besoins, notamment le reporting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/003-datalakehouse.png&#34;
	width=&#34;589&#34;
	height=&#34;754&#34;
	srcset=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/003-datalakehouse_hu24dae463978d3b0f2f8d47f596271f5e_31506_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/003-datalakehouse_hu24dae463978d3b0f2f8d47f596271f5e_31506_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Le reporting avec un datalakehouse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;78&#34;
		data-flex-basis=&#34;187px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;J&amp;rsquo;espère que maintenant vous comprenez mieux l&amp;rsquo;utilisation de ces différents datatrucs.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
