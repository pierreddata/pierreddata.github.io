<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Data on Le D de Data</title>
        <link>https://blog.ddata.fr/categories/data/</link>
        <description>Recent content in Data on Le D de Data</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>fr-fr</language>
        <lastBuildDate>Sun, 26 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.ddata.fr/categories/data/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Datalake : les principaux formats de fichiers</title>
        <link>https://blog.ddata.fr/p/datalake-file-format/</link>
        <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-file-format/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-file-format/david-bruno-silva-Z19vToWBDIc-unsplash.jpg" alt="Featured image of post Datalake : les principaux formats de fichiers" /&gt;&lt;p&gt;Dans cet article je vous présente les principaux formats de fichier que l&amp;rsquo;on trouve dans un datalake.&lt;/p&gt;
&lt;h2 id=&#34;contexte&#34;&gt;Contexte&lt;/h2&gt;
&lt;p&gt;Lorsque vous travaillez avec un datalake vous allez manipuler un certain nombre de formats de fichier. Je vous présente ici ceux que vous rencontrerez le plus souvent.&lt;/p&gt;
&lt;p&gt;Vous trouverez dans mes articles précédents les explications concernant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les architectures de type datalakehouse : &lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/&#34; &gt;Datawarehouse ou datalakehouse, quelle source pour vos reportings ?&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Les datalakes :&lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/datalake-la-base/&#34; &gt;Datalake : la base d&amp;rsquo;un projet data moderne&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quest-ce-quun-fichier-&#34;&gt;Qu&amp;rsquo;est-ce qu&amp;rsquo;un fichier ?&lt;/h2&gt;
&lt;p&gt;La question peut sembler triviale pour certain j&amp;rsquo;ai rencontré suffisamment de personnes pour qui cela n&amp;rsquo;était pas clair pour prendre le temps de l&amp;rsquo;expliquer.&lt;br&gt;
En informatique on utilise des fichiers pour stocker de l&amp;rsquo;information et des dossiers pour ranger nos fichiers dans des structures hiérarchiques.&lt;br&gt;
Un fichier peut contenir tous types d&amp;rsquo;informations, du code binaire de vos programmes aux données des documents que vous réalisez avec ces programmes.&lt;br&gt;
On appelle un format de fichier l&amp;rsquo;extension du fichier.&lt;br&gt;
L&amp;rsquo;extension du fichier est simplement la suite de caractères se trouvant après le dernier point du nom complet du fichier. Les extensions sont souvent masquées dans l&amp;rsquo;explorateur de fichier de Windows.&lt;br&gt;
Par exemple, &lt;em&gt;MonFichier.json&lt;/em&gt; est un fichier de type json.
Attention l&amp;rsquo;extension du fichier ne garantit en aucun cas que le contenu du fichier est bien celui prétendu par l&amp;rsquo;extension. Il s&amp;rsquo;agit simplement d&amp;rsquo;une convention, pas d&amp;rsquo;une contrainte.
Enfin on peut distinguer 2 grands types de contenus dans nos fichiers :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;les contenus texte, les fichiers sont lisibles et éditables avec un éditeur de texte.&lt;/li&gt;
&lt;li&gt;les contenus binaires, les fichiers sont utilisables avec des programmes particuliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;json&#34;&gt;JSON&lt;/h2&gt;
&lt;p&gt;Les fichiers JSON :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.json&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;ils contiennent du texte&lt;/li&gt;
&lt;li&gt;ils sont produits par des systèmes automatisés comme des API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Un fichier JSON permet de stocker de l&amp;rsquo;information hiérarchique, en effet ce format provient de la programmation est est utilisé pour stockée des objets complexes.&lt;br&gt;
Pour faire simple, dans un JSON vous stockez des couples clé/valeur sous la forme &lt;em&gt;&amp;ldquo;MaClé&amp;rdquo;: &amp;ldquo;MaValeur&amp;rdquo;&lt;/em&gt; chaque couple est séparé par une virgule.&lt;br&gt;
La valeur peut être :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Une chaine de caractères, dans ce cas elle est entre quote &lt;em&gt;&amp;quot;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;une valeur numérique, dans ce cas elle est saisie directement après les 2 points. On utilise le point comme séparateur décimal.&lt;/li&gt;
&lt;li&gt;un valeur booléenne, dans ce cas on saisie directement &lt;em&gt;true&lt;/em&gt; ou &lt;em&gt;false&lt;/em&gt; sans quote.&lt;/li&gt;
&lt;li&gt;un objet, qui n&amp;rsquo;est défini comme un ensemble de clés/valeur correspondant à une clé. Dans ce cas la valeur se trouve entre accolades.&lt;/li&gt;
&lt;li&gt;un tableau, qui peut contenir plusieurs valeurs de n&amp;rsquo;importe quel type.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Voici un exemple de fichier JSON :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;MonObjet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;Nom&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Nom de mon objet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;&amp;#34;Quantite&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;MonTableauDeNombre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;MonTableauDObjet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;Titre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Mon livre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;&amp;#34;Auteur&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Max&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;Titre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Mon second livre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;&amp;#34;Auteur&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Max&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;csv&#34;&gt;CSV&lt;/h2&gt;
&lt;p&gt;Les fichiers CSV :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;ils contiennent du texte&lt;/li&gt;
&lt;li&gt;ils sont produits par des systèmes automatisés comme des exports de logiciel métier.&lt;/li&gt;
&lt;li&gt;le format peut différer en fonction des informations régionales utilisées pour le produire.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Un fichier CSV permet de stocker de l&amp;rsquo;information sous la forme d&amp;rsquo;un tableau de données.&lt;br&gt;
On retrouve une ligne dans le fichier par ligne dans la table, toutes les lignes du fichier doivent avoir la même structure.&lt;/p&gt;
&lt;p&gt;Pour chaque fichier CSV on doit connaitre les informations le constituant afin de pouvoir le lire sans erreur, notamment :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quel est le séparateur de colonne, le point-virgule en français ou la virgule en américain.&lt;/li&gt;
&lt;li&gt;Quel est le séparateur décimal, la virgule en français ou le point en américain.&lt;/li&gt;
&lt;li&gt;Qu’elle est de délimiteur de colonne s’il existe, en général la double quote &lt;em&gt;&amp;quot;&lt;/em&gt; qui permet d&amp;rsquo;utiliser les séparateurs de colonnes dans du texte sans qu&amp;rsquo;il soit interprété.&lt;/li&gt;
&lt;li&gt;Si la première ligne est l&amp;rsquo;en tête des colonnes ou une ligne de données.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Voici un exemple de fichier CSV américain :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Titre,Auteur,Quantité
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Mon livre, Max, 10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Mon second livre, Max, 25
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;excel&#34;&gt;Excel&lt;/h2&gt;
&lt;p&gt;Les fichiers Excel :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.xls&lt;/em&gt; ou &lt;em&gt;.xlsx&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;ils contiennent des données binaires.&lt;/li&gt;
&lt;li&gt;ils sont produits par Microsoft Excel ou des systèmes d&amp;rsquo;export de données.&lt;/li&gt;
&lt;li&gt;le nombre de lignes d&amp;rsquo;une feuille de calcul est limité (1 048 576 lignes pour les fichiers xlsx)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Les fichiers Excel sont en général une source de problème, car étant à la main des utilisateurs, ils ont tendance à modifier la structure de leur fichier sans connaitre les conséquences sur les traitements de données utilisant leurs fichiers.&lt;br&gt;
Attention on trouve encore beaucoup de fichier Excel 97-2003, ceux avec l&amp;rsquo;extension &lt;em&gt;xls&lt;/em&gt;, ce format de fichier est vieillissant. Évitez-les si vous le pouvez.&lt;/p&gt;
&lt;h2 id=&#34;parquet&#34;&gt;Parquet&lt;/h2&gt;
&lt;p&gt;Les fichiers Parquet :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on en général l&amp;rsquo;extension &lt;em&gt;.parquet&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;ils contiennent des données binaires.&lt;/li&gt;
&lt;li&gt;ils sont inmutables, une fois le fichier créer vous ne pouvez pas modifier sont contenu.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Les fichiers parquet sont le format roi des datalakes. Ils stockent des tables et apportent grand nombre de services :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Présence des métadonnées de la table
&lt;ul&gt;
&lt;li&gt;nom de colonne&lt;/li&gt;
&lt;li&gt;type de données&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stockage colonne optimisée pour les traitements analytiques.&lt;/li&gt;
&lt;li&gt;Compression des données.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Étant inmutable, l&amp;rsquo;utilisation de ces fichiers demande une bonne stratégie d&amp;rsquo;alimentation pour optimiser vos traitements. En général on trouve les scénarii suivant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remplacement intégral des fichiers avec la nouvelle version des données.&lt;/li&gt;
&lt;li&gt;Remplacement partiel des fichiers avec un sous-ensemble des données, on ne remplace que certains fichiers, chaque fichier correspond à une partition de données connue.&lt;/li&gt;
&lt;li&gt;Ajout uniquement des nouvelles données dans de nouveaux fichiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le format parquet est un format open source géré par la &lt;a class=&#34;link&#34; href=&#34;https://parquet.apache.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;fondation Apache&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;delta-lake&#34;&gt;Delta lake&lt;/h3&gt;
&lt;p&gt;La gestion des suppression et modification de données dans le système source n&amp;rsquo;est pas simple à gérer avec les fichiers parquets.&lt;br&gt;
Afin de simplifier cela, il existe le format Delta lake qui permet de gérer ces problèmes. Le résultat est un ensemble de fichiers parquet et json qui représente l&amp;rsquo;état des données. Chaque modification créée de nouveaux fichiers.
Vous trouverez plus d&amp;rsquo;information sur le &lt;a class=&#34;link&#34; href=&#34;https://delta.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;site de delta lake&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datalake : comment traiter les données d&#39;un datalake dans Azure</title>
        <link>https://blog.ddata.fr/p/datalake-compute/</link>
        <pubDate>Sun, 05 Mar 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-compute/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-compute/deepmind-ZJKE4XVlKIA-unsplash.jpg" alt="Featured image of post Datalake : comment traiter les données d&#39;un datalake dans Azure" /&gt;&lt;p&gt;Dans cet article je vous présente les principaux services permettant de traiter les données d&amp;rsquo;un datalake dans Azure.&lt;/p&gt;
&lt;h2 id=&#34;rappel&#34;&gt;Rappel&lt;/h2&gt;
&lt;p&gt;Vous trouverez dans mes articles précédents les explications concernant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les architectures de type datalakehouse : &lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/&#34; &gt;Datawarehouse ou datalakehouse, quelle source pour vos reportings ?&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Les datalakes :&lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/datalake-la-base/&#34; &gt;Datalake : la base d&amp;rsquo;un projet data moderne&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;les-services-de-données-spécialisés&#34;&gt;Les services de données spécialisés&lt;/h2&gt;
&lt;h3 id=&#34;img10126-icon-service-data-factorypng-azure-data-factory&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10126-icon-service-Data-Factory.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10126-icon-service-Data-Factory_hu8e9af3290ac174282009dd85995d7eea_896_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10126-icon-service-Data-Factory_hu8e9af3290ac174282009dd85995d7eea_896_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Data Factory&lt;/h3&gt;
&lt;p&gt;Azure Data Factory est un outil de type ETL/ELT (Extract Transform Load / Extract Load Transform), il permet de réaliser des déplacements de données d&amp;rsquo;un point à un autre en réalisant des transformations sur les données. Parmi les usages classiques, on retrouve :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alimentation de la zone Bronze d&amp;rsquo;un datalake avec des données en provenance de service cloud et de vos réseaux d&amp;rsquo;entreprise (via un programme installé sur vos serveurs)&lt;/li&gt;
&lt;li&gt;Transformation et structuration des données :
&lt;ul&gt;
&lt;li&gt;Création de fichiers structurés de type Parquet&lt;/li&gt;
&lt;li&gt;Re modélisation des données applicative vers un modèle en étoile&lt;/li&gt;
&lt;li&gt;Transformation de données semi-structurées de type JSON en données structuré de type Parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suivi de l&amp;rsquo;exécution des traitements de transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/data-factory/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img00606-icon-service-azure-synapse-analyticspng-azure-synapse-analytics&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/00606-icon-service-Azure-Synapse-Analytics.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/00606-icon-service-Azure-Synapse-Analytics_hu8e9af3290ac174282009dd85995d7eea_1467_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/00606-icon-service-Azure-Synapse-Analytics_hu8e9af3290ac174282009dd85995d7eea_1467_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Synapse Analytics&lt;/h3&gt;
&lt;p&gt;Azure Synapse Analytics est avant tout une suite d&amp;rsquo;outils comprenant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;un outil de type ETL/ELT, qui est simplement une version d&amp;rsquo;Azure Data Factory&lt;/li&gt;
&lt;li&gt;des moteurs de traitements de données :
&lt;ul&gt;
&lt;li&gt;SQL
&lt;ul&gt;
&lt;li&gt;Pools provisionnés : Vous louez des capacités de traitements SQL dédiés.&lt;/li&gt;
&lt;li&gt;Serverless : Vous payez vos traitements SQL à la requête.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apache Spark : le moteur de traitement de données Apache Spark qui permet de créer des traitements de données fortement scalable. Vous pouvez écrire des traitements dans différents langages (Python/R/Scala/Java) et les exécuter sur la plate-forme Spark.&lt;/li&gt;
&lt;li&gt;Data Explorer : un moteur de traitement spécialisé dans les traitements des logs et des séries temporelles.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;un outil de monitoring&lt;/li&gt;
&lt;li&gt;un espace de création regroupant tous les services ci-dessus.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Azure Synapse Analytics est le service phare de la suite Azure pour les traitements de données.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/synapse-analytics/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10787-icon-service-azure-databrickspng-azure-databricks&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10787-icon-service-Azure-Databricks.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10787-icon-service-Azure-Databricks_hu8e9af3290ac174282009dd85995d7eea_1519_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10787-icon-service-Azure-Databricks_hu8e9af3290ac174282009dd85995d7eea_1519_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Databricks&lt;/h3&gt;
&lt;p&gt;Azure Databricks est l&amp;rsquo;implémentation de Databricks dans Azure.&lt;br&gt;
Databricks est un outil basé sur Apache Spark tout en incluant des services complémentaires. Au même titre qu&amp;rsquo;Azure Synapse Analytics, il permet de créer des traitements d&amp;rsquo;ingestion et de préparation de données.&lt;/p&gt;
&lt;p&gt;Contrairement à Azure Synapse Analytics, Databricks est prévu pour être multicloud, vous retrouverez ce service dans AWS (Amazon) ou GCP (Google).&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/databricks/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10145-icon-service-azure-data-explorer-clusterspng-azure-data-explorer&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10145-icon-service-Azure-Data-Explorer-Clusters.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10145-icon-service-Azure-Data-Explorer-Clusters_hu8e9af3290ac174282009dd85995d7eea_1395_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10145-icon-service-Azure-Data-Explorer-Clusters_hu8e9af3290ac174282009dd85995d7eea_1395_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Data Explorer&lt;/h3&gt;
&lt;p&gt;Azure Data Explorer est un moteur de traitement spécialisé dans les traitements des logs et des séries temporelles.&lt;br&gt;
Microsoft a créé un langage de requête dédié nommé KQL (Kusto Query Language) permettant de plonger dans les puits de logs comme le commandant Cousteau dans la mer.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/data-explorer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;les-services-cognitifs&#34;&gt;Les services cognitifs&lt;/h2&gt;
&lt;h3 id=&#34;img10162-icon-service-cognitive-servicespng-azure-cognitive-services&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10162-icon-service-Cognitive-Services.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10162-icon-service-Cognitive-Services_hu8e9af3290ac174282009dd85995d7eea_1235_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10162-icon-service-Cognitive-Services_hu8e9af3290ac174282009dd85995d7eea_1235_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Cognitive Services&lt;/h3&gt;
&lt;p&gt;Azure Cognitive Services est un ensemble de service de type IA préentrainé que vous pouvez consommer directement sous forme d&amp;rsquo;API.&lt;br&gt;
Parmi les principaux services proposés, on retrouve :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La reconnaissance de texte&lt;/li&gt;
&lt;li&gt;La reconnaissance faciale&lt;/li&gt;
&lt;li&gt;La reconnaissance d&amp;rsquo;entité sur des images ou vidéos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vous pouvez intégrer ces services dans vos traitements de données pour enrichir ces dernières.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/cognitive-services/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10167-icon-service-machine-learning-studio-workspacespng-azure-machine-learning&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10167-icon-service-Machine-Learning-Studio-Workspaces.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10167-icon-service-Machine-Learning-Studio-Workspaces_hu8e9af3290ac174282009dd85995d7eea_868_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10167-icon-service-Machine-Learning-Studio-Workspaces_hu8e9af3290ac174282009dd85995d7eea_868_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Machine Learning&lt;/h3&gt;
&lt;p&gt;Azure Machine Learning est un outil de création d&amp;rsquo;expérience de Machine Learning, il prend en charge de bout en bout ce type de projet de la création du modèle à son exploitation en production.&lt;br&gt;
Les modèles ainsi créés pourront enrichir vos traitements de données.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;les-services-de-calcul-utilisable-pour-traiter-les-données&#34;&gt;Les services de calcul utilisable pour traiter les données&lt;/h2&gt;
&lt;h3 id=&#34;img10029-icon-service-function-appspng-azure-functions&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10029-icon-service-Function-Apps.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10029-icon-service-Function-Apps_hu8e9af3290ac174282009dd85995d7eea_1188_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10029-icon-service-Function-Apps_hu8e9af3290ac174282009dd85995d7eea_1188_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Functions&lt;/h3&gt;
&lt;p&gt;Azure Functions permet d&amp;rsquo;exécuter du code à la demande dans Azure, sans louer de ressources en permanence. Vous êtes facturé à l&amp;rsquo;usage.&lt;br&gt;
Vous pouvez créer des traitements s&amp;rsquo;exécutant à la demande, sur des évènements Azure ou de manière planifiée.&lt;br&gt;
Les principaux langages disponibles sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;li&gt;Java&lt;/li&gt;
&lt;li&gt;PowerShell&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;TypeScript&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Par défaut les Azure Functions sont limitées à 5 minutes d&amp;rsquo;exécutions, 10 maximum. Si vous avez besoin de plus regarder &lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=csharp-inproc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;les fonctions durables.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/functions/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;img10201-icon-service-logic-appspng-azure-logic-apps&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10201-icon-service-Logic-Apps.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10201-icon-service-Logic-Apps_hu8e9af3290ac174282009dd85995d7eea_949_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10201-icon-service-Logic-Apps_hu8e9af3290ac174282009dd85995d7eea_949_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Logic Apps&lt;/h3&gt;
&lt;p&gt;Azure Logic Apps est un outil de création de traitement low code. Il est très proche en termes d&amp;rsquo;interface et d&amp;rsquo;utilisation de Power Automate, mais contrairement à ce dernier vous payez les traitements à l&amp;rsquo;exécution, quels que soient les connecteurs utilisés.&lt;br&gt;
Vous pouvez créer des traitements s&amp;rsquo;exécutant à la demande, sur des évènements Azure ou de manière planifiée.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/logic-apps/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;autres-services-azure-à-intégrer&#34;&gt;Autres services Azure à intégrer&lt;/h2&gt;
&lt;h3 id=&#34;img10245-icon-service-key-vaultspng-azure-key-vault&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/10245-icon-service-Key-Vaults.png&#34;
	width=&#34;30&#34;
	height=&#34;30&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/10245-icon-service-Key-Vaults_hu7a6ed2925b8411b46ca180016908f0d9_1806_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/10245-icon-service-Key-Vaults_hu7a6ed2925b8411b46ca180016908f0d9_1806_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt; Azure Key Vault&lt;/h3&gt;
&lt;p&gt;Azure Key Vault est un coffre-fort pour vos mots de passe, est-il besoin de rappeler qu&amp;rsquo;un mot de passe n&amp;rsquo;a rien à faire en clair dans un code ?&lt;br&gt;
Azure Key Vault vous permettra de sécurisé vos mots de passe et autres secrets et le mettra à disposition des applications ayant le droit de les utiliser. Vos différentes briques de services Azure ou autre pourront ainsi utiliser des secrets en toute discrétion.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/key-vault/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;imgpurviewpng-microsoft-purview&#34;&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/purview.png&#34;
	width=&#34;30&#34;
	height=&#34;19&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/purview_huebae1058d42e27fb2cdbb3964620300c_1234_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/purview_huebae1058d42e27fb2cdbb3964620300c_1234_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt; Microsoft Purview&lt;/h3&gt;
&lt;p&gt;Microsoft Purview est en ensemble d&amp;rsquo;outils permettant de mettre en place une gouvernance de données globale. Il vous permet aussi de mettre en place un catalogue de données d&amp;rsquo;entreprise.&lt;/p&gt;
&lt;p&gt;Vous retrouverez les informations sur le produit &lt;a class=&#34;link&#34; href=&#34;https://azure.microsoft.com/fr-fr/products/purview/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture-type&#34;&gt;Architecture type&lt;/h2&gt;
&lt;p&gt;Tous les outils présentés ci-dessus peuvent travailler ensemble dans des architectures data moderne, ma préférée étant celle autour de Synapse pour un usage BI:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-compute/img/enterprise-bi-scoped-architecture.png&#34;
	width=&#34;1200&#34;
	height=&#34;846&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-compute/img/enterprise-bi-scoped-architecture_hu1f3b34477e815894fea8669db0f97484_250544_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-compute/img/enterprise-bi-scoped-architecture_hu1f3b34477e815894fea8669db0f97484_250544_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Décisionnel d’entreprise&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Vous trouverez les références de cette architecture &lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/fr-fr/azure/architecture/example-scenario/analytics/enterprise-bi-synapse&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt; et une version plus globale &lt;a class=&#34;link&#34; href=&#34;https://learn.microsoft.com/fr-fr/azure/architecture/example-scenario/dataplate2e/data-platform-end-to-end&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datalake : la base d&#39;un projet data moderne</title>
        <link>https://blog.ddata.fr/p/datalake-la-base/</link>
        <pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/datalake-la-base/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/datalake-la-base/ticka-kao-o87vASD6Ksk-unsplash.jpg" alt="Featured image of post Datalake : la base d&#39;un projet data moderne" /&gt;&lt;p&gt;Dans cet article je vous présente le concept de Datalake ou lac de données en bon français. Un datalake est la fondation d&amp;rsquo;un projet data moderne, vous le retrouverez dans la plupart de vos futurs projets.&lt;/p&gt;
&lt;h2 id=&#34;contexte&#34;&gt;Contexte&lt;/h2&gt;
&lt;p&gt;Comme je vous l&amp;rsquo;ai présenté &lt;a class=&#34;link&#34; href=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/&#34; &gt;dans ce précédent article&lt;/a&gt;, le datalake est un élément clé des projets data moderne. Je me propose maintenant de vous présenter plus en détail ce qu&amp;rsquo;est un datalake, comment on le structure et comment on peut s&amp;rsquo;en servir.&lt;br&gt;
Contrairement à ce que l&amp;rsquo;on pourrait croire, les datalakes ne sont pas réservés aux gros projets de data et leur philosophie d&amp;rsquo;usage peut-être reprise dans tout projet data.&lt;/p&gt;
&lt;h2 id=&#34;le-stockage&#34;&gt;Le stockage&lt;/h2&gt;
&lt;p&gt;Lorsque l&amp;rsquo;on parle datalake on parle avant tout d&amp;rsquo;un stockage de données pouvant accepter tout type de format de données.&lt;br&gt;
En général lorsque l&amp;rsquo;on parle de datalake on pense stockage cloud, l&amp;rsquo;avantage de ce type de stockage est sa capacité à être étendue en fonction de vos besoins et de vos finances. Bien que cette solution est la plus souple, on peut très bien créer un datalake en dehors du cloud.&lt;br&gt;
Les principaux types de stockages pour vos datalakes peuvent être :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En cloud dans l&amp;rsquo;écosystème Microsoft
&lt;ul&gt;
&lt;li&gt;Azure Datalake Gen 2&lt;/li&gt;
&lt;li&gt;Sharepoint&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sur vos infrastructures locales
&lt;ul&gt;
&lt;li&gt;Un partage réseau&lt;/li&gt;
&lt;li&gt;Un disque dur&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lorganisation&#34;&gt;L&amp;rsquo;organisation&lt;/h2&gt;
&lt;p&gt;Une fois la solution de stockage choisie, il est indispensable de réfléchir à l&amp;rsquo;organisation de votre datalake.&lt;br&gt;
Sans rigueur votre lac de données deviendra un marais de données (dataswamp).&lt;/p&gt;
&lt;h3 id=&#34;les-zones&#34;&gt;Les zones&lt;/h3&gt;
&lt;p&gt;Bien que chacun puisse créer l&amp;rsquo;organisation qu&amp;rsquo;il souhaite, on retrouve en général les 3 zones suivantes :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-la-base/img/001-datalake-zone.png&#34;
	width=&#34;799&#34;
	height=&#34;230&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-la-base/img/001-datalake-zone_hu727fac40be1e93915e54196ea9c46c88_7320_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-la-base/img/001-datalake-zone_hu727fac40be1e93915e54196ea9c46c88_7320_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Les zones d&amp;rsquo;un datalake&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;347&#34;
		data-flex-basis=&#34;833px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bronze : Cette zone sert de landing zone pour les données, elle permet d&amp;rsquo;écrire les données brutes reçues dans le datalake. Aucune transformation n&amp;rsquo;est réalisée sur les données.&lt;/li&gt;
&lt;li&gt;Silver : cette zone permet de raffiner les données de la zone Bronze et d&amp;rsquo;effectuer les traitements de préparation de données parmi lesquels on retrouve :
&lt;ul&gt;
&lt;li&gt;La transformation de données en un format de table avec
&lt;ul&gt;
&lt;li&gt;Le nommage des colonnes&lt;/li&gt;
&lt;li&gt;Le typage des colonnes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;La détection des anomalies&lt;/li&gt;
&lt;li&gt;La détection des problèmes de qualité&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gold : cette zone contient les données avec le plus haut niveau de raffinage, on retrouve par exemple
&lt;ul&gt;
&lt;li&gt;Des données modélisées pour le reporting (modèle en étoile).&lt;/li&gt;
&lt;li&gt;Des données préaggrégées.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le nom de ces zones peut varier, mais leur usage reste. D&amp;rsquo;autres zones peuvent être créées en fonction de vos besoins spécifiques, par exemple une zone sandbox pour des expérimentations.&lt;/p&gt;
&lt;p&gt;Les coûts de stockage étant relativement bas, en général, les traitements permettant le raffinage des données d&amp;rsquo;une zone à l&amp;rsquo;autre ne suppriment pas les données traitées. Ainsi si vous traitez des données sources en utilisant 10 champs sur 50 dans le fichier brut et que vos besoins évoluent, vous aurez toujours la possibilité de retraiter l&amp;rsquo;ensemble des données brut pour répondre à vos nouveaux besoins.&lt;/p&gt;
&lt;h3 id=&#34;les-partitions&#34;&gt;Les partitions&lt;/h3&gt;
&lt;p&gt;Le partitionnement des données va vous permettre de ranger vos données de manière précise. Quand on parle de partitionnement, il s&amp;rsquo;agit simplement de mettre vos fichiers dans des dossiers et sous dossier différent. L&amp;rsquo;avantage du partitionnement est notamment de pouvoir accéder rapidement à des données en ne lisant que les fichiers présents dans les partitions qui nous intéressent.&lt;br&gt;
Vous pouvez mixer les partitions comme vous le souhaitez.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/datalake-la-base/img/002-datalake-partition.png&#34;
	width=&#34;774&#34;
	height=&#34;436&#34;
	srcset=&#34;https://blog.ddata.fr/p/datalake-la-base/img/002-datalake-partition_hu0016a9332b0f3ada4911fd5db105c496_15833_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/datalake-la-base/img/002-datalake-partition_hu0016a9332b0f3ada4911fd5db105c496_15833_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Exemple de partitions&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;En général, on crée des partitions en créant des  dossiers sous la forme &lt;em&gt;NomPartition=ValeurPartition&lt;/em&gt;.&lt;br&gt;
Les principaux partitionnements que vous utiliserez sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Par source de données avec par exemple des dossiers sous la forme &lt;em&gt;/service=rh/application=monapplication&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Par version de traitement ou de format de fichier avec par exemple des dossiers sous la forme &lt;em&gt;/version=2.0&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Par date avec par exemple des dossiers sous la forme &lt;em&gt;/year=2023/month=02/day=26&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;les-principaux-formats-de-données&#34;&gt;Les principaux formats de données&lt;/h2&gt;
&lt;p&gt;Votre datalake va contenir des fichiers, voici quelque format classique que vous y trouverez :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parquet : C&amp;rsquo;est le format roi des datalakes, il s&amp;rsquo;agit d&amp;rsquo;un format de fichier dédier aux données analytiques, il conserve le nom et le type de données des colonnes et compresse les données contenues dans le fichier.&lt;/li&gt;
&lt;li&gt;CSV : Format classique d&amp;rsquo;extraction de données, il contient le nom des colonnes, mais les séparateurs utilisés peuvent varier en fonction de l&amp;rsquo;origine régionale du fichier (les séparateurs US et FR sont différents).&lt;/li&gt;
&lt;li&gt;Excel : Est-ce besoin de le présenter ? Un incontournable dans tous projets data.&lt;/li&gt;
&lt;li&gt;json, yaml, xml : Il s&amp;rsquo;agit en général de fichiers issus de traitement automatisé tel des API. Ces fichiers sont semi-structuré.&lt;/li&gt;
&lt;li&gt;log : les fichiers de log pourront être stockés longtemps et conserver une possibilité de traitement dans un datalake. Ces fichiers sont en général semi-structuré.&lt;/li&gt;
&lt;li&gt;Binaire : les vidéos et images sont des formats de données binaires, ils peuvent contenir certaines métadonnées exploitables directement (coordonnée GPS pour une photo par exemple), mais en général un traitement de type IA et nécessaire pour qualifier le contenu du fichier.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;la-sécurité&#34;&gt;La sécurité&lt;/h2&gt;
&lt;p&gt;La sécurité est un point clé de votre datalake, elle est portée par les possibilités offertes par la solution de stockage que vous avez choisie.&lt;br&gt;
Dans tous les cas, penser à créer plusieurs datalakes si vous avez des besoins forts de sécurité, une séparation des données peut prémunir contre des niveaux de droits inadéquats accordés à certain utilisateur.&lt;/p&gt;
&lt;h2 id=&#34;traitement-des-données&#34;&gt;Traitement des données&lt;/h2&gt;
&lt;p&gt;Le traitement des données d&amp;rsquo;un datalake fera l&amp;rsquo;objet d&amp;rsquo;un prochain article.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Datawarehouse ou datalakehouse, quelle source pour vos reportings ?</title>
        <link>https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/</link>
        <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/</guid>
        <description>&lt;img src="https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/antoine-petitteville-RIdYHUISNuM-unsplash.jpg" alt="Featured image of post Datawarehouse ou datalakehouse, quelle source pour vos reportings ?" /&gt;&lt;p&gt;Dans cet article, je vous présente les différentes sources possibles pour vos reportings. Les solutions natives de vos applications, un datawarehouse ou un datalakehouse ? Je vous propose de passer en revue ces solutions.&lt;/p&gt;
&lt;h2 id=&#34;le-reporting-de-données-sans-la-bi&#34;&gt;Le reporting de données sans la BI&lt;/h2&gt;
&lt;p&gt;Vos applications sont en général fournies avec des possibilités de reporting dédié fournies par l&amp;rsquo;éditeur de la solution.&lt;/p&gt;
&lt;p&gt;Les avantages de ces solutions de reporting natives sont principalement :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le coût normalement inclut dans la solution.&lt;/li&gt;
&lt;li&gt;Des rapports en lien très fort avec les données de la solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Par contre les inconvénients sont en général :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Une faible capacité de customisation et d&amp;rsquo;intégration de règle spécifique à votre usage.&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;impossibilité de croiser avec d&amp;rsquo;autres sources de données extérieures à la solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si l’on faisait une analogie avec la cuisine, nous avons affaire à un plat industriel prêt à être consommé.&lt;/p&gt;
&lt;p&gt;On peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui délivre des rapports aux utilisateurs via des outils de reporting différents les uns des autres.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/001-data-sans-bi.png&#34;
	width=&#34;601&#34;
	height=&#34;706&#34;
	srcset=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/001-data-sans-bi_hu8a19905e549f9e1eaa5a52d8f8030dd9_33837_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/001-data-sans-bi_hu8a19905e549f9e1eaa5a52d8f8030dd9_33837_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Le reporting sans BI&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;85&#34;
		data-flex-basis=&#34;204px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;la-bi-à-la-rescousse-&#34;&gt;La BI à la rescousse !&lt;/h2&gt;
&lt;p&gt;Comment faire mieux que cette solution de données en silo ? La réponse est simplement l&amp;rsquo;informatique décisionnelle souvent appelée BI acronyme de son nom anglais Business Inteligence.&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;objectif de la BI est de vous permettre d&amp;rsquo;utiliser un seul outil pour tous vos reportings, basé sur une source de données consolidées permettant le croisement des données de vos différents silos de données grâce à la mise en relation des données communes.&lt;/p&gt;
&lt;h2 id=&#34;les-services-attendus-dune-couche-data&#34;&gt;Les services attendus d&amp;rsquo;une couche data&lt;/h2&gt;
&lt;p&gt;Pour réaliser cet objectif, les solutions proposées par la BI ont évoluée dans le temps, mais les services attendus par cette couche intermédiaire reste les mêmes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Capacité de traitement et de stockage dédié distinct de l&amp;rsquo;infrastructure de l&amp;rsquo;application&lt;/li&gt;
&lt;li&gt;Centralisation des données&lt;/li&gt;
&lt;li&gt;Modélisation des données optimisée pour le reporting (modèle en étoile)&lt;/li&gt;
&lt;li&gt;Historisation des données
&lt;ul&gt;
&lt;li&gt;Pour certaine dimension, il est nécessaire de conserver la trace des changements, on parle en général de dimension à variation lente (Slow changing dimension).&lt;/li&gt;
&lt;li&gt;Pour certaines tables de faits, comme les stocks, on souhaite conserver des photos à un instant T des données afin de les assembler en un film de données permettant de suivre l&amp;rsquo;évolution des changements dans le temps.&lt;/li&gt;
&lt;li&gt;Réaliser des transformations des données sources comme par exemple passées de données cumulées (non agrégable) en données journalière (agrégable).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cette liste n&amp;rsquo;est pas exhaustive des services attendus par les datatrucs.&lt;/p&gt;
&lt;h2 id=&#34;les-datawarehouses&#34;&gt;Les datawarehouses&lt;/h2&gt;
&lt;p&gt;La solution que l&amp;rsquo;on retrouve le plus fréquemment dans les solutions BI historiques des entreprises est le datawarehouse ou entrepôt de données en bon français.&lt;/p&gt;
&lt;p&gt;Un datawarehouse est basiquement une base de données dédiée au reporting. Cette base de données est chargée via un ETL (acronyme d&amp;rsquo;Extract Transform Load). Le chargement est en général réalisé par des traitements en mode batch, en opposition à un traitement en temps réel, qui charge les données à une fréquence prédéfinies, classiquement une fois par jour.&lt;/p&gt;
&lt;p&gt;Les données subissent des transformations lors du chargement afin que les tables résultantes soient optimisées pour le reporting, on passe en général d&amp;rsquo;une modélisation OLTP (celle de l&amp;rsquo;application) à une modélisation en étoile.&lt;/p&gt;
&lt;p&gt;Les datawarehouses sont des projets techniques en général réalisés par l&amp;rsquo;IT de l&amp;rsquo;entreprise.&lt;/p&gt;
&lt;p&gt;Si l’on faisait une analogie avec la cuisine, nous avons affaire à un plat fait maison, mais l&amp;rsquo;œuf utilisé pour la tarte ne peut plus servir à faire des crêpes.&lt;/p&gt;
&lt;p&gt;On peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui sont chargées dans une base intermédiaire dédié au reporting, le datawarehouse.&lt;br&gt;
Le reporting peut être réalisé par un outil unique  ou pas en fonction des besoins des utilisateurs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/002-datawarehouse.png&#34;
	width=&#34;578&#34;
	height=&#34;675&#34;
	srcset=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/002-datawarehouse_hu65ec1f30ab54919c049d7a2243557acb_25785_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/002-datawarehouse_hu65ec1f30ab54919c049d7a2243557acb_25785_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Le reporting avec un datawarehouse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;85&#34;
		data-flex-basis=&#34;205px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;les-datalakehouses&#34;&gt;Les datalakehouses&lt;/h2&gt;
&lt;p&gt;Les datawarehouses ont certains inconvénients et ne sont pas optimisés pour répondre à certains besoins modernes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les données étant stockées dans une base de données, ils supportent très mal les données non structurées et les changements de structure des données dans le temps.&lt;/li&gt;
&lt;li&gt;Les données de type streaming (données en temps réel comme un capteur ou flux de données twitter) sont difficiles à intégrer dans un datawarehouse.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pour répondre à ces besoins modernes tout en gardant les services fournis par les datawarehouses, la BI moderne propose comme réponse les datalakehouses.&lt;/p&gt;
&lt;p&gt;Un datalakehouse est la fusion des datalakes et des datawarehouses. J&amp;rsquo;ai expliqué ce qu&amp;rsquo;est un datawarehouse ci-dessus, je vais donc expliqué le concept de datalake.&lt;/p&gt;
&lt;p&gt;Les datalakes sont la réponse au stockage de données répondant aux critères suivants :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Il accepte les formats multiples
&lt;ul&gt;
&lt;li&gt;Données structurées en provenance des bases de données&lt;/li&gt;
&lt;li&gt;Données semis structuré comme des fichiers Excel, JSON, CSV&lt;/li&gt;
&lt;li&gt;Données non structurées comme des images ou vidéos, qui pourront subir des traitements de type IA pour en extraire de l&amp;rsquo;information exploitable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Il supporte des fréquences de chargement variables allant du mode batch de datawarehouse au temps réel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ce stockage permettant tous ces cas d&amp;rsquo;usages n&amp;rsquo;est ni plus ni moins qu&amp;rsquo;un stockage avec les mêmes services qu&amp;rsquo;un disque dur, mais disponible en général dans le cloud.&lt;/p&gt;
&lt;p&gt;Le plus important avec un datalake est la rigueur avec laquelle on range ces données afin de ne pas finir avec un dataswamp ou marais de données.&lt;/p&gt;
&lt;p&gt;Un datalakehouse est donc une architecture de raffinage de données composé d&amp;rsquo;un datalake et de divers outils permettant le raffinage des données. Différents outils peuvent être utilisés aux différentes phases de raffinage en fonction des besoins.&lt;/p&gt;
&lt;p&gt;Les principales étapes de raffinage des données sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L&amp;rsquo;alimentation du datalake avec les données brutes, sans transformation, dans une zone que l&amp;rsquo;on nomme couramment &lt;em&gt;Bronze&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Le raffinage des données brut à l&amp;rsquo;aide de divers traitements en fonction des besoins, on stocke le résultat dans une zone que l&amp;rsquo;on nomme couramment &lt;em&gt;Silver&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;La mise à disposition des données de reporting dans une zone que l&amp;rsquo;on nomme couramment &lt;em&gt;Gold&lt;/em&gt;. Les données peuvent par exemple être préaggrégées pour améliorer le reporting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On conserve les données de chaque étape ainsi on peut, si besoin, ajouter de nouveaux traitements sur d&amp;rsquo;anciennes données.&lt;br&gt;
Les données du datalake du dalakehouse seront donc disponibles pour d&amp;rsquo;autres usages tels que :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Du machine learning.&lt;/li&gt;
&lt;li&gt;Un référentiel d&amp;rsquo;entreprise mis à disposition d&amp;rsquo;autres applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si l’on faisait une analogie avec la cuisine, nous avons affaire à un garde-manger où on en prend les éléments que l&amp;rsquo;on souhaite cuisiner en fonction des besoins.&lt;/p&gt;
&lt;p&gt;On peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui sont chargées dans un stockage intermédiaire sans usage spécifique puis préparé en fonction des besoins, notamment le reporting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/003-datalakehouse.png&#34;
	width=&#34;589&#34;
	height=&#34;754&#34;
	srcset=&#34;https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/003-datalakehouse_hu24dae463978d3b0f2f8d47f596271f5e_31506_480x0_resize_box_3.png 480w, https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/img/003-datalakehouse_hu24dae463978d3b0f2f8d47f596271f5e_31506_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Le reporting avec un datalakehouse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;78&#34;
		data-flex-basis=&#34;187px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;J&amp;rsquo;espère que maintenant vous comprenez mieux l&amp;rsquo;utilisation de ces différents datatrucs.&lt;/p&gt;
&lt;p&gt;Merci de votre attention.&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
