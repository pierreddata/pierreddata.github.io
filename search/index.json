[{"content":"Dans cet article je me propose de vous faire découvrir Snowflake et de créer un compte gratuit pour essayer ses fonctionnalités.\nContexte Qu\u0026rsquo;est-ce que Snowflake ?\nSnowflake est un système de gestion de vos données dans le cloud en mode PaaS.\nIl est adapté pour gérer des données structurées et non structurées. Il a la caractéristique d\u0026rsquo;être multi cloud provider, AWS, Azure et GCP peuvent héberger vos projets Snowflake.\nSnwoflake est un outil ayant atteint un bon niveau de maturité, qui peut être le cœur de votre dataplateforme.\nLes principaux concurrents Les principaux concurrents de Snowflake sont :\nRedshift chez Amazon. Azure Synapse et Microsoft Fabric chez Microsoft. BigQuery chez Google. Databricks et Spark peuvent aussi être vus comme des compétiteurs. Les fonctionnalités de Snowflake pour une data plateforme Voici à mon sens les principales fonctionnalités que Snowflake apportera à votre data plateforme :\nBase de données scalable, c\u0026rsquo;est le principal intérêt des bases de données dans le Cloud, elles sont scalable sans limites techniques, par contre la limite financière elle reste bien réelle. La séparation de stockage et du calcul permet de ne payer votre moteur de calcul (warehouse) que lorsque vous vous en servez. Le multi warehouse, vous permet d\u0026rsquo;avoir autant de moteurs de calcul que nécessaire pour une même base ou plusieurs bases. La scalabilité des moteurs de calcul à la demande vous permet de changer la puissance de calcul en fonction de vos besoins. Le data time travelling vous permet de voir l\u0026rsquo;état de vos données dans le passé. Le zéro copy clone, vous permet de cloner un objet, table, schéma ou base de données, instantanément. Cela vous permet de créer vos environnements de développement ou pré production à moindre effort. Le paiement à l\u0026rsquo;utilisation permet d\u0026rsquo;avoir une tarification prévisible de vos dépenses. Le dynamique data masking vous permet d\u0026rsquo;afficher certaines données masquées totalement ou partiellement en fonction de vos utilisateurs. La possibilité de gérer finement la sécurité afin de contrôler qui voit quoi. Avec des droits par objet Une intégration SSO avec Azure AD Créer un compte pour tester Snowflake Pour créer un compte de démonstration Snowflake, il vous suffit d\u0026rsquo;une adresse email, vous pouvez bien sûr créer des adresses de tests, comme par exemple dans un environnement Microsoft 365 gratuit.\nPensez a créer un profil dans votre navigateur si vous utilisez une autre adresse email que votre adresse principale.\nVotre compte gratuit fonctionnera jusqu\u0026rsquo;à l\u0026rsquo;atteinte d\u0026rsquo;une de ces 2 limites :\nConsommation de 400 $. Durée de 30 jours. Rendez-vous sur le site de Snowflake.\nDans le coin, en haut à droite, sélectionnez commencer gratuitement.\nSaisissez les informations demandées. Appuyez sur continuer pour passer à l\u0026rsquo;étape suivante. Sélectionner le niveau de service de Snowflake, pour avoir toutes les possibilités listées ci-dessus, le niveau Entreprise est indispensable. Sélectionner le cloud provider qui hébergera votre instance de Snowflake. Une fois cette sélection faite, choisissez la région chez le fournisseur dans la liste qui apparait en dessous. Validez les conditions générales. Appuyez sur démarrer pour passer à l\u0026rsquo;étape suivante. Sur les écrans suivants, vous pouvez répondre à un questionnaire ou ignorer les questions.\nSur le dernier écran, snowflake vous informe vous avoir envoyer un lien d\u0026rsquo;activation dans votre boite email afin de finaliser l\u0026rsquo;inscription. Cliquez sur le lien dans le mail.\nL\u0026rsquo;étape suivante est cruciale, vous allez définir le compte de l\u0026rsquo;administrateur du compte Snowflake. Ne perdez pas ces informations !\nDonnez le nom d\u0026rsquo;utilisateur Choisissez et confirmez un mot de passe fort. Appuyez sur get started. Bravo vous avez créer votre compte Snowflake.\nPrenez le temps de copier l\u0026rsquo;adresse du site dans votre navigateur, car elle contient des informations cruciales. Elle se présente sous la forme :\nhttps://app.snowflake.com/XXXXXX/YYYYYY/worksheets oi XXXXXX : est votre code d\u0026rsquo;organisation Snowflake. YYYYYY : est votre code de compte dans votre organisation. Ces codes vous seront utiles par la suite. Je vous présenterai les bases de Snowflake dans un prochain article.\nMerci de votre attention.\n","date":"2023-09-17T00:00:00Z","image":"https://blog.ddata.fr/p/tester-snowflake/nathan-wolfe-rtMiBkMCOsw-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_1418558_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/tester-snowflake/","title":"Créer un compte gratuit pour tester Snowflake"},{"content":"Connaissez-vous ces quelques raccourcis clavier très utiles ?\nContexte Lorsque j\u0026rsquo;observe les utilisateurs en informatique, je constate fréquemment que la connaissance de quelques raccourcis clavier pourrait leur faire gagner du temps et de la maitrise.\nJe vous propose de vous présenter ici ce qui me semble le plus important dans l\u0026rsquo;écosystème Microsoft Windows.\nLes raccourcis clavier doivent être réalisés de manière séquentielle, par exemple pour copier un texte sélectionné vous connaissez le {Control} + c. Vous appuyez sur la touche contrôle, vous la maintenez appuyée et appuyer ensuite sur la touche c.\nDans les exemples ci-dessous, j\u0026rsquo;indiquerais les touches spéciales entre accolades {}.\nVerrouiller sa session La sécurité informatique commence par le verrouillage de votre session quand vous quittez votre poste de travail. Vous pouvez le faire rapidement grâce au raccourci clavier {Windows} + l.\nAvec l comme lock.\nDirection le bureau Vous utilisez votre bureau et vous avez besoin d\u0026rsquo;y accéder rapidement ? Utilisez le raccourci clavier {Windows} + d.\nAvec d comme desktop.\nBien déplacer son curseur Voici une série de raccourcis clavier que j\u0026rsquo;utilise tous les jours, il me permette de me déplacer efficacement partout où une saisie de texte est demandée. Cela fonctionne aussi bien dans les formulaires de votre navigateur web, votre traitement de texte ou votre éditeur de code.\nDans Word et autres outils de saisie Déplacez-vous de mot en mot avec la flèche droite et la flèche gauche en maintenant la touche contrôle enfoncée.\nVoici 2 autres raccourcie pour ce déplacer efficacement :\n{Control} + {Début} pour aller directement au début du document {Control} + {Fin} pour aller directement au fin du document La touche {Début} ce trouve en général à coté de la touche {Insér.}, elle est représenter par une flèche diagonnal pointant en haut à gauche.\nLa touche {Fin} ce trouve en général à coté de la touche {Suppr.}, en général il est écrit Fin dessus.\nDans Excel La touche {Control} avec les flèches de directions permet dans Excel de sauter rapidement entre les cellules non vide. Si vous êtes sur une cellule vide vous allez directement à la prochaine cellule non vide dans la direction choisie. Si vous êtes sur une cellule non vide, vous allez directement à la dernière cellule non vide dans la direction choisi.\nSélectionner précisément du texte Dans Word et autres outils de saisie Faites comme pour le déplacement de mot en mot, mais ajoutez la touche {Majuscule} enfoncée.\nDans Excel Les touches {Majuscule} + {Control} avec les flèches de directions permet dans Excel de sélectionner rapidement des plages de cellules.\nMettre en commentaire Pas besoin de ce souvenir des symboles utilisés pour faire des commentaires dans les différents langages. Il suffit de connaitre ce raccourcie qui fonctionne dans les outils Microsoft, notamment Power BI, pour l\u0026rsquo;éditeur Power Query et l\u0026rsquo;éditeur DAX, et vscode.\n{Control} + k ; {Control} + c {Control} + k ; {Control} + u Moyen mnémotechnique pour s\u0026rsquo;en souvenir : c = comment et u = uncomment.\nCette combinaison est efficace mais plus dificile à réalisé car vous devevez réaliser 2 séquence de raccourcie à la suite. La touche {Control} peut être maintenu appuyé ou non entre les 2 séquences.\nPrenez le temps de vous entrainez à maitriser votre clavier vous pouvez gagner beaucoup de temps et de maitrise.\nMerci de votre attention.\n","date":"2023-07-27T00:00:00Z","image":"https://blog.ddata.fr/p/bien-utiliser-son-clavier/girl-with-red-hat-gL3-jTs_Q7g-unsplash_hu86a9728bd947d53586706dc716952899_2687596_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/bien-utiliser-son-clavier/","title":"Un raccourci clavier peu vous faire gagner beaucoup de temps"},{"content":"Microsoft Fabric Data Activator, en mode IOT\nContexte Dans l\u0026rsquo;article Microsoft Fabric Data Activator, alertes avancées dans Power BI, je vous ai présenté Data Activator dans le contexte d\u0026rsquo;un rapport Power BI.\nJe vous propose maintenant de voir Data Activator dans un contexte plus complexe et pour cela nous allons repartir de l\u0026rsquo;exemple d\u0026rsquo;utilisation de Fabric présenté dans mon article Données en temps réel dans votre rapport Power BI avec Microsoft Fabric.\nPréparation de l\u0026rsquo;environnement À la suite de l\u0026rsquo;article Données en temps réel dans votre rapport Power BI avec Microsoft Fabric l\u0026rsquo;environnement de travail est prêt à être utilisé pour exploiter Data Activator.\nOn va modifier légèrement le PowerShell qui simule un IOT afin que ce dernier envoie des informations pendant plus longtemps, mais à une fréquence moins importante.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # Votre chaine de connexion $ConnectionString = \u0026#39;REMPLACER_PAR_VOTRE_CHAINE_DE_CONNEXION\u0026#39; # Nombre de message à envoyer $Messages = 2000 # Id des capteurs $HardwareID = \u0026#34;12345\u0026#34;, \u0026#34;23456\u0026#34;, \u0026#34;34567\u0026#34;,\u0026#34;45678\u0026#34;,\u0026#34;67890\u0026#34; $Pattern = \u0026#39;Endpoint=(.+);SharedAccessKeyName=(.+);SharedAccessKey=(.+);EntityPath=(.+)\u0026#39; ([uri]$Endpoint),$PolicyName,$Key,$Queue = ($ConnectionString -replace $Pattern,\u0026#39;$1;$2;$3;$4\u0026#39;) -split \u0026#39;;\u0026#39; # Depending on your environment you may need to load the assembly System.Web # $null = [Reflection.Assembly]::LoadWithPartialName(\u0026#34;System.Web\u0026#34;) $UrlEncodedEndpoint = [System.Web.HttpUtility]::UrlEncode($Endpoint) $Expiry = [DateTimeOffset]::Now.ToUnixTimeSeconds() + 3600 $RawSignatureString = \u0026#34;$UrlEncodedEndpoint`n$Expiry\u0026#34; $HMAC = New-Object System.Security.Cryptography.HMACSHA256 $HMAC.Key = [Text.Encoding]::ASCII.GetBytes($Key) $HashBytes = $HMAC.ComputeHash([Text.Encoding]::ASCII.GetBytes($RawSignatureString)) $SignatureString = [Convert]::ToBase64String($HashBytes) $UrlEncodedSignatureString = [System.Web.HttpUtility]::UrlEncode($SignatureString) $SASToken = \u0026#34;SharedAccessSignature sig=$UrlEncodedSignatureString\u0026amp;se=$Expiry\u0026amp;skn=$PolicyName\u0026amp;sr=$UrlEncodedEndpoint\u0026#34; # Boucle d\u0026#39;envoi des messages for ($i = 0; $i -lt $Messages; $i++) { # Création du message à envoyer au format JSON $IOTFakeMessage = @{ HardwareId = \u0026#34;ID-\u0026#34; + $HardwareID[(Get-Random -Minimum 0 -Maximum $HardwareID.Count)] Date = Get-Date CaptorInfo = @{ Temp = Get-Random -Minimum 20 -Maximum 28 Hum = Get-Random -Minimum 40 -Maximum 60 } } | ConvertTo-Json -Depth 99 # Préparation de la requête HTTP $Params = @{ Uri = \u0026#34;https://$($Endpoint.Host)/$Queue/messages\u0026#34; ContentType = \u0026#39;application/json;charset=utf-8\u0026#39; Method = \u0026#39;Post\u0026#39; Body = $IOTFakeMessage Headers = @{ \u0026#39;Authorization\u0026#39; = $SASToken } } # Envoi du message vers le service BUS Invoke-RestMethod @Params # Attente avant le prochain message Start-Sleep -Milliseconds (Get-Random -Minimum 2000 -Maximum 5000) } Maintenant on envoie 2000 messages avec un délai entre 2 et 5 secondes entre chaque message.\nPenser à remplacer la chaine de caractère REMPLACER_PAR_VOTRE_CHAINE_DE_CONNEXION par la chaine de connexion (voir article précédent).\nRendez-vous dans l\u0026rsquo;espace de travail créer dans l\u0026rsquo;article précédent et ouvrez l\u0026rsquo;eventstream que l\u0026rsquo;on avait créé.\nSur le composant central, appuyez sur + pour ajouter une sortie. Sélectionnez Custom App. Entrez un nom de destination, ici DataActivator. Appuyez sur Add pour ajouter cette nouvelle destination. Maintenant le flux en entrée envoie les informations vers 2 destinations distinctes.\nSélectionnez la sortie que l\u0026rsquo;on vient de créer. Notez la valeur de Consumer group. Afficher la valeur de Connection string-primary key en appuyant sur l\u0026rsquo;icône oeil. Notez la valeur de Connection string-primary key. Création du réflexe Rendez-vous dans l\u0026rsquo;espace de travail.\nSélection de l\u0026rsquo;expérience utilisateur Afin de simplifier l\u0026rsquo;interface, Fabric est organisé autour de différentes expériences utilisateur afin de ne pas présenter l\u0026rsquo;ensemble des types objets disponible à tout le monde. Pour notre exemple nous pouvons utiliser l\u0026rsquo;expérience Data Activator.\nAppuyez sur l\u0026rsquo;icône en bas à gauche de votre écran, si vous êtes arrivé par le portail Power BI vous trouver l\u0026rsquo;icône Power BI, sinon vous trouverez l\u0026rsquo;icône de l\u0026rsquo;expérience utilisateur en cours. Sélectionnez Data Activator Après le changement d\u0026rsquo;expérience, vous devez vous repositionner dans votre espace de travail.\nCréation d\u0026rsquo;un réflexe Dans le menu de l\u0026rsquo;espace de travail, appuyez sur + Nouveau. Sélectionnez Preview ou Reflexe si le texte dans l\u0026rsquo;interface est corrigé 😊. Celui à prendre à une icône en éclair ⚡. La fenêtre de Réflexe apparait\nCréation de la capture de données Lancer le script PowerShell avant de continuer.\nAppuyez sur Get Data. Vérifiez que Event Hubs est sélectionné. Saisissez un nom, ici Demo IOT. Remplir la zone Connection string avec la valeur Connection string-primary key récupérée précédemment. Remplir la zone Consumer group avec la valeur Consumer group récupérée précédemment. Saisissez Date dans Time Field, cela correspond à la clé de temps du JSON renvoyer par le PowerShell (ligne 32 du script) Appuyez sur Connect. Les données arrivent Attendez un peu, les données commencent à arriver.\nVous pouvez voir l\u0026rsquo;event créer précédemment, un réflexe peut avoir plusieurs events. Vous voyez les données arriver en temps réel. En vert ce sont les données qui viennent juste d\u0026rsquo;arriver. Appuyez sur Design pour créer une alerte. Vous pouvez constater que l\u0026rsquo;on ne trouve pas d\u0026rsquo;objet dans la partie Design. Appuyez sur Data pour commencer à créer une alerte. Création d\u0026rsquo;une alerte complexe Nous allons commencer par créer un objet qui va nous permettre ensuite de créer des alertes.\nPositionnez-vous sur l\u0026rsquo;event créer. Pointez sur la colonne HardwareID et appuyez sur les 3 points pour ouvrir le menu. Sélectionnez + Create object pour créer un objet. Choisir un nom pour l\u0026rsquo;objet, IOT par exemple. Laissez cocher Go to design mode after creation. Appuyez sur Create pour finir la création de l\u0026rsquo;objet. Vous pouvez voir l\u0026rsquo;objet IOT que l\u0026rsquo;on vient de créer. L\u0026rsquo;objet a été créé sur l\u0026rsquo;event Demo IOT. On peut voir l\u0026rsquo;arrivée des données. On a un point pour chaque capture de données. Vous pouvez sélectionner les HardwareID que l\u0026rsquo;on monitor dans l\u0026rsquo;écran. Vous pouvez choisir la fenêtre de temps des données affichées par le graphique. Vous pouvez mettre en pause la mise à jour en temps réel. Appuyez sur New Property pour créer une propriété. Vous pouvez voir la nouvelle propriété. Renommez la propriété en Température. Sélectionner CaptorInfo.Temp dans la liste. Nous avons le résultat suivant :\nVous pouvez constater que l\u0026rsquo;on a maintenant des courbes correspondant aux valeurs de la température. Boutons vous permettant d\u0026rsquo;ajouter des étapes de traitement. Chaque étape est réalisée sur le résultat de l\u0026rsquo;étape précédente, vous avez donc la possibilité d\u0026rsquo;ajouter plusieurs étapes permettant de définir des critères plus ou moins complexe afin d\u0026rsquo;identifier les données pour lesquels vous souhaitez déclencher une alerte.\nÉtapes disponibles en preview Voici la liste des étapes disponible lors de la publication de cet article. Les choses ont surement évolué depuis, car nous sommes sur une version en Private Preview.\nDetect : permets de sélectionner les évènements de la série temporelle de manière unitaire sur un critère\nChanges : Conserve les évènements ayant subi un changement de la valeur. Change from : conserve les évènements ayant changé par rapport à une valeur de référence. Change to : conserve les évènements ayant atteint une valeur de référence. Is not equal : conserve les évènements différents d\u0026rsquo;une valeur de référence. Is equal : conserve les évènements égaux à une valeur de référence. Exits range : conserve les évènements qui sorte d\u0026rsquo;une plage de référence. Enter range : conserve les évènements qui entrent dans une plage de référence. Is less than : conserve les évènements inférieurs à une valeur de référence. Is greater than : conserve les évènements supérieurs à une valeur de référence. Become less than : conserve l\u0026rsquo;évènement devenant inférieur à une valeur de référence. Become greater than : conserve l\u0026rsquo;évènement devenant supérieur à une valeur de référence. Is false : conserve les évènements ayant une valeur faux. Is true : conserve les évènements ayant une valeur vraie. Become false : conserve l\u0026rsquo;évènement devenant faux. Become true : conserve l\u0026rsquo;évènement devenant vrai. Tous les évènements de Detect peuvent être regardés soit\nà chaque changement pour un nombre de changements sur une période de temps entre 1 minute et 24 heures. Summarize : permets de sélectionner des évènements de la série temporelle sur un critère de regroupement de plusieurs évènements contigus sur une plage de temps.\nMaximum over time : permets de calculer le maximum des valeurs d\u0026rsquo;un évènement sur une plage de temps. Minimum over time : permets de calculer le minimum des valeurs d\u0026rsquo;un évènement sur une plage de temps. Average over time : permets de calculer la moyenne des valeurs d\u0026rsquo;un évènement sur une plage de temps. Count : permets de compter le nombre d\u0026rsquo;évènements sur une plage de temps. Pour Summarize les fenêtres de temps des calculs sont entre 1 minute et 24 heures.\nFilter : permets de sélectionner des évènements sur un critère de filtre.\nFilter : permets un filtre numérique avec les opérateurs suivants Greater Greater or equal Less Less or equal Equal Not equal *Property filter : permets de filtrer par rapport à une autre propriété. Compare : permets de sélectionner des évènements sur un critère de filtre avec les opérateurs suivants :\nGreater Greater or equal Less Less or equal Equal Not equal La différence entre Filter et Compare, est que Filter renvoie la valeur de l\u0026rsquo;évènement alors que Compare renvoie une valeur vraie ou fausse.\nAct : permets de déclencher une action pour les évènements restant suite à l\u0026rsquo;application des filtres. Pour le moment seul un envoi d\u0026rsquo;email est disponible. À partir du moment où vous ajoutez une étape de type Act, la propriété est automatiquement transformée en Trigger.\nComme vous pouvez le voir, Data Activator va vous permettre de créer des alertes complexes sur vos données. Les étapes présentées ici sont bien entendu aussi disponibles quand la source de votre event est Power BI.\nMerci de votre attention.\n","date":"2023-07-09T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-data-activator-iot/chutipon-pattanatitinon-LG8Mt7f7lXQ-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_954281_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-data-activator-iot/","title":"Microsoft Fabric Data Activator, en mode IOT"},{"content":"Microsoft Fabric Data Activator va vous permettre de créer des alertes à partir de vos données Power BI.\nCe modèle offre plus de possibilités que le système d\u0026rsquo;alerte actuel via un tableau de bord décrit dans mon article Power BI : Créer une alerte sur vos données.\nContexte Data Activator est un composant de Microsoft Fabric qui permet de créer des alertes sur vos données.\nDans cet article, je vais supposer que vous avez déjà pris en main Microsoft Fabric et lu les précédents articles que j\u0026rsquo;ai écrits sur ce sujet. Je ne reviendrais donc pas sur les concepts déjà vus.\nData Activator est disponible uniquement en private preview au moment où j\u0026rsquo;écris cet article. Vous pouvez demander à activer cette fonctionnalité en allant sur ce lien https://aka.ms/dataActivatorPreview.\nPréparation de l\u0026rsquo;environnement Création de l\u0026rsquo;espace de travail Dans le menu de gauche, sélectionnez Espace de travail, puis + Nouvel espace de travail.\nNommez votre espace de travail Data Activator. Ouvrez les options avancées Sélectionnez un type d\u0026rsquo;espace de travail compatible avec les objets Micrsoft Fabric. Appuyez sur Appliquez pour créer votre espace de travail. Sélection de l\u0026rsquo;expérience utilisateur Afin de simplifier l\u0026rsquo;interface, Fabric est organisé autour de différentes expériences utilisateur afin de ne pas présenter l\u0026rsquo;ensemble des types objets disponible à tout le monde. Pour notre exemple nous pouvons utiliser l\u0026rsquo;expérience Data Activator.\nAppuyez sur l\u0026rsquo;icône en bas à gauche de votre écran, si vous êtes arrivé par le portail Power BI vous trouver l\u0026rsquo;icône Power BI, sinon vous trouverez l\u0026rsquo;icône de l\u0026rsquo;expérience utilisateur en cours. Sélectionnez Data Activator Après le changement d\u0026rsquo;expérience, vous devez vous repositionner dans votre espace de travail.\nCréation d\u0026rsquo;un lakeHouse Nous allons maintenant créer un lakehouse dédié à notre exemple.\nUn lakehouse est une espace de stockage acceptant n\u0026rsquo;importe quel type de données. Il sert de zone de stockage générique des données de notre couche bronze.\nDans l\u0026rsquo;écosystème Fabric un lakehouse est un sous-ensemble de OneLake, l\u0026rsquo;espace de stockage unifié de votre entreprise.\nDans le menu de l\u0026rsquo;espace de travail, appuyez sur + Nouveau. Sélectionnez Lakehouse. Saisissez le nom du lakehouse, ici DemoDataActivator qui est le nom de la source de données. Appuyez sur Créer pour créer le lakehouse. L\u0026rsquo;écran qui s\u0026rsquo;affiche est l\u0026rsquo;explorateur de lakehouse, nous allons créer une vue dans le point de terminaison SQL associé au lakehouse.\nRendez-vous dans la liste déroulante en haut à droite pour basculer en mode point de terminaison SQL. Ouvrez Schemas. Ouvrez dbo. Au niveau de Views sélectionnez les 3 points. Appuyez sur Nouvelle vue. Nous allons ajouter une vue SQL dont le résultat évolue toutes les secondes afin de simuler des données qui se rafraichissent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE OR ALTER VIEW [dbo].[DemoDataActivator] AS ( SELECT A.Couleur ,CONVERT(INT,(CONVERT(INT, FORMAT(GETDATE(),\u0026#39;HHmmss\u0026#39;)) * A.Coef)) AS Valeur FROM ( SELECT \u0026#39;Rouge\u0026#39; AS Couleur, 2.5 AS Coef UNION SELECT \u0026#39;Vert\u0026#39; AS Couleur, 1.5 AS Coef UNION SELECT \u0026#39;Bleu\u0026#39; AS Couleur, 1 AS Coef UNION SELECT \u0026#39;Noir\u0026#39; AS Couleur, 0.7 AS Coef ) A ) GO Dans la fenêtre de la vue, remplacer le code par défaut par le code de la vue disponible ci-dessus. Appuyez sur Run pour créer la vue dans le point de terminaison SQL. Vous pouvez voir que c\u0026rsquo;est un succès. Création d\u0026rsquo;un rapport Power BI Nous allons maintenant réaliser dans Power BI desktop, un rapport basé sur la vue créer dans le lakehouse.\nOuvrez Power BI Desktop. Je réalise mon rapport avec la version de juin 2023.\nDans le menu Accueil. Sélectionnez Hub de données OneLake. Appuyez sur Lakehouses. Sélectionnez le lakehouse DemoDataActivator. Appuyez sur le bouton avec le chevron vers le bas. Choisissez Se connecter au point de terminaison SQL. Sélectionner la vue DemoDataActivator. Vous pouvez voir un aperçu des données. Appuyez sur Charger. Sélectionnez DirectQuery. Appuyez sur OK. Ajoutez un visuel Graphiques à barres groupées. Ajoutez le champ Couleur, puis le champ Valeur. Faite de la mise en forme si vous voulez. Enregistrez le rapport en DemoDataActivatorDesktopDQOnSQLLakehouse. Publiez le rapport dans l\u0026rsquo;espace de travail Data Activator. Rendez-vous dans l\u0026rsquo;espace de travail Data Activator.\nPointez la souris sur le jeu de données DemoDataActivatorDesktopDQOnSQLLakehouse et appuyez sur les 3 points. Sélection Paramètres dans le menu. Rendez-vous dans la section Informations d\u0026rsquo;identification de la source de données. Appuyez sur Modifier les informations d\u0026rsquo;identification pour mettre à jour l\u0026rsquo;authentification du jeu de données sur le point de terminaison SQL. Sélectionnez OAuth2 dans Méthode d\u0026rsquo;authentification. Appuyez sur Se connecter. Rendez-vous dans la section Actualisation planifiée et optimisation des performances. Sélectionnez 15 minutes dans Fréquence d\u0026rsquo;actualisation. Appuyez sur Appliquer. Le terrain de jeu est maintenant fonctionnel, nous allons pouvoir créer une alerte dans Data Activator.\nRéflexe Data Activator, se présente comme un nouveau type d\u0026rsquo;objet nommé Réflexe dans l\u0026rsquo;espace de travail.\nC\u0026rsquo;est ce réflexe qui reçoit les informations et génère les alertes.\nCréation Nous allons maintenant créer un déclencheur. Lors de la création d\u0026rsquo;un déclencheur, on va choisir entre la création d\u0026rsquo;un réflexe et la réutilisation d\u0026rsquo;un réflexe existant.\nPour créer le réflexe, ouvrez le rapport publié dans le service.\nPointez votre souris sur le visuel pour faire apparaitre le menu contextuel, appuyez sur les 3 points. Sélectionnez Action de déclencheur. Sélectionnez la mesure à surveiller, si plusieurs mesures sont présentes dans le visuel elles apparaissent dans la liste déroulante. Sélectionnez l\u0026rsquo;axe de ventilation de la mesure à prendre en compte. Sélectionnez la fréquence de capture de l\u0026rsquo;information. Définissez un critère d\u0026rsquo;alerte Donnez l\u0026rsquo;espace de travail où sera stocké le réflexe. Donnez le nom du réflexe. Appuyez sur Continuer. Un message apparait pour vous confirmer la création de votre réflexe.\nAppuyez sur le lien pour ouvrir le réflexe créé. Vous pouvez aussi accéder au réflexe en cliquant sur l\u0026rsquo;objet créer dans l\u0026rsquo;espace de travail :\nPrincipe de fonctionnement ⚠ La description du fonctionnement présenté ici et celui de la private preview de juin 2023. Cela a pu évoluer depuis.\nUn réflexe se compose de 2 éléments :\nUn premier qui capture l\u0026rsquo;état de vos données à la fréquence définie lors de la création. Plusieurs processus de capture peuvent être définis au sein d\u0026rsquo;un même réflexe. Un outil pour analyser les données et déclencher une action en fonction du résultat de l\u0026rsquo;analyse. Plusieurs déclencheurs peuvent exister au sein d\u0026rsquo;un même réflexe. Les données de votre jeu de données capturer par réflexe deviennent automatiquement une série temporelle, et l\u0026rsquo;analyse que vous allez en faire sera contrainte par ce modèle de série temporelle.\nPour rappel une série temporelle est une valeur qui évolue dans le temps. Par exemple dans notre requête nous aurons :\nValeur pour bleu capturer le 01/07/2023 à 15:00:15 = 130010 Valeur pour bleu capturer le 01/07/2023 à 15:15:15 = 131510 \u0026hellip;.. Valeur pour bleu capturer le 01/07/2023 à 18:45:15 = 164510 Pour rappel la requête renvoie un format heures minutes secondes pour la valeur Bleu. L\u0026rsquo;heure du serveur est toujours en temps universel (UTC+0) donc avec 2 heures de moins que l\u0026rsquo;heure de Paris en juillet.\nLorsque l\u0026rsquo;on ouvre un réflexe, on arrive sur un écran avec 2 onglets :\nData : Permets de voir arriver les données capturées en temps réel. Design : Permets de définir les règles qui déclencheront des actions en fonction des données. Présentation des données capturées Panneau avec la liste des captures de données d\u0026rsquo;un réflexe. Permets de renommer une capture de données. Permets de filtrer la capture de données. Permets d\u0026rsquo;avoir des informations détaillées sur la capture de données. Permets de voir arriver les données capturées, la couleur verte permet d\u0026rsquo;identifier les données qui viennent d\u0026rsquo;arriver. On retrouve les valeurs en provenance de notre Power BI. Permets de passer de l\u0026rsquo;écran Data à l\u0026rsquo;écran Design. Personnaliser votre alerte Rendez-vous sur l\u0026rsquo;écran Design. Par défaut on arrive sur le déclencheur créer par défaut. Vous pouvez voir la série temporelle associée. Vous pouvez sélectionner les identifiants à surveiller, dans notre cas Bleu, Noir, Rouge et Vert. Vous pouvez sélectionner la fenêtre de temps à afficher. Vous pouvez mettre en pause le rafraichissement de la série temporelle. Vous pouvez accéder à l\u0026rsquo;écran de détail. Sélectionnez votre déclencheur. Il a été créé automatiquement via le rapport. Vous pouvez voir les points de collecte des données. On choisit de traiter les informations du champ Somme_de_Valeur, elle s\u0026rsquo;affiche dans un graphe. Vous pouvez constater que l\u0026rsquo;on a une mise à jour des données une fois toutes les 15 minutes, ce qui correspond au rafraichissement du jeu de données et non à la lecture des données de la vue comme on aurait pu s\u0026rsquo;y attendre. On défini un critère pour filtrer la série de l\u0026rsquo;étape précédente, avec comme critère devient plus grand que 170 000. Une seule occurrence est trouvée elle apparait dans le nouveau graphe. On définit les informations à envoyer par mail quand un évènement est détecté dans l\u0026rsquo;étape précédente. On peut démarrer ou tester le déclencheur. Ici se termine un premier tour d\u0026rsquo;horizon de Data Activator.\nRetour d\u0026rsquo;expérience Lors de mes tests, j\u0026rsquo;ai pu constater le comportement suivant en fonction du mode de connexion du jeu de données et de la configuration de rafraichissement :\nDirect query : Refresh de data activator est égale à la fréquence de refresh du dataset, pas de refresh sur le jeu de données par défaut du lakehouse. Import : : Refresh de data activator est égale à la fréquence de refresh du dataset. Direct Lake sur jeu de données par défaut : Pas de refresh sur le jeu de données par défaut du lakehouse. Merci de votre attention.\n","date":"2023-07-02T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-data-activator-power-bi/brandon-morgan-3qucB7U2l7I-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_384387_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-data-activator-power-bi/","title":"Microsoft Fabric Data Activator, alertes avancées dans Power BI"},{"content":"Comprenez le Query Folding et optimise vos requêtes Power Query.\nContexte Le query folding est un sujet trop souvent méconnu quand on débute Power Query. Ne vous inquiéter pas c\u0026rsquo;est relativement simple à comprendre et très utile à connaitre.\nSa traduction dans la documentation officielle sous le nom de pliage de requête est particulièrement incompréhensible.\nÊtes-vous concerné ? Le point important avec le query folding, c\u0026rsquo;est qu\u0026rsquo;il ne concerne pas toutes les sources de données. Vous êtes concerné lorsque vos sources sont des bases de données, pour les autres sources vous n\u0026rsquo;aurez probablement pas à vous inquiéter du Query Folding.\nC\u0026rsquo;est quoi le Query Folding ? Lorsque vous réalisez une requête Power Query cette dernière va lire la source de données puis faire les transformations demandées.\nLe Query Folding est l\u0026rsquo;action d\u0026rsquo;envoyer certaines transformations directement à la source de données afin que ces transformations soient faites par la source de données.\nPourquoi y faire attention ? Performance est la réponse à la question. En effet, si la source de données s\u0026rsquo;occupe de réaliser certaines transformations, elle sera en général plus performante, car si c\u0026rsquo;est une baser de données, c\u0026rsquo;est sa raison d\u0026rsquo;être d\u0026rsquo;exécuter des requêtes.\nDe plus les données transformées sont en général moins volumineuses que les données brutes, le transfert entre la base et power query sera donc plus rapide s’il y a moins de volume.\nQuand s\u0026rsquo;arrête le Query Folding ? À partir du moment où une étape de votre requête n\u0026rsquo;est pas supportée par le query folding, celui s\u0026rsquo;arrête et les étapes suivantes seront traité pat Power Query.\nPour voir si une étape supporte encore le query folding, faite un clic droit dessus, si l\u0026rsquo;élément de menu Afficher la requête native n\u0026rsquo;est pas grisé, cela signifie que la requête gère le query folding jusqu\u0026rsquo;a cette étape. Quel type d\u0026rsquo;étape gère le Query Folding ? Les étapes suivantes normalement gèrent le Query Folding\nSuppression de colonnes Renommage de colonnes Filtrage de lignes, avec des valeurs statiques ou des paramètres Power Query Regroupement et totalisation Fusion non approximative de requêtes de la même source supportant le Query Folding Ajout de requêtes de la même source supportant le Query Folding Transformation simple de colonne Fusion de colonne Changement de type Colonne conditionnelle Optimiser vos requêtes Maintenant que vous comprenez le Query Folding, vous pouvez optimiser vos requêtes qui gère le query folding en réalisant en premier dans la requête les étapes qui supporte le Query Folding puis les autres après.\nMerci de votre attention.\n","date":"2023-06-26T00:00:00Z","image":"https://blog.ddata.fr/p/power-query-query-folding/jason-leung-wHddViTmSvA-unsplash_hu71f8ecb5fe253ecb731b7a50aebe9db7_2962660_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-query-query-folding/","title":"Query folding, optimiser vos requêtes Power Query."},{"content":"Le match est terminé qu’elles sont les résultats ?\nContexte Je vous ai présenté 3 des outils de préparation de données disponible dans Microsoft Fabric.\nLes flux de données. Le SQL. Les blocs-notes. Après ce match où chacun vous a montré ces capacités, c\u0026rsquo;est l\u0026rsquo;heure du verdict !\nFlux de données C\u0026rsquo;est l\u0026rsquo;outil No code par excellence, parfait pour des utilisateurs non techniques.\nSi vos utilisateurs ont une expérience avec Power BI, ils retrouveront leurs habitudes.\nCet outil est parfait pour les usages classiques, mais ne répondras probablement pas du tout ou difficilement à des usages avancés comme :\nPartitionnement intelligent des nos donnés. Historisation des données. Gestion des dimensions à variation lente. Ma vision de l\u0026rsquo;outil :\nIl couvre 70 à 80 % des traitements à réaliser. Il ne gère pas ou mal les formats non structurés. Il peut être pris en main par tous les publics avec un minimum d\u0026rsquo;expérience data. SQL C\u0026rsquo;est un grand-père de la data, le SQL a fait ses preuves et il est parfait quand on travaille avec de la donnée structurée.\nSi vous avez des projets data existant, vous y trouverez certainement des requêtes SQL pouvant être réutilisées avec un minimum d\u0026rsquo;adaptation. Vous allez pouvoir recycler vos vieux projets dans Fabric.\nMa vision de l\u0026rsquo;outil :\nIl couvre 80 à 90 % des traitements à réaliser. Il ne gère pas ou mal les formats non structurés. Il peut être pris en main par toutes personnes avec une expérience SQL. Pratique pour migrer des traitements existants basés sur le SQL. Spark Python C\u0026rsquo;est un des outils phares de l\u0026rsquo;écosystème daté moderne.\nGrâce à de nombreuses librairies Python, mais aussi avec la possibilité de passer sur d\u0026rsquo;autre langage comme R, cet outil est un vrai couteau suisse.\nLa communauté et la documentation sont aussi très riches? Si vous utilisez par exemple Pandas, vous trouverez des milliers d\u0026rsquo;exemples d\u0026rsquo;utilisation afin d\u0026rsquo;avancer plus rapidement.\nMa vision de l\u0026rsquo;outil :\nIl couvre 100 % des traitements à réaliser. Il gère les formats non structurés. Plus difficile à prendre en main, il nécessite un profil maitrisant les concepts data et la programmation. Cette option rassure les DSI car les traitements réalisés sont moins dépendants de la plateforme que les autres solutions. Lequel choisir ? À cette question je répondrais simplement : pourquoi choisir ?\nTous les membres de votre entreprise qui font de la data ont le même profil ?\nProbablement pas, où alors vous êtes la seule personne à faire de la data dans votre entreprise 😨.\nDonc pourquoi choisir ?\nAvec Fabric vous avez un outil qui vous permet d\u0026rsquo;utiliser le résultat des traitements d\u0026rsquo;une autre personne facilement grâce à OneLake, le datalake unifié sous vos lakehouse et warehouse. Par exemple, un utilisateur métier peut alimenter un lakehouse avec un dataflow et un data scientist peut consommé cette donnée dans un traitement en Python.\nPar contre il y a un revers à la médaille, cette grande liberté au sein de l\u0026rsquo;outil va demander de mettre en place une gouvernance rigoureuse afin de ne pas transformer votre environnement.\nCulture Data En résumé, Microsoft Fabric n\u0026rsquo;est qu\u0026rsquo;un outil et ne vous affranchira la mise en place dans votre entreprise d\u0026rsquo;une culture data forte. Pour cela vous devrez fédérer une communauté interne autour de la data et mettre en place un ensemble d\u0026rsquo;outils, avec notamment :\nUne bonne gouvernance Documentation sur la gouvernance Microsoft Fabric Feuille de route sur l’adoption de Power BI : gouvernance Une culture data avec l\u0026rsquo;excellente série de Matthew Roche (US) La mise en place d\u0026rsquo;un centre d\u0026rsquo;excellence : Feuille de route sur l’adoption de Power BI : Centre d’excellence Merci de votre attention.\n","date":"2023-06-22T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-the-match-result/tangerine-newt-Ob5GikE2raU-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_2681829_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-the-match-result/","title":"Préparation de données avec Microsoft Fabric, les résultats"},{"content":"On termine le match avec les blocs-notes.\nContexte Afin de découvrir les différents types de traitement proposé par Microsoft Fabric, je vous ai proposé un match entre 3 différentes possibilités : Préparation de données avec Microsoft Fabric, le match !.\nJe me propose ici de réaliser les transformations avec un bloc-notes.\nPour rappel, les blocs-notes permettent d\u0026rsquo;écrire des séquences de code alterné avec des séquences de texte au format markdown.\nLe code est exécuté par un moteur Spark, mais dans l\u0026rsquo;écosystème de Fabric vous n\u0026rsquo;y avez pas accès.\nLes langages supportés sont :\nPython Spark SQL Scala R Pour ce match, j\u0026rsquo;ai choisi d\u0026rsquo;utiliser le langage Python avec la bibliothèque Pandas 🐼.\nCréation de notre bloc-notes Rendez-vous dans l\u0026rsquo;espace de travail Fabric - Le match [Silver] créer dans l\u0026rsquo;article de préparation du match.\nOuvrez le lakehouse LeMatch.\nDans le menu sélectionnez Ouvrir le notebook. Sélectionnez Nouveau notebook. Le bloc-notes s\u0026rsquo;ouvre :\nTâches préparatoires à une transformation de données Charger les données Pour charger les données, on va utiliser un assistant pour écrire le premier bloc de code.\nOuvrez le dossier Files. Sélectionnez le raccourci Bronze_Le_Match. Sur le fichier countries.csv appuyez sur les 3 points. Sélectionnez Charger des données. Sélectionnez Pandas. Le code suivant est ajouté dans le bloc-notes :\n1 2 3 4 import pandas as pd # Load data into pandas DataFrame from \u0026#34;/lakehouse/default/\u0026#34; + \u0026#34;Files/Bronze_Le_Match/countries.csv\u0026#34; df = pd.read_csv(\u0026#34;/lakehouse/default/\u0026#34; + \u0026#34;Files/Bronze_Le_Match/countries.csv\u0026#34;) display(df) Explication rapide du code :\nOn demande à importer la librairie pandas pour pouvoir l\u0026rsquo;utiliser. On a un commentaire On charge dans la variable df le contenu du fichier csv. On affiche le contenu de la variable df. Pour exécuter le code, appuyez sur l\u0026rsquo;icône Run cell. Vous pouvez voir le résultat directement dans le bloc-notes. La variable df est utilisable dans les cellules suivantes du notebook.\nSi vous avez un message d\u0026rsquo;erreur vous disant que df n\u0026rsquo;est pas défini, le serveur a probablement réinitialisé votre session. Il suffit de relancer le code de chargement de données pour continuer.\nObtenir un échantillonnage des données Pour ne voir que les 10 premières lignes du jeu de données, on utilise le code suivant :\n1 df.head(10) Appuyez sur + Code pour ajouter une nouvelle cellule dans le bloc-notes. Saisir le code. Exécuter le code, appuyez sur l\u0026rsquo;icône Run cell. Vous pouvez voir le résultat directement dans le bloc-notes. Pour la suite je ne vous montrerais que le résultat du code, la procédure d\u0026rsquo;ajout et d\u0026rsquo;exécution étant toujours la même.\nVoir les métadonnées du jeu de données Pour voir les métadonnées, on utilise le code suivant :\n1 df.info() Réaliser les transformations Sélectionner certaines colonnes Pour ne sélectionner que les 3 colonnes name, currencies et capital, on utilise le code suivant :\n1 df[[\u0026#34;name\u0026#34;,\u0026#34;currencies\u0026#34;,\u0026#34;capital\u0026#34;]] Le jeu de données df est modifié, recharger le avec la commande de chargement de données pour la suite des exemples.\nFiltrer les lignes à conserver Pour ne sélectionner que les lignes ayant pour valeur EUR dans la colonne currencies, on utilise le code suivant :\n1 df.loc[df[\u0026#34;currencies\u0026#34;] == \u0026#34;EUR\u0026#34; ] Modifier le nom des colonnes Pour renommer que les 3 colonnes name, currencies et capital, en nom, devise et capitale, on utilise le code suivant :\n1 2 3 4 5 6 7 8 new_names = {\u0026#39;name\u0026#39;:\u0026#39;Nom\u0026#39;, \u0026#39;currencies\u0026#39;:\u0026#39;devise\u0026#39;, \u0026#39;capital\u0026#39;:\u0026#39;capitale\u0026#39; } df.rename(columns =new_names, inplace =True) # voir le résultat df.head() Modifier le type de données des colonnes Pour convertir la colonne area qui est de type nombre décimal en nombre entier, on utilise le code suivant :\n1 df.astype({\u0026#39;area\u0026#39;:\u0026#39;int\u0026#39;}) Gestion des valeurs absentes Pour remplacer les valeurs vides par la valeur -1 de la colonne landlocked, on utilise le code suivant :\n1 2 df[\u0026#39;landlocked\u0026#39;].fillna(-1, inplace=True) df.head() Enlever les doublons Pour obtenir la liste des devises dédoublonnée, on utilise le code suivant :\n1 2 df2 = df[[\u0026#34;currencies\u0026#34;]].drop_duplicates(subset=[\u0026#34;currencies\u0026#34;], keep=False) display(df2) Sauvegarder le résultat dans la couche Silver Le résultat que l\u0026rsquo;on souhaite conserver dans la couche silver est le résultat de la variable df2 vers un fichier parquet, tel que le montre le code suivant :\n1 2 df2 = df.loc[df[\u0026#34;currencies\u0026#34;] == \u0026#34;EUR\u0026#34; ] df2.to_parquet(\u0026#34;/lakehouse/default/\u0026#34; + \u0026#34;Files/MatchNotebook.parquet\u0026#34;) Exécutez le code ci-dessus pour sauvegarder les données sélectionnées dans un fichier parquet. Depuis l\u0026rsquo;explorateur de lakehouse, localiser votre fichier (pensez à rafraichir si il n\u0026rsquo;apparait pas) et générer le code pour le lire (comme pour le csv au début). Vous pouvez voir le résultat dans la couche silver. Merci de votre attention.\n","date":"2023-06-21T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-the-match-notebook/tangerine-newt-M8-uyRHdsHc-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_1320955_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-the-match-notebook/","title":"Préparation de données avec Microsoft Fabric, les blocs-notes"},{"content":"On continu le match avec un vieux compagnon le SQL.\nContexte Afin de découvrir les différents types de traitement proposé par Microsoft Fabric, je vous ai proposé un match entre 3 différentes possibilités : Préparation de données avec Microsoft Fabric, le match !.\nJe me propose ici de réalisé les transformations avec du SQL.\nPour rappel, le langage SQL est le langage utilisé pour interagir avec les bases de données. Il permet d\u0026rsquo;interroger les données contenues dans des tables et permet aussi de réaliser des modifications sur les données.\nCréation de notre table SQL Dans le cadre de Microsoft Fabric, nous avons 2 possibilités pour utiliser le SQL :\nle point de terminaison SQL du lakehouse permet de faire des requêtes seulement en lecture sur les fichiers managés. C\u0026rsquo;est les fichiers présentés sous la forme de table. le warehouse qui permet de gérer une base de données complète en gérant à la fois les requêtes de lecture que de modification de données. Pour le match, j\u0026rsquo;ai choisi d\u0026rsquo;utiliser le point de terminaison SQL du lakehouse.\nLa première étape est de transformer notre fichier non managé en table. Une table dans le point de terminaison SQL est un fichier en mode managé sur lequel on peut réaliser des requêtes SQL.\nOuvrez le lakehouse LeMatch de l\u0026rsquo;espace de travail Fabric - Le match [Silver].\nRendez-vous dans le dossier du raccourci Bronze_Le_Match. Sélectionnez les 3 points sur la ligne du fichier. Dans le menu, choisissez Charger dans Tables. Entrez countries dans le nom de la table Appuyez sur Confirmer pour continuer. Sélectionnez les tables du lakehouse. Sélectionnez la table countries. Vous pouvez voir un aperçu du contenu. Passez en mode point de terminaison SQL pour pouvoir écrire une requête SQL sur la table. Vous pouvez aussi directement ouvrir le point de terminaison SQL depuis l\u0026rsquo;espace de travail.\nTâches préparatoires à une transformation de données Charger les données Pour charger les données d\u0026rsquo;une table en SQL on utilise la requête suivante :\n1 SELECT * FROM Schema.Table Dans notre cas le nom de schéma est le nom par défaut dbo et le nom de la table est countries, la requête SQL est donc :\n1 SELECT * FROM dbo.countries Dans le point de terminaison SQL, sélectionnez Nouvelle requête SQL. Saisir le code de la requête. Appuyez sur Run. Vous pouvez voir le résultat. Obtenir un échantillonnage des données Souvent lire un échantillon des données, les 100 premières lignes par exemple suffit à ce faire une première idée du jeu de données.\nEn SQL vous pouvez le faire comme cela :\n1 2 SELECT TOP 100 * FROM dbo.countries Vous pouvez constater que l\u0026rsquo;on a lu que 100 lignes. Voir les métadonnées du jeu de données Pour lire les métadonnées de la table, on peut utiliser la requête suivante :\n1 2 3 4 5 6 7 8 9 10 11 12 13 select c.name as column_name ,c.column_id ,schema_name(t.schema_id) as type_schema ,t.name as type_name ,t.is_user_defined ,t.is_assembly_type ,c.max_length ,c.precision ,c.scale from sys.columns as c join sys.types as t on c.user_type_id=t.user_type_id where c.object_id = object_id(\u0026#39;dbo.countries\u0026#39;) order by c.column_id; Réaliser les transformations Sélectionner certaines colonnes Pour ne sélectionner que les 3 colonnes name, currencies et capital, on utilise la requête suivante :\n1 2 3 4 5 SELECT name, currencies, capital FROM dbo.countries Filtrer les lignes à conserver Pour ne sélectionner que les lignes ayant pour valeur EUR dans la colonne currencies, on utilise la requête suivante :\n1 2 3 SELECT * FROM dbo.countries WHERE currencies = \u0026#39;EUR\u0026#39; Modifier le nom des colonnes Pour ne sélectionner que les 3 colonnes name, currencies et capital, et les renommer nom, devise et capitale, on utilise la requête suivante :\n1 2 3 4 5 SELECT name AS nom, currencies AS devise, capital as capitale FROM dbo.countries Modifier le type de données des colonnes Pour convertir la colonne area qui est de type nombre décimal en nombre entier, on utilise la requête suivante :\n1 2 3 SELECT convert(int, area) as area FROM dbo.countries Gestion des valeurs absentes En SQL les valeurs vides sont appelées valeur null. Pour remplacer les valeurs vides par la valeur -1 de la colonne landlocked et ne conserver en plus que les colonnes name, currencies et capital, on utilise la requête suivante :\n1 2 3 4 5 6 SELECT name, currencies, capital ,COALESCE(landlocked, -1) as landlocked FROM dbo.countries Enlever les doublons Pour obtenir la liste des devises dédoublonnée, on utilise la requête suivante :\n1 2 3 4 5 SELECT currencies FROM dbo.countries GROUP BY currencies Sauvegarder le résultat dans la couche Silver Le résultat que l\u0026rsquo;on souhaite conserver dans la couche silver est le résultat de la requête suivante :\n1 2 3 SELECT * FROM dbo.countries WHERE currencies = \u0026#39;EUR\u0026#39; Pour copier le résultat de la requête SQL réalisé sur la zone bronse dans la zone silver, nous allons utiliser un pipeline de données.\nVous trouverez un exemple de pipeline de données dans mon article Votre premier pipeline d\u0026rsquo;intégration de données avec Microsoft Fabric.\nLe magasin de données de type lakehouse ne gère, dans la preview, que les tables et les fichiers. Il n\u0026rsquo;est pas possible d\u0026rsquo;exécuter une requête SQL, pour résoudre ce problème j\u0026rsquo;ai créé un warehouse dans l\u0026rsquo;espace de travail nommé MatchSQL. Le warehouse est un moteur de base de données avec plus de fonctionnalités. Il peut notamment exécuter des requêtes sur les tables du lakehouse dans l\u0026rsquo;activité de copie du pipeline. Pour cela, il suffit d\u0026rsquo;ajouter le nom du lakehouse dans la requête SQL. Si aucun nom n\u0026rsquo;est indiqué c\u0026rsquo;est l\u0026rsquo;endroit ù on est connecté qui est utilisé.\nLa requête SQL devient donc :\n1 2 3 SELECT * FROM LeMatch.dbo.countries WHERE currencies = \u0026#39;EUR\u0026#39; Je créer donc un pipeline dans l\u0026rsquo;espace de travail Fabric - Le match [Silver].\nSélectionnez Data warehouse comme magasin de données. Sélectionnez le warehouse. Indiquez que vous allez utiliser une requête. Entrez le code SQL de la requête. Sélectionnez Lakehouse comme magasin de données. Sélectionnez le lakehouse. Indiquez que vous allez utiliser une table comme destination. Cochez Modifier pour saisir librement le nom de la table. Saisir le nom de la table destination. Enregistrez et exécutez le pipeline de données, puis rendez-vous dans le lakehouse de l\u0026rsquo;espace de travail Fabric - Le match [Silver].\nDans l\u0026rsquo;explorateur du lakehouse ouvrez Tables Votre nouvelle table est bien créée. Merci de votre attention.\n","date":"2023-06-20T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-the-match-sql/tangerine-newt-a9rxefN9vgY-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_3268696_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-the-match-sql/","title":"Préparation de données avec Microsoft Fabric, le SQL"},{"content":"On commence le match avec le flux de données.\nContexte Afin de découvrir les différents types de traitement proposé par Microsoft Fabric, je vous ai proposé un match entre 3 différentes possibilités : Préparation de données avec Microsoft Fabric, le match !.\nJe me propose ici de réaliser les transformations avec un flux de données.\nPour rappel, un flux de données est simplement du Power Query Online. On retrouve la même interface Power Query que dans Power BI mais les traitements sont exécutés dans le service Microsoft Fabric.\nLes flux de de données que l\u0026rsquo;on va utiliser sont les nouveaux flux de données disponibles dans Microsoft Fabric : Flus de données Gen2. La principale différence avec les flux de données classique est que vous pouvez choisir dans quelle destination Power Query écrit le résultat.\nCréation de notre flux de données Rendez-vous dans l\u0026rsquo;espace de travail Fabric - Le match [Silver] créer dans l\u0026rsquo;article de préparation du match. Et passer sur l\u0026rsquo;expérience utilisateur Data Factory.\nDans le menu de l\u0026rsquo;espace de travail, appuyez sur + Nouveau. Sélectionnez Flux de données Gen2. L\u0026rsquo;éditeur Power Query Online s\u0026rsquo;ouvre.\nLa création d\u0026rsquo;un flux de données provoque la création automatique de :\nUn lakehouse et son point de terminaison SQL. Un jeu de données par défaut. Tâches préparatoires à une transformation de données Charger les données Pour charger les données, nous retrouvons la procédure classique de Power Query :\nAllez dans le menu Accueil. Appuyez sur l\u0026rsquo;icône Obtenir les données. Sélectionnez Lakehouse. Les éléments s\u0026rsquo;autoconfigurent, vous avez juste à appuyer sur Suivant. Ouvrez votre espace de travail, ici Fabric - Le match [Silver]. Ouvrez votre lakehouse, ici LeMatch. Ouvrez Fichiers. Ouvrez le raccourci Bronze_Le_Match. Sélectionnez le fichier countries.csv. Appuyez sur Créer pour créer une requête dans Power Query qui ce connecte au fichier. Obtenir un échantillonnage des données Après la création de la requête, Power Query vous affiche automatiquement un échantillon de vos données limité aux 100 premières lignes.\nVoir les métadonnées du jeu de données Dans notre cas la première ligne contient le nom des colonnes, nous allons donc utiliser la fonction Utiliser la première ligne pour les en-têtes afin d\u0026rsquo;avoir les bons noms de colonne, et nous allons laisser Power Query détecter le type de données des colonnes.\nAllez dans le menu Accueil. Appuyez sur l\u0026rsquo;icône Utiliser la première ligne pour les en-têtes. 2 étapes sont automatiquement créées, la seconde et le résultat de l\u0026rsquo;autodetection des types de données de la colonne par Power Query. Cette étape peut être modifiée si le résultat ne vous convient pas. Les métadonnées sont lisibles directement sur les en-t^tes de colonnes. L\u0026rsquo;icône correspond aux types de données. ❗ Pour les sources de données exposant les métadonnées, comme les bases de données ou les fichiers Parquet, ces dernières sont automatiquement lu depuis la source.\nRéaliser les transformations Sélectionner certaines colonnes Pour ne sélectionner que les 3 colonnes name, currencies et capital, on procède ainsi :\nAllez dans le menu Accueil. Appuyez sur l\u0026rsquo;icône Choisir des colonnes. Sélectionnez les colonnes. Appuyez sur OK. Voici le résultat :\nSupprimer l\u0026rsquo;étape nouvellement créée avant de passer à la suite.\nFiltrer les lignes à conserver Pour ne sélectionner que les lignes ayant pour valeur EUR dans la colonne currencies, on procède ainsi :\nAppuyez sur le bouton au niveau de la colonne à filtrer. Définissez votre filtre. Appuyez sur OK. Modifier le nom des colonnes Pour renommer les 3 colonnes name, currencies et capital, en nom, devise et capitale, on procède ainsi :\nFaite un clic droit sur l\u0026rsquo;en-tête de la colonne à renommée. Choisissez Renommer\u0026hellip; dans le menu, vous pourrez ensuite renommer la colonne. Vous pouvez aussi double-cliquer sur le nom de la colonne ou la sélectionner et appuyer sur la touche F2.\nModifier le type de données des colonnes Pour convertir la colonne area qui est de type nombre décimal en nombre entier, on procède ainsi :\nFaite un clic sur l\u0026rsquo;icône représentant le type de données dans l\u0026rsquo;en-tête de la colonne. Choisissez le nouveau type. Vous pouvez aussi passer par le menu Accueil Et choisir l\u0026rsquo;item Type de données. Gestion des valeurs absentes Pour remplacer les valeurs vides par la valeur -1 de la colonne landlocked, on procède ainsi :\nSélectionnez la colonne landlocked. Allez dans le menu Transformer. Sélectionnez Remplacer les valeurs. Recherchez la valeur null. Remplacer là par la valeur -1. Appuyez sur OK. Enlever les doublons Pour obtenir la liste des devises dédoublonnée, on procède ainsi :\nOn commence par ne conserver que la liste des devises.\nFaite un clic droit sur l\u0026rsquo;en-tête de la colonne currencies. Choisissez Supprimer les autres colonnes dans le menu. Allez dans le menu Accueil. Appuyez sur le texte Supprimer les lignes. Sélectionnez Supprimer les doublons. On obtient le résultat suivant :\nSauvegarder le résultat dans la couche Silver Le résultat que l\u0026rsquo;on souhaite conserver dans la couche silver est le résultat de ka section Filtrer les lignes à conserver ci-dessus.\nPour enregistrer le résultat de la requête Power query dans le lakehouse, on procède ainsi :\nEn bas droite de l\u0026rsquo;éditeur Power Query, appuyez sur le + au niveau de Destination des données. Choisir Lakehouse dans la liste. Les éléments s\u0026rsquo;autoconfigurent, vous avez juste à appuyer sur Suivant. La sauvegarde se fait obligatoirement dans une table. Por rappel les tables du lakehouse sont des fichiers managés par le système au format delta lake, vous pouvez les utiliser dans des requêtes SQL via le point de terminaisonSQL associé au lakehouse.\nSélectionnez Nouvelle table Ouvrez votre espace de travail, ici Fabric - Le match [Silver]. Ouvrez votre lakehouse, ici LeMatch. Entrez le nom de votre table ici LeMatchDataflow. Appuyez sur Suivant. Vous pouvez ensuite choisir les paramètres de mise à jour de la table et le mapping des colonnes. On laisse tout par défaut.\nAppuyez sur Enregistrer les paramètres. Vos paramètres peuvent être édités ici via la roue crantée. Appuyez sur Publier pour publier votre flux de données et l\u0026rsquo;exécuter. Suite à sa publication, votre flux de données s\u0026rsquo;actualise automatiquement, vous pouvez l\u0026rsquo;actualiser manuellement dans l\u0026rsquo;espace de travail.\nNous allons maintenant vérifier le résultat, pour cale rendez-vous dans le lakehouse de l\u0026rsquo;espace de travail Fabric - Le match [Silver].\nDans l\u0026rsquo;explorateur du lakehouse ouvrez Tables Votre nouvelle table est bien créée. Pour enregistrer le résultat dans un fichier du lakehouse, on procède ainsi :\nMerci de votre attention.\n","date":"2023-06-19T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-the-match-dataflow/tangerine-newt-w9QdAC1f8-0-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_2607520_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-the-match-dataflow/","title":"Préparation de données avec Microsoft Fabric, les flux de données"},{"content":"Vous voulez traiter vos données dans Microsoft Fabric, mais vous êtes perdu ?\nPour vous faire votre avis, je vous propose un match entre les différents outils pour réaliser un même traitement de données simple.\nContexte Dans mes 2 premiers articles sur Microsoft Fabric, je vous ai présenté :\nL\u0026rsquo;intégration de données en mode temps réel et le KQL. L\u0026rsquo;intégration de données en mode batch et les pipelines de données. Maintenant que vos données sont dans votre zone bronze 🥉, il est temps de les raffiner pour les faire passer dans la zone silver 🥈.\nMais quels outils utilisés ? Je me propose de faire un match entre 3 possibilités disponibles dans Microsoft Fabric :\nLes flux de données (Power Query Online). Le SQL. Les blocs-notes (Python). Le match ! Afin de comparer ces 3 solutions, je vous propose de la faire sur un exemple simple avec un résultat attendu identique pour les 3 possibilités.\nLe but n\u0026rsquo;est pas de vous former, mais plutôt de vous montrer la logique et le travail n\u0026quot;céssaire à fournir pour chacune des options.\nPour ce match chaque solution devra réaliser les tâches suivantes :\nTâches préparatoires à une transformation de données Charger les données depuis la zone bronze Obtenir un échantillonnage des données Voir les métadonnées du jeu de données Réaliser les transformations Sélectionner certaines colonnes Filtrer les lignes à conserver Modifier le nom des colonnes Modifier le type de données des colonnes Gestion des valeurs absentes Enlever les doublons Sauvegarder le résultat dans la zone silver La préparation du match Avant tout bon match, une préparation est nécessaire. Nous allons maintenant préparer un terrain de jeu commun à nos 3 solutions.\nCréation de la zone bronze 🥉 Rendez-vous sur le portail Power BI, Fabric doit être activé dans votre environnement, si ce n\u0026rsquo;est pas le cas, suivez ce tutoriel : Version d’évaluation de Microsoft Fabric.\nCréation de l\u0026rsquo;espace de travail Dans le menu de gauche, sélectionnez Espace de travail, puis + Nouvel espace de travail.\nNommez votre espace de travail Fabric - Le match [Bronze]. Ouvrez les options avancées Sélectionnez un type d\u0026rsquo;espace de travail compatible avec les objets Micrsoft Fabric. Appuyez sur Appliquez pour créer votre espace de travail. Sélection de l\u0026rsquo;expérience utilisateur Afin de simplifier l\u0026rsquo;interface, Fabric est organisé autour de différentes expériences utilisateur afin de ne pas présenter l\u0026rsquo;ensemble des types objets disponible à tout le monde. Pour notre exemple nous pouvons utiliser l\u0026rsquo;expérience Data Factory ou Engineering données.\nAppuyez sur l\u0026rsquo;icône en bas à gauche de votre écran, si vous êtes arrivé par le portail Power BI vous trouver l\u0026rsquo;icône Power BI, sinon vous trouverez l\u0026rsquo;icône de l\u0026rsquo;expérience utilisateur en cours. Sélectionnez Data Factory Après le changement d\u0026rsquo;expérience, vous devez vous repositionner dans votre espace de travail.\nCréation d\u0026rsquo;un lakeHouse Nous allons maintenant créer un lakehouse dédié à notre exemple.\nUn lakehouse est une espace de stockage acceptant n\u0026rsquo;importe quel type de données. Il sert de zone de stockage générique des données de notre couche bronze.\nDans l\u0026rsquo;écosystème Fabric un lakehouse est un sous-ensemble de OneLake, l\u0026rsquo;espace de stockage unifié de votre entreprise.\nDans le menu de l\u0026rsquo;espace de travail, appuyez sur + Nouveau. Sélectionnez Lakehouse. Saisissez le nom du lakehouse, ici LeMatch qui est le nom de la source de données. Appuyez sur Créer pour créer le lakehouse. L\u0026rsquo;écran qui s\u0026rsquo;affiche est l\u0026rsquo;explorateur de lakehouse.\nDans l\u0026rsquo;explorateur de lakehouse, vous pouvez naviguer dans les dossiers. La section Files permet d\u0026rsquo;avoir la vision des fichiers bruts stockés dans le lakehouse, on parle de données non managées. La section Tables permet d\u0026rsquo;avoir la vision base de données des fichiers dans le lakehouse et de pouvoir les requêter avec le langage SQL, on parle de données managées. Vous pouvez passer de la vue lakehouse à la vue point de terminaison SQL ici. Restez dans l\u0026rsquo;explorateur de lakehouse.\nChargement du fichier servant pour le match dans la zone bronze Nous allons commencer par charger le fichier sur votre poste de travail. Faite un clic droit sur le lien du fichier et choisissez Enregistrer sous pour le sauvegarder en local.\nNous allons créer un dossier pour contenir notre fichier.\nDans l\u0026rsquo;explorateur de lakehouse appuyez sur les 3 points face au dossier Files. Sélectionnez Nouveau sous-dossier. Saisissez le nom du dossier, ici Bronze_Le_Match. ⚠ Le nom ne dois pas comporter d\u0026rsquo;espace ou de caractères spéciaux hors le _ pour la suite de la démonstration. Appuyez sur Créer pour créer le dossier. Dans l\u0026rsquo;explorateur de lakehouse appuyez sur les 3 points face au dossier [Bronze] Le match dans le dossier Files. Ouvrez le sous-menu Charger. Sélectionnez Charger des fichiers. Appuyez sur l\u0026rsquo;icône du dossier pour parcourir vos données locales et sélectionnez le fichier countries.csv sauvegardé. Appuyez sur Charger pour charger manuellement le fichier dans le lakehouse. Une fois l\u0026rsquo;opération terminée vous en êtes informé. Vous avez maintenant votre fichier dans la zone bronze.\nCréation de la zone silver 🥈 Nous allons maintenant créer une zone silver pour accueillir les traitements et le données résultante de ces derniers.\nPour cela nous allons créer un espace de travail et un lakehouse en reprenant les étapes précédentes.\nNom de l\u0026rsquo;espace de travail : Fabric - Le match [Silver] Nom du lakehouse : LeMatch Création d\u0026rsquo;un raccourci vers la zone bronze. Afin que les traitements de la zone silver puissent utiliser les données de la zone bronze sans réaliser de copie, nous allons créer un raccourci du dossier contenant les données dans la zone silver.\nDans l\u0026rsquo;explorateur de lakehouse appuyez sur les 3 points face au dossier Files. Sélectionnez Nouveau raccourci. Sélectionnez Microsoft OneLake. Sélectionnez le lakehouse de la zone bronze. Appuyez sur Suivant pour continuer. Ouvrez le dossier Files. Sélectionnez le dossier Bronze_Le_Match. Appuyez sur Créer pour créer le raccourci. Vous avez maintenant votre raccourci dans la zone silver.\nLe terrain de jeu est prêt, c\u0026rsquo;est parti Vous retrouverez ici les liens vers chaque partie du match :\nLes flux de données (Power Query Online). Le SQL. Les blocs-notes (Python). Merci de votre attention.\n","date":"2023-06-18T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-the-match-preparation/tangerine-newt-ep09aQX8LUs-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_961791_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-the-match-preparation/","title":"Préparation de données avec Microsoft Fabric, le match !"},{"content":"Dans cet article je vous propose de tester et de créer une première intégration de données avec un pipeline de Microsoft Fabric.\nContexte Dans cet article je vous propose de suivre toutes les étapes de création d\u0026rsquo;un flux d\u0026rsquo;intégration de données.\nPour réaliser cette tâche, nous allons utiliser le pipeline de données. Un pipeline permet de facilement faire de la copie de données entre 2 points et d\u0026rsquo;ordonnancer toute sorte de tâches afin de créer des traitements de données robustes et cohérents.\nArchitecture médaillon Nous allons nous inspirez de l\u0026rsquo;architecture médaillon, pour simplifier nous allons avoir 3 zones dans lesquelles vont résider nos données :\nLa zone bronze 🥉 : Elle contient les données brutes historiques, non traitées. La zone silver 🥈 : Elle contient les données préparer, valider et prête à l\u0026rsquo;usage. La zone gold 🥇 : Elle contient les données affinées et agrégées utilisables directement notamment par les outils analytiques. Lors du passage d\u0026rsquo;une zone à une autre, les données de la zone précédente sont conservées en l\u0026rsquo;état. Nous avons donc plusieurs copies de la même donnée dans des états différents.\nPréparer votre environnement de travail Je vous propose de réaliser un exemple d\u0026rsquo;intégration de données externe, via une source de données de type OData public vers la couche bronze 🥉.\nNous allons maintenant préparer un environnement pour recevoir les données.\nCréation d\u0026rsquo;un espace de travail gérant les objets Fabric Rendez-vous sur le portail Power BI, Fabric doit être activé dans votre environnement, si ce n\u0026rsquo;est pas le cas, suivez ce tutoriel : Version d’évaluation de Microsoft Fabric.\nDans le menu de gauche, sélectionnez Espace de travail, puis + Nouvel espace de travail.\nDonnez un nom à votre espace de travail Ouvrez les options avancées Sélectionnez un type d\u0026rsquo;espace de travail compatible avec les objets Micrsoft Fabric. Appuyez sur Appliquez pour créer votre espace de travail. Sélection de l\u0026rsquo;expérience utilisateur Afin de simplifier l\u0026rsquo;interface, Fabric est organisé autour de différentes expériences utilisateur afin de ne pas présenter l\u0026rsquo;ensemble des types objets disponible à tout le monde. Pour notre exemple nous pouvons utiliser l\u0026rsquo;expérience Data Factory ou Engineering données.\nAppuyez sur l\u0026rsquo;icône en bas à gauche de votre écran, si vous êtes arrivé par le portail Power BI vous trouver l\u0026rsquo;icône Power BI, sinon vous trouverez l\u0026rsquo;icône de l\u0026rsquo;expérience utilisateur en cours. Sélectionnez Data Factory Après le changement d\u0026rsquo;expérience, vous devez vous repositionner dans votre espace de travail.\nCréation d\u0026rsquo;un DatalakeHouse Nous allons maintenant créer un Lakehouse dédié à notre exemple.\nUn lakehouse est une espace de stockage acceptant n\u0026rsquo;importe quel type de données. Il sert de zone de stockage générique des données de notre couche bronze.\nDans l\u0026rsquo;écosystème Fabric un lakehouse est un sous-ensemble de OneLake, l\u0026rsquo;espace de stockage unifié de votre entreprise.\nDans le menu de l\u0026rsquo;espace de travail, appuyez sur + Nouveau. Sélectionnez Lakehouse. Saisissez le nom du lakehouse, ici TripPin qui est le nom de la source de données. Appuyez sur Créer pour créer le lakehouse. L\u0026rsquo;écran qui s\u0026rsquo;affiche est l\u0026rsquo;explorateur de lakehouse.\nDans l\u0026rsquo;explorateur de lakehouse, vous pouvez naviguer dans les dossiers. La section Files permet d\u0026rsquo;avoir la vision des fichiers bruts stockés dans le lakehouse, on parle de données non managées. La section Tables permet d\u0026rsquo;avoir la vision base de données des fichiers dans le lakehouse et de pouvoir les requêter avec le langage SQL, on parle de données managées. Vous pouvez passer de la vue lakehouse à la vue point de terminaison SQL ici. Pour chaque lakehouse, un point de terminaison SQL est automatiquement créé, ce dernier permet de créer des requêtes de SQL de type SELECT uniquement sur vos fichiers de données. Vous pouvez aussi faire de jointure SQL entre 2 tables ou plus présente dans le lakehouse.\nRetournez au niveau de l\u0026rsquo;espace de travail.\nVous pouvez constater que 2 objets on était automatiquement créer avec votre lakehouse Le point de terminaison SQL comme vue ci-dessus. Un jeu de données par défaut pour Power BI. Ce jeu de données est managé par Fabric. Créer un pipeline Maintenant que notre environnement de travail est prêt, nous allons pouvoir créer notre traitement d\u0026rsquo;intégration de données, pour cela nous allons choisir un pipeline de données.\nLes pipelines de données présents dans Microsoft Fabric, sont une évolution des pipelines que l\u0026rsquo;on trouve dans les services Azure Data factory et Azure Synapse Analytics.\nEn termes d\u0026rsquo;interface utilisateur et de possibilité, ils sont très proches de ces derniers, mais ils ont subi une très forte simplification. Par exemple, la notion de service lié et de jeu de données n\u0026rsquo;existe plus dans Fabric elle est remplacée par les connexions. On constate aussi que seules les principales activités de pipeline reste présentent dans Fabric.\nDans le menu de l\u0026rsquo;espace de travail, appuyez sur + Nouveau. Sélectionnez Pipeline de données. Saisissez le nom du pipeline, ici Source_To_Bronze_TripPin qui est le nom de la source de données. Appuyez sur Créer pour créer le pipeline. On retrouve principalement la zone d\u0026rsquo;édition des activités du pipeline Dans le menu Home nous avons les icônes de sauvegarde de notre travail. On retrouve les principales fonctions du menu Run. Ainsi que les principales activités. Un pipeline se compose :\nD\u0026rsquo;activités qui correspondent à des briques permettant de réaliser des actions. De connexions permettent de communiquer avec un service de données. D\u0026rsquo;une planification pour automatiser l\u0026rsquo;exécution du pipeline. Les connexions Les connexions permettent de communiquer avec un service de données.\nElles peuvent être créées directement depuis le pipeline, depuis le portail de gestion des passerelles et connexions ou depuis votre espace de travail en ouvrant le menu de la roue crantée en haut à droite puis en sélectionnant Gérer des connexions et des passerelles.\nPour le moment Fabric ne gère que les connexions vers des services Internet, prochainement un accès a vos données d\u0026rsquo;entreprise sera surement possible via la passerelle Power BI ou un processus équivalent.\nLes activités Les activités sont des composants génériques qui utilisent les connexions pour réaliser des actions standard dessus. Toutes les activités ne peuvent pas utiliser tous les types de connexion.\nLes principales activités sont :\nCopy data : permets de réaliser une copie de données entre une source et une destination. Dataflow : permets d\u0026rsquo;exécuter un dataflow. Notebook : permets d\u0026rsquo;exécuter un notebook. Loockup : permets d\u0026rsquo;interroger les données d\u0026rsquo;une connexion. Invoke pipeline : permets d\u0026rsquo;exécuter un autre pipeline. Vous retrouverez aussi des activités vous permettant de gérer l\u0026rsquo;exécution de votre pipeline avec notamment des variables ou des boucles.\nCopier une table de données dans notre lakehouse Nous allons maintenant procéder à la création de notre premier pipeline. Le but de ce pipeline va être de copier les données d\u0026rsquo;une source OData vers notre lakehouse.\nDans le menu Home du pipeline. Sélectionnez Copy data. Cliquez sur Add to canvas. L\u0026rsquo;activité de copie apparait dans la zone d\u0026rsquo;édition du pipeline. Lorsque l\u0026rsquo;activité de copie est sélectionnée, vous pouvez accéder à ces propriétés. Dans les propriétés de l\u0026rsquo;activité de copie, sélectionnez Source. Appuyez sur + Nouveau pour créer une nouvelle connexion. Sélectionnez le connecteur OData. Appuyez sur Continuer. Dans URL entrez l\u0026rsquo;adresse https://services.odata.org/TripPinRESTierService. Dans Nom de la connexion entrez TripPinRESTierService. Appuyez sur Créer pour créer la nouvelle connexion. Dans Chemin d\u0026rsquo;accès sélectionnez People. Appuyez sur Aperçu des données pour tester si vous accédez aux données. Vous devez voir les données sources comme dans l\u0026rsquo;écran ci-dessus.\nFermez la fenêtre d\u0026rsquo;aperçu et rendez-vous sur l\u0026rsquo;onglet Destination de l\u0026rsquo;activité de copie de données. Sélectionnez le lakehouse TripPin que l\u0026rsquo;on a créé précédemment. Cochez Modifier pour pouvoir saisir librement le nom de la table. Rentrez People pour le nom de la table. Enregistrez votre pipeline. Lancez une première exécution de votre pipeline. Vous pouvez voir les informations d\u0026rsquo;exécution du pipeline, la coche verte ✅ nous indique un succès.\nExplorer le résultat de l\u0026rsquo;exécution du pipeline Rendez-vous dans le lakahouse depuis l\u0026rsquo;espace de travail.\nDans la partie Explorateur, vous pouvez voir votre table People. Cliquez dessus. Vous pouvez voir ici un aperçu du contenu de votre table. Notre pipeline a envoyé une requête OData sur la source de données et à copier le résultat dans notre lakehouse sous la forme de fichier delta lake. C\u0026rsquo;est dernier ont automatiquement était mis à votre disposition de manière managée sous la forme d\u0026rsquo;une table pouvant être requêtée en SQL.\nCopier plusieurs tables de données dans notre lakehouse Sur la source de notre activité de copie, on peut constater que plusieurs chemins d\u0026rsquo;accès diffèrent son possible. Nous allons modifier notre pipeline afin de copier tous les chemins dans un seul pipeline.\nAllez dans votre espace de travail et ouvrez le pipeline créé précédemment. L\u0026rsquo;objectif est maintenant de réaliser une boucle permettant de copier l\u0026rsquo;ensemble des chemins d\u0026rsquo;accès possible de notre source de données, chaque chemin devant être stocké dans une table distincte.\nPour cela nous allons procéder en 2 étapes :\nObtenir la liste des chemins disponible. Pour chaque élément de cette liste, utiliser une même activité de copie que l\u0026rsquo;on va variabiliser. Nous allons maintenant ajouter une activité de type Lookup à notre pipeline.\nAppuyer sur Lookup pour ajouter l\u0026rsquo;activité au pipeline. Sélectionnez l\u0026rsquo;activité pour accéder à ces propriétés. Renommez l\u0026rsquo;activité en Liste des chemins TripPin. Pour obtenir la liste des chemins, nous allons interroger le service en mode HTTP (Requête Web).\nDans les propriétés de l\u0026rsquo;activité Liste des chemins TripPin, sélectionnez Source. Appuyez sur + Nouveau pour créer une nouvelle connexion. Sélectionnez le connecteur HTTP. Appuyez sur Continuer. Dans URL entrez l\u0026rsquo;adresse https://services.odata.org/TripPinRESTierService. Dans Nom de la connexion entrez TripPinRESTierService HTTP. Ne rien saisir dans Type d\u0026rsquo;authentification, on reste en mode anonyme. Appuyez sur Créer pour créer la nouvelle connexion. Passez le format de fichier reçu en JSON. Cliquez sur Aperçu des données. Vous pouvez voir le résultat de l\u0026rsquo;appel de la requête HTTP. Vous pouvez constater que la liste des chemins est dans la clé value du document JSON sous forme de tableau, car dans un bloc entre crochets \u0026ldquo;[]\u0026rdquo;. Vous pouvez aussi voir que la valeur que l\u0026rsquo;on souhaite utiliser est url, elle permettra de variabiliser les appels. Fermez l\u0026rsquo;aperçu. Pour traiter les éléments de la liste, nous allons ajouter une activité de type ForEach. Cette activité permet de faire une boucle pour chaque élément de la liste.\nRendez-vous dans le menu Activities. Appuyer sur ForEach pour ajouter l\u0026rsquo;activité au pipeline. Sélectionnez l\u0026rsquo;activité pour accéder à ces propriétés. Renommez l\u0026rsquo;activité en Pour chaque chemin. Sélectionnez la coche verte sur l\u0026rsquo;activité Liste des chemins TripPin. Faite un glisser-déposer sur l\u0026rsquo;activité Pour chaque chemin. Vous avez créé un lien de dépendance entre les 2 activités. Pour chaque chemin s\u0026rsquo;exécutera après l\u0026rsquo;exécution réussie de Liste des chemins TripPin. Sélectionnez l\u0026rsquo;activité pour accéder à ces propriétés. Allez dans l\u0026rsquo;ongle Paramètres. La propriété éléments doit contenir la liste des éléments à parcourir pour la boucle ForEach. Appuyez sur le lien Ajouter du contenu dynamique. La fenêtre Générateur d’expressions de pipeline apparait. Cette fenêtre permet de variabiliser des propriétés via un ensemble de fonctions standard et les éléments spécifiques au pipeline. On ne peut référencer toutes les activités parentes à l\u0026rsquo;activité courante. Le lien de parenté a été réalisé à l\u0026rsquo;étape 2. Cliquez sur Liste des chemins TripPin first row, et le texte @activity(\u0026lsquo;Liste des chemins TripPin\u0026rsquo;).output.firstRow apparait dans la zone d\u0026rsquo;édition. Dans cette zone d\u0026rsquo;édition, vous pouvez saisir l\u0026rsquo;expression dynamique. Rappelez-vous dans le JSON renvoyé par l\u0026rsquo;activité Liste des chemins TripPin la liste des chemins ce trouve dans la propriété value. Nous allons donc rajouter .value à l\u0026rsquo;expression afin d\u0026rsquo;indiquer où se trouve notre liste. Vous devez donc avoir @activity(\u0026lsquo;Liste des chemins TripPin\u0026rsquo;).output.firstRow.value comme valeur. Appuyez sur OK pour fermer. La propriété Eléments de l\u0026rsquo;activité Pour chaque chemin à maintenant un fond vert pour vous indiquer qu\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une expression. Nous allons maintenant déplacer l\u0026rsquo;activité de copie dans la boucle. Faites un clic droit sur l\u0026rsquo;activité de copie et choisissez Couper. Sur l\u0026rsquo;activité Pour chaque chemin sélectionnez l\u0026rsquo;icône en forme de crayon pour accéder à la zone d\u0026rsquo;édition intérieure de l\u0026rsquo;activité. Vous pouvez à tout moment voir où vous êtes. Quand vous voudrez retourner dans la zone de dessin principale vous pourrez clique sur le lien pour y accéder. Faite un clic droit dans un endroit vide de la zone de dessin et choisissez Coller. Votre activité de copie apparait. Sélectionnez l\u0026rsquo;activité pour accéder à ces propriétés. Renommez l\u0026rsquo;activité en Copie du chemin. Sélectionnez l\u0026rsquo;activité pour accéder à ces propriétés. Allez dans l\u0026rsquo;ongle Source. Cochez Modifier pour pouvoir saisir librement le chemin. Appuyez sur le lien Ajouter du contenu dynamique. S’il n\u0026rsquo;apparait pas positionné, cliquez dans la zone de saisie du chemin d\u0026rsquo;accès. Cliquez sur Sur chaque chemin. Le texte item() apparait dans la zone d\u0026rsquo;édition. Cette valeur est la référence de l\u0026rsquo;élément en cours dans la lecture de la liste. Quand on a regardé l\u0026rsquo;aperçu, la propriété des éléments de la liste que l\u0026rsquo;on s=ouhaité lire était url. Nous allons donc l\u0026rsquo;indiquer, le texte complet de l\u0026rsquo;expression est @item().url. Appuyez sur OK pour fermer. Nous allons maintenant variabiliser la destination.\nAllez dans l\u0026rsquo;ongle Destination. Cochez Modifier pour pouvoir saisir librement le nom de la table. Appuyez sur le lien Ajouter du contenu dynamique. S’il n\u0026rsquo;apparait pas positionné, cliquez dans la zone de saisie du nom de la table. Cliquez sur Sur chaque chemin. Le texte item() apparait dans la zone d\u0026rsquo;édition. Cette valeur est la référence de l\u0026rsquo;élément en cours dans la lecture de la liste. Quand on a regardé l\u0026rsquo;aperçu, la propriété des éléments de la liste que l\u0026rsquo;on s=ouhaité lire était url. Nous allons donc l\u0026rsquo;indiquer, le texte complet de l\u0026rsquo;expression est @item().url. Appuyez sur OK pour fermer. Nous allons maintenant sauvegarder et exécuter le pipeline, aller dans le menu Home et appuyer sur l\u0026rsquo;icône de sauvegarde puis d\u0026rsquo;exécution du pipeline.\nOuvrez le chevron pour voir le détail de l\u0026rsquo;exécution de la boucle Vous pouvez constater qu\u0026rsquo;il y a eu plusieurs exécutions différentes de l\u0026rsquo;activité de copie. Rendez-vous dans le lakahouse depuis l\u0026rsquo;espace de travail.\nDans la partie Explorateur, vous pouvez voir que toutes les tables on était créées. Bravo vous avez créer votre premier pipeline.\nL\u0026rsquo;exemple présenter permet seulement d\u0026rsquo;effleurer les possibilités des pipelines. Comme vous devez le présentir, cet outil offre un grand nombre de possibilités pour le traitement de vos données.\nMerci de votre attention.\n","date":"2023-06-11T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-first-pipeline/bruno-garcia-RDJEYavSQZM-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_2557256_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-first-pipeline/","title":"Votre premier pipeline d'intégration de données avec Microsoft Fabric"},{"content":"Quelques conseils pour améliorer la maintenance de vos projets Power Query.\nContexte Nous allons aborder dans cet article un point très important dans Power Query : la maintenance.\nAvec un peu d\u0026rsquo;expérience vous vous rendrez compte que pensais à la maintenabilité de ses requêtes Power Query est quelque chose de très important.\nPensez à celui qui va passer après vous, c\u0026rsquo;est probablement votre moi de dans 6 mois et il vous sera reconnaissant d\u0026rsquo;avoir travaillé proprement.\nUtiliser des émoticônes dans les noms (requêtes / étapes / colonne) La première chose à faire est de bien nommer les différents objets que vous manipulez :\nLes requêtes étant les tables de votre modèle de données, elles doivent donc avoir un nom clairement compréhensible pour les métiers qui vont utiliser le modèle de données. Les noms des colonnes des tables doivent suivre cette même règle. En général, le nom des étapes n\u0026rsquo;est pas clair. On retrouve souvent les noms par défaut. Prenez le temps de renommer vos étapes clairement. Vous pouvez même utiliser des émoticônes dans le nom des étapes. Cet exemple vient du post linkdn de Rick de Groot.\n💥 Sous Windows, pour ajouter simplement des émoticônes, utilisez le raccourci clavier avec la touche windows + ;\nUtiliser des commentaires Ajouter des commentaires au-dessus d\u0026rsquo;une étape dans l\u0026rsquo;éditeur avancé pour le voir apparaitre\nDans l\u0026rsquo;éditeur Power Query, allez dans le menu Acceuil. Ouvrez l\u0026rsquo;éditeur avancé Vous pouvez saisir un commentaire sur une ligne en le précédent de // Ou un commentaire multiligne entre /* */ Les commentaires apparaissent au niveau des étapes de la requête. Économiser les étapes renommées et types On voit souvent un grand nombre d\u0026rsquo;étapes Colonnes renommées ou Type modifié dans une même requête. Cela se produit quand on agit de la sorte :\nCréation d\u0026rsquo;une étape Colonnes renommées Ajout d\u0026rsquo;autres étapes. On souhaite de nouveau renommer des colonnes, dans ce cas une nouvelle étape de type Colonnes renommées est créée pour Power Query. C\u0026rsquo;est bien sûr la même chose pour Type modifié.\nPour éviter cela, vous pouvez vous repositionner sur une étape Colonnes renommées ou Type modifié précédente et faire les modifications nécessaires. Power Query vous demande si vous souhaitez insérer une étape, répondez oui et Power Query réutilisera l\u0026rsquo;étape en cours si elle est du même type que l\u0026rsquo;action réalisée.\nPour optimiser l\u0026rsquo;usage des étapes Colonnes renommées ou Type modifié, positionnez les à la fin de vos requêtes, vous pouvez les déplacer simplement dans l\u0026rsquo;interface via un glisser-déposer.\nModifier le nom par défaut défini par l\u0026rsquo;assistant Lorsque vous ajoutez des colonnes à votre requête, Power Query, certains assistants, ne propose pas de choisir le nom de la nouvelle colonne. Par exemple, si j\u0026rsquo;ajoute une colonne \u0026ldquo;extraire les premiers caractères\u0026rdquo;.\nLe code Power Query généré est visible dans la barre de formule.\nVous pouvez voir le nom donner par défaut par Power Query. Ce nom est modifiable dans l\u0026rsquo;étape en éditant la formule. Référencer les flux de données Lorsque vous utilisez des flux de données, votre requête Power Query ressemble à cela visuellement :\nMais si on ouvre le code, nous avons ceci :\nNous avons 5 étapes dans l\u0026rsquo;éditeur avancé. Alors que l\u0026rsquo;on ne voit que 2 étapes, dont l\u0026rsquo;étape Navigation qui n\u0026rsquo;existe pas dans l\u0026rsquo;éditeur avancé. Cela est dû à un masquage automatique de certaines étapes sous la dénomination Navigation. C\u0026rsquo;est les étapes qui permettent de naviguer dans le service de flux de données est de sélectionner votre flux de données.\nSi vous voulez voir toutes les étapes dans l\u0026rsquo;interface, il suffit d\u0026rsquo;ajouter une étape avant la première étape.\nAjout d\u0026rsquo;une étape ne faisant rien. Maintenant on voit toutes les étapes de la requête. Comme vous pouvez le constater lorsque vous appelez un flux de données, Power Query utilise les guid (identifiant unique) de ce dernier pour l\u0026rsquo;identifier de manière unique. C\u0026rsquo;est bien, mais si vous envisagez d\u0026rsquo;avoir des environnements distincts pour les tests et la production ou si vous supprimez et recréez votre flux de données, vous serez confronté au changement de la valeur de guid.\nPour éviter, ces problèmes, nous pouvons améliorer le code généré par l\u0026rsquo;assistant, par ce code :\n1 2 3 4 5 6 7 8 9 10 11 let WorkspaceName = \u0026#34;INSERER LE NOM DE DE L\u0026#39;ESPACE DE TRAVAIL DE VOTRE FLUX DE DONNEES\u0026#34;, DataflowName = \u0026#34;INSERER LE NOM DE VOTRE FLUX DE DONNEES\u0026#34;, EntityName = \u0026#34;INSERER LE NOM DE LA REQUETE DANS VOTRE FLUX DE DONNEES\u0026#34;, Source = PowerPlatform.Dataflows(null), Workspaces = Source{[Id=\u0026#34;Workspaces\u0026#34;]}[Data], Workspace = Workspaces{[workspaceName=WorkspaceName]}[Data], Dataflow = Workspace{[dataflowName=DataflowName]}[Data], Entity = Dataflow{[entity=EntityName,version=\u0026#34;\u0026#34;]}[Data] in Entity Modifier les lignes 2, 3 et 4 avec vos valeurs. Créez une nouvelle requête vide et dans l\u0026rsquo;éditeur avancé remplacez le code par celui-ci dessus. On peut aussi utiliser des paramètres Power Query pour variabiliser le nom de l\u0026rsquo;espace de travail et du flux de données et centraliser ce paramétrage pour toutes nos requêtes.\n⚠ Ce changement ne garantit plus l\u0026rsquo;unicité du nom du flux de données, mais c\u0026rsquo;est ce qui est recherché.\nMerci de votre attention.\n","date":"2023-06-07T00:00:00Z","image":"https://blog.ddata.fr/p/power-query-advanced-m-part2/luis-feliciano-9EH5AkClwZ0-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_873198_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-query-advanced-m-part2/","title":"Power Query améliorer la maintenance."},{"content":"Dans cet article je vous propose de tester l\u0026rsquo;aventure Real-time Analytics de Microsoft Fabric.\nContexte Microsoft Fabric propose différentes perspectives de gestion de la donnée. Nous allons ici nous concentrer sur la partie temps réel.\nPour cela je vous propose le scénario suivant, nous avons des capteurs qui émettent des données régulièrement, mais pas de manière régulière. Nous voulons pouvoir créer un rapport Power BI pour observer les données en temps réel.\nPour réaliser cela, nous devrons :\nSimuler l\u0026rsquo;envoi des données des capteurs. Collecter les données. Traiter les données. Restituer les données dans Power BI. Présentation de l\u0026rsquo;architecture Voici les différentes briques d\u0026rsquo;architecture que nous allons utiliser :\nL\u0026rsquo;envoi des données de nos capteurs sera simulé par un script PowerShell. La collecte des données se fera via un Eventstream de Microsoft Fabric. Le stockage des données sera réalisé\u0026quot; dans une base de données KQL. La restitution des données se fera dans un rapport Power BI. Le schéma d\u0026rsquo;architecture est donc le suivant : Présentation de nos capteurs Pour simuler les capteurs, nous allons utiliser un simple script PowerShell qui va créer aléatoirement les messages des capteurs. Le script de création des messages des capteurs est le suivant :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Nombre de message à envoyer $Messages = 100 # Id des capteurs $HardwareID = \u0026#34;12345\u0026#34;, \u0026#34;23456\u0026#34;, \u0026#34;34567\u0026#34;,\u0026#34;45678\u0026#34;,\u0026#34;67890\u0026#34; # Boucle d\u0026#39;envoi des messages for ($i = 0; $i -lt $Messages; $i++) { # Création du message à envoyer au format JSON $IOTFakeMessage = @{ HardwareId = \u0026#34;ID-\u0026#34; + $HardwareID[(Get-Random -Minimum 0 -Maximum $HardwareID.Count)] Date = Get-Date CaptorInfo = @{ Temp = Get-Random -Minimum 20 -Maximum 28 Hum = Get-Random -Minimum 40 -Maximum 60 } } | ConvertTo-Json -Depth 99 # Envoi du message dans la fenêtre PowerShell Write-Host $IOTFakeMessage # Attente avant le prochain message Start-Sleep -Milliseconds (Get-Random -Minimum 200 -Maximum 4000) } Pour que cela fonctionne, ouvrez une fenêtre PowerShell, copiez le script et appuyez sur entrée pour l\u0026rsquo;exécuter.\nJ\u0026rsquo;ai testé avec une version de PowerShell Core 7.\nDans cette version du script, les messages sont simplement affichés dans la fenêtre PowerShell.\nPréparation de l\u0026rsquo;environnement Fabric Création de l\u0026rsquo;espace de travail Nous allons créer un nouvel espace de travail pour cette démonstration, comme cela vous pourrez le supprimer à la fin pour libérer les ressources utilisées.\nAllez dans Espace de travail dans le panneau de gauche et sélectionnez + Nouvel espace de travail.\nChoisissez un nom pour votre espace de travail. Choisissez un modèle de licence supportant les objets Fabric. Appuyez sur Appliquer pour créer votre espace de travail. Une fois l\u0026rsquo;espace de travail créer un rendez-vous dedans.\nDans le panneau de gauche cliquez sur l\u0026rsquo;icône des expériences Fabric, tout en bas, c\u0026rsquo;est Power BI si vous êtes entré par le portail Power BI. Sélectionnez l\u0026rsquo;expérience Fabric Real-Time Analytics. Réouvrez l\u0026rsquo;espace de travail si vous vous retrouvez sur l\u0026rsquo;écran d\u0026rsquo;accueil.\nCréation de la base KQL Nous allons maintenant procéder à la création de la base de données KQL\nAppuyez sur Nouveau Sélectionnez Base de données KQL. Donnez le nom DemoIOT à la base de données. Appuyez sur Créer pour créer la base. Le traitement de création peut prendre quelques secondes.\nCollecte et traitement des données Création du traitement Nous allons maintenant procéder à la création d\u0026rsquo;un traitement d\u0026rsquo;intégration eventstream.\nAppuyez sur Nouveau Sélectionnez Eventstream. Donnez le nom DemoIOT à l\u0026rsquo;eventstream'. Appuyez sur Créer pour créer l\u0026rsquo;eventstream'. La fenêtre d\u0026rsquo;édition du traitement apparait, nous allons créer un point d\u0026rsquo;écoute pour nos capteurs en créant une Custom App dans la source du traitement.\nLe point d\u0026rsquo;écoute est supporté par le service bus Azure.\nAppuyez sur New source. Sélectionnez Custom App. Donnez le nom MesCapteurs à la custom app. Appuyez sur Create pour créer la custom app. Nous allons maintenant copier la chaine de connexion au point d\u0026rsquo;écoute pour paramétrer nos capteurs.\nVérifiez que vous êtes sur les sources de données. Sélectionnez la source MesCapteurs. Appuyez sur l\u0026rsquo;icône d\u0026rsquo;œil sir la ligne Connection string-primary key pour afficher la chaine de connexion. Appuyez sur l\u0026rsquo;icône de copie pour copier la chaine de connexion. Script de simulation de capteur émettant vers notre traitement Nous allons maintenant finaliser notre script simulant nos capteurs en ajoutant l\u0026rsquo;écriture dans le Azure service bus servant de point d\u0026quot;écoute.\nPour cela, remplacez la chaine de caractère REMPLACER_PAR_VOTRE_CHAINE_DE_CONNEXION par la chaine de connexion copiée à l\u0026rsquo;étape précédente sur la ligne 2 du script ci-dessous.\nLe script PowerShell est inspiré de l\u0026rsquo;article Azure Service Bus with PowerShell.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # Votre chaine de connexion $ConnectionString = \u0026#39;REMPLACER_PAR_VOTRE_CHAINE_DE_CONNEXION\u0026#39; # Nombre de message à envoyer $Messages = 100 # Id des capteurs $HardwareID = \u0026#34;12345\u0026#34;, \u0026#34;23456\u0026#34;, \u0026#34;34567\u0026#34;,\u0026#34;45678\u0026#34;,\u0026#34;67890\u0026#34; $Pattern = \u0026#39;Endpoint=(.+);SharedAccessKeyName=(.+);SharedAccessKey=(.+);EntityPath=(.+)\u0026#39; ([uri]$Endpoint),$PolicyName,$Key,$Queue = ($ConnectionString -replace $Pattern,\u0026#39;$1;$2;$3;$4\u0026#39;) -split \u0026#39;;\u0026#39; # Depending on your environment you may need to load the assembly System.Web # $null = [Reflection.Assembly]::LoadWithPartialName(\u0026#34;System.Web\u0026#34;) $UrlEncodedEndpoint = [System.Web.HttpUtility]::UrlEncode($Endpoint) $Expiry = [DateTimeOffset]::Now.ToUnixTimeSeconds() + 3600 $RawSignatureString = \u0026#34;$UrlEncodedEndpoint`n$Expiry\u0026#34; $HMAC = New-Object System.Security.Cryptography.HMACSHA256 $HMAC.Key = [Text.Encoding]::ASCII.GetBytes($Key) $HashBytes = $HMAC.ComputeHash([Text.Encoding]::ASCII.GetBytes($RawSignatureString)) $SignatureString = [Convert]::ToBase64String($HashBytes) $UrlEncodedSignatureString = [System.Web.HttpUtility]::UrlEncode($SignatureString) $SASToken = \u0026#34;SharedAccessSignature sig=$UrlEncodedSignatureString\u0026amp;se=$Expiry\u0026amp;skn=$PolicyName\u0026amp;sr=$UrlEncodedEndpoint\u0026#34; # Boucle d\u0026#39;envoi des messages for ($i = 0; $i -lt $Messages; $i++) { # Création du message à envoyer au format JSON $IOTFakeMessage = @{ HardwareId = \u0026#34;ID-\u0026#34; + $HardwareID[(Get-Random -Minimum 0 -Maximum $HardwareID.Count)] Date = Get-Date CaptorInfo = @{ Temp = Get-Random -Minimum 20 -Maximum 28 Hum = Get-Random -Minimum 40 -Maximum 60 } } | ConvertTo-Json -Depth 99 # Préparation de la requête HTTP $Params = @{ Uri = \u0026#34;https://$($Endpoint.Host)/$Queue/messages\u0026#34; ContentType = \u0026#39;application/json;charset=utf-8\u0026#39; Method = \u0026#39;Post\u0026#39; Body = $IOTFakeMessage Headers = @{ \u0026#39;Authorization\u0026#39; = $SASToken } } # Envoi du message vers le service BUS Invoke-RestMethod @Params # Attente avant le prochain message Start-Sleep -Milliseconds (Get-Random -Minimum 200 -Maximum 4000) } Finalisation du traitement des données Nous allons maintenant finaliser le traitement de données des capteurs en indiquant une destination aux données reçues.\nAppuyez sur New destination. Sélectionnez KQL Database. Donnez le nom StockageMesCapteurs dans Destination name. Sélectionnez l\u0026rsquo;espace de travail. Sélectionnez la base de données KQL. Appuyez sur Create and configure pour continuer. On paramètre la table de destination dans la base KQL : Sélectionnez Nouvelle table. Donnez le nom MesCapteurs à la table. Appuyez sur Suivant : Source pour continuer. Lancez le script PowerShell paramétré avec votre chaine de connexion pour qu\u0026rsquo;il commence à émettre des données.\nSur l\u0026rsquo;écran Source laissez les paramètres par défaut et appuyez sur Suivant : Schéma pour continuer. Choisissez le format de données JSON. Vous pouvez voir que des données arrivent de vos capteurs. Les données de la colonne CaptorInfo sont au format JSON, pour une analyse plus simple nous allons réaliser une transformation de données consistant à créer une colonne par valeur. Cliquez sur le symbole + pour ajouter une colonne. Saisissez Temp dans le nom de la colonne. Appuyez sur Créer pour saisir le chemin de la source de données. Choisir Int dans Type de colonne. Entrez $.CaptorInfo.Temp dans Nouvelle source. Le symbole $ représente la racine du document JSON, nous indiquons ensuite le nom des clés amenant à la valeur qui nous intéresse. Appuyez sur OK pour ajouter la colonne. Vous pouvez voir la valeur saisie pour la nouvelle colonie. Appuyez sur Créer pour ajouter la colonne. Répétez l\u0026rsquo;opération pour la colonne Hum, entrez $.CaptorInfo.Hum dans Nouvelle source. Appuyez sur Créer pour ajouter la colonne. Vous pouvez maintenant voir vos 2 nouvelles colonnes. Vous pouvez supprimer la colonne CaptorInfo. Appuyez sur le chevron vers le bas et sélectionnez Delete column. Appuyez sur Suivant : Récapitulatif pour continuer. Vous avez fini de configurer la destination, appuyer sur Fermer. Votre traitement d\u0026rsquo;intégration ressemble à cela : Restitution des données Pour la restitution des données, nous allons utiliser les assistants disponibles au niveau de la base de données KQL.\nRendez-vous dans votre espace de travail est cliqué sur la base de données KQL. Création du rapport Power BI Vous pouvez voir la base de données KQL que l\u0026rsquo;on a créée. Appuyez sur la table MesCapteurs pour la sélectionner. Appuyer sur les 3 points au niveau du nom de la table pour ouvrir le menu contextuel. Sélectionnez Générer un rapport Power BI. L\u0026rsquo;extraction des données via une requête KQL est faite automatiquement par l\u0026rsquo;assistant. Une fenêtre de création de rapport Power BI apparait. Le jeu de données associé à ce rapport correspond aux données de notre table.\nAjouter un visuel Graphique en courbe dans votre rapport et positionnez-le pour recouvrir la zone de dessin. Dans Données vous retrouvez les champs de votre table extraits par une requête KQL. Positionez les champs sur le graphique : Mettre Date dans Axe X. Mettre Hum et Temp dans Axe Y. Mettre HardwareId dans Petits multiples. Votre graphe doit ressembler à cela. Si aucune donnée n\u0026rsquo;est présente, relancer le script PowerShell et appuyer sur le bouton Actualiser du rapport. Cliquez dans la zone sous les pointillés pour accéder aux paramétrages de la page du rapport Power BI. Rendez-vous dans l\u0026rsquo;onglet de mise en forme. Activez Actualisation de la page en basculant le bouton et cliquez sur le texte pour ouvrir le panneau de configuration. Paramétrez votre page pour une actualisation automatique toutes les 10 secondes. Dans le menu fichier, appuyez sur Enregistrer.\nDonnez un nom à votre rapport. Sélectionnez l\u0026rsquo;espace de travail. Appuyez sur Continuer pour publier le rapport. Vous pouvez maintenant fermer la fenêtre Power BI.\nTest du rapport Power BIPower BI Retournez dans l\u0026rsquo;espace de travail où vous avez sauvegardé le rapport Power BI et ouvrez-le.\nVous pouvez voir ici vos données qui vont se rafraichir automatiquement si le script PowerShell s\u0026rsquo;exécute. Vous pouvez ajouter un filtre sur les 10 dernières minutes de données pour ne pas voir les données des tests précédents. Si le rafraichissement ne se fait pas vous pouvez le forcer via le bouton Actualiser. Lors de mes expérimentations, le rafraichissement automatique du rapport Power BI était plutôt chaotique. Le rafraichissement par le bouton Actualiser rafraichissait bien les données donc c\u0026rsquo;est la partie Power BI qui ne se comporte pas tout à fait comme attendu.\nConclusion Dans ce scénario je simule des capteurs d\u0026rsquo;IOT, mais vous pouvez utiliser cette technique pour n\u0026rsquo;importe quel type de données. Par exemple vous pouvez monitorer vos scripts PowerShell en envoyant directement des données à la base KQL.\nMerci de votre attention.\n","date":"2023-05-30T00:00:00Z","image":"https://blog.ddata.fr/p/fabric-poc-iot/jorge-ramirez-HpFcqb6gUwE-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_3315002_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/fabric-poc-iot/","title":"Données en temps réel dans votre rapport Power BI avec Microsoft Fabric"},{"content":"Allez au-delà des assistants dans Power Query et découvrez quelques usages avancés.\nContexte Vous utilisez Power Query pour intégrer vos données, mais vous voulez aller plus loin avec ?\nJe vous propose dans cet article quelques techniques avancées pour booster votre productivité avec Power Query.\nAu-delà des assistants La première étape du parcours sera d\u0026rsquo;aller au-delà des assistants. En effet, bien que ces derniers soient très utiles, ils ont des limites.\nAfficher la barre de formule Pour cela vous devrez prendre l\u0026rsquo;habitude de lire le code M générer par l\u0026rsquo;assistant à chaque étape, car cela vous permettra :\nDe vérifier que le code est bien générique et adapté à la situation. D\u0026rsquo;optimiser le code écrit par l\u0026rsquo;assistant. Pour réussir à faire cela, il suffit d\u0026rsquo;activer la barre de formule dans l\u0026rsquo;éditeur Power Query.\nDans l\u0026rsquo;éditeur Power Query, allez dans le menu Affichage. Cochez Barre de formule. Vous pourrez voir et modifier le code de l\u0026rsquo;étape en cours de sélection dans la barre de formule. Utiliser la documentation Maintenant que vous avez accès rapidement au code M généré par les assistants, prenez l\u0026rsquo;habitude de jeter un coup d\u0026rsquo;œil sur la documentation des fonctions.\nVous retrouverez la documentation complète sur le site de Microsoft en français ou en anglais. Au vu de la qualité de certaines traductions, personnellement je privilégie la documentation en anglais.\nLe nom des fonctions du langage M est toujours en 2 parties : Le nom de la famille de la fonction suivi du nom de la fonction, les 2 étant séparés par un point.\nAllez dans Fonctions. Vous retrouverez la documentation de chaque fonction dans sa famille. Quand vous êtes dans l\u0026rsquo;aide, profitez-en pour regarder le nom des autres fonctions de la même famille que celle qui vous intéresse, parfois on y trouve de l\u0026rsquo;inspiration ou on découvre de nouvelles choses.\nInsérer du code Power Query manuellement Pour insérer une étape dans votre traitement dans laquelle vous pouvez écrire le code M librement, il vous suffit\nDans l\u0026rsquo;éditeur Power Query, allez dans le menu Ajouter une colonne. Choisissez Colonne personnalisée. Entrez un nom pour votre nouvelle colonne. Tapez votre code M. Vous pouvez ajouter une des colonnes déjà existantes en double cliquant dessus. Vérifiez que le code M est syntaxiquement correct. Appuyez sur OK pour terminer. 🎁 Une petite astuce : quand vous avez une erreur de syntaxe en M, dans 99 % des cas c\u0026rsquo;est dû a une des 2 causes suivantes :\nUne erreur de casse. Le code M est sensible à la casse, cela veut dire que le même texte écrit avec une minuscule au lieu d\u0026rsquo;une majuscule est un terme différent. Exemple \u0026ldquo;Coucou\u0026rdquo; et différent de \u0026ldquo;coucou\u0026rdquo; pour le langage M. Une erreur entre les parenthèses \u0026ldquo;()\u0026rdquo;, accolades \u0026ldquo;{}\u0026rdquo; ou crochet \u0026ldquo;[]\u0026rdquo;. Elles sont soit mal positionnées, soit manquantes (souvent celle qui est fermante). Si le problème ne vient pas d\u0026rsquo;un de ces 2 cas : bon courage.\nExemples d\u0026rsquo;utilisation des fonctions non disponible dans les assistants Parmi les multiples fonctions M voici 2 exemples qui vous permettront de mieux comprendre l\u0026rsquo;intérêt de la démarche décrite ci-dessus.\nText.PadStart \u0026amp; Text.PadEnd Les fonctions M Text.PadStart et Text.PadEnd ne sont pas disponibles au travers des assistants, mais sont d\u0026rsquo;une efficacité redoutable pour mettre en forme proprement nos données.\nEn effet elles permettent de créer des chaines de caractères de taille fixe en remplissant les caractères absents avec une valeur de votre choix.\nPar exemple, pour votre dimension calendrier, vous souhaitez afficher les mois en texte sur 2 caractères, vous pouvez le faire simplement avec Text.PadStart.\nLa formule permet de transformer en une étape le contenu de la colonne Column1 en texte avec la fonction Text.From puis de formater le résultat sur 2 caractères avec un zéro si on a qu\u0026rsquo;un caractère avec la fonction Text.PadStart. Valeur d\u0026rsquo;origine de la colonne Column1 Valeur du résultat de la fonction. Vous pouvez tester cela directement en copiant le code M ci-dessous dans une requête vide à l\u0026rsquo;aide de l\u0026rsquo;éditeur avancé.\n1 2 3 4 5 6 let Source = {1..12}, #\u0026#34;Converti en table\u0026#34; = Table.FromList(Source, Splitter.SplitByNothing(), null, null, ExtraValues.Error), #\u0026#34;Personnalisée ajoutée\u0026#34; = Table.AddColumn(#\u0026#34;Converti en table\u0026#34;, \u0026#34;Mois\u0026#34;, each Text.PadStart(Text.From([Column1]),2,\u0026#34;0\u0026#34;)) in #\u0026#34;Personnalisée ajoutée\u0026#34; Text.Format Les fonctions M Text.Format vous permettent de créer des concaténations de chaines de caractères en gérant la culture à utiliser pour faire l\u0026rsquo;affichage.\nVous pouvez tester cela directement en copiant le code M ci-dessous dans une requête vide à l\u0026rsquo;aide de l\u0026rsquo;éditeur avancé.\n1 2 3 4 5 6 let Source = List.DateTimes(DateTime.LocalNow(),10,#duration(1,0,0,0)), #\u0026#34;Converti en table\u0026#34; = Table.FromList(Source, Splitter.SplitByNothing(), null, null, ExtraValues.Error), #\u0026#34;Personnalisée ajoutée\u0026#34; = Table.AddColumn(#\u0026#34;Converti en table\u0026#34;, \u0026#34;Message avec date US\u0026#34;, each Text.Format(\u0026#34;Message du #{0}\u0026#34;, {[Column1]}, \u0026#34;en-US\u0026#34;)) in #\u0026#34;Personnalisée ajoutée\u0026#34; Saisir sans le point Une astuce qui va vous simplifier la saisie, quand vous saisissez un nom de fonction M dans l\u0026rsquo;éditeur, ne saisissez pas le point afin que l\u0026rsquo;aide à la saisie fonctionne de manière optimale.\nMerci de votre attention.\n","date":"2023-05-28T00:00:00Z","image":"https://blog.ddata.fr/p/power-query-advanced-m-part1/brian-wangenheim-VYOrErmJes4-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_1564048_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-query-advanced-m-part1/","title":"Power Query au-delà des assistants."},{"content":"Vous avez la même intégration de données à répéter ? Par exemple un dossier avec des fichiers Excel au même format ? Power Query vous simplifie la vie avec les fonctions.\nContexte Power Query est un outil formidable pour automatiser vos préparations de données.\nMais souvent vous vous retrouvez à refaire les mêmes requêtes avec juste un paramètre qui change, c\u0026rsquo;est à ce moment que vous devez apprendre les fonctions afin d\u0026rsquo;automatiser ce type de tâche.\nPour bien comprendre cet article il est recommandé de maitriser l\u0026rsquo;éditeur avancé de Power Query présenter dans l\u0026rsquo;article Bien utiliser l\u0026rsquo;éditeur avancé de Power Query\nCas d\u0026rsquo;usage Les cas d\u0026rsquo;usages des fonctions sont multiples, voici quelques exemples classiques à garder en tête :\nAppliquer un même ensemble de transformations sur des fichiers, Excel par exemple, contenu dans un dossier. Appliquer un même ensemble de transformations sur les feuilles d\u0026rsquo;un fichier Excel. Appliquer un ensemble de transformations dans plusieurs requêtes Power Query différentes. Les exemples ci-dessus ont tous en commun l\u0026rsquo;usage de fonction.\nEn informatique une fonction est une portion de code réalisant une tâche spécifique.\nUne fonction est une requête Power Query spécifique, elle est donc :\nÉcrite et maintenue en dehors des autres requêtes Power Query. Appelable par n\u0026rsquo;importe quelles autres requêtes Power Query. Elle peut prendre ou non des paramètres en entrée afin de variabiliser son exécution. Exemple Imaginons le traitement dans lequel vous souhaitez intégrer un flux RSS sous la forme d\u0026rsquo;une table dans Power Query. Les étapes de transformation seront les suivantes :\nLe code brut Power Query est le suivant :\n1 2 3 4 5 6 let Source = Xml.Tables(Web.Contents(\u0026#34;https://www.nasa.gov/rss/dyn/breaking_news.rss\u0026#34;)), channel = Source{0}[channel], item = channel{0}[item] in item Maintenant que nous avons fait cette première intégration, nous souhaitons ajoute de nouveaux flux RSS en plus dans la même table. Nous avons 2 manières pour faire cela.\nSans l\u0026rsquo;usage d\u0026rsquo;une fonction Vous devrez\nCréer une requête par flux RSS Fusionner toutes les requêtes ensemble. Cette solution est fastidieuse et difficilement maintenable.\nAvec une fonction Power Query Nous allons avoir besoin de seulement 2 requêtes Power Query :\nUne requête contenant la fonction Une requête contenant la liste des flux RSS que l\u0026rsquo;on souhaite traiter. Nous allons donc commencer par transformer le code précédent en fonction. Pour faire cela, nous devons analyser le code et réfléchir à l\u0026rsquo;automatisation que l\u0026rsquo;on veut mettre en place.\nLes numéros de lignes que je cite correspondent à ceux visibles dans l\u0026rsquo;éditeur avancé de la copie d\u0026rsquo;écran précédente.\nSur la ligne 2, on lit le contenu d\u0026rsquo;une URL via la fonction Web.Contents et on envoie le résultat de cette lecture à la fonction Xml.Tables afin d\u0026rsquo;interpréter le résultat en tant que table XML. Le résultat est stocké dans Source. Sur la ligne 3, on lit la colonne channel ,via [channel], de la première ligne, via {0}, de Source. En Power Query la première ligne à l\u0026rsquo;indice zéro. Le résultat est stocké dans channel. Sur la ligne 4, on lit la colonne item ,via [item], de la première ligne, via {0}, de channel. Le résultat est stocké dans item. Le contenu de item est renvoyé par la requête Power Query. Maintenant que nous avons analysé le code, on se rend compte que si on variabilise l\u0026rsquo;URL, on peut répéter les autres opérations sur des données ayant la même structure.\nLa fonction Power Query sera donc la suivante :\nLe code brut Power Query est le suivant :\n1 2 3 4 5 6 7 8 9 let readFeed = (URLFeed) =\u0026gt; let Source = Xml.Tables(Web.Contents(URLFeed)), channel = Source{0}[channel], item = channel{0}[item] in item in readFeed Sur la ligne 1, nous déclarons la fonction readFeed qui prend comme paramètre URLFeed. Le bloc de code entre la ligne 2 et 7 correspond à notre code de l\u0026rsquo;exemple initial, vous remarquerez sur la ligne 3 une modification, on a remplacé la valeur de l\u0026rsquo;URL par la valeur du paramètre URLFeed de la fonction. Sur les lignes 8 et 9 la fonction renvoie la valeur retournée par le bloc de code entre les lignes 2 et 7. Pour tester votre fonction, sélectionnez-la. Remplir le paramètre avec la valeur souhaitée. Appuyez sur le bouton Appeler. Une nouvelle requête est créée avec l\u0026rsquo;appel de la fonction et son résultat. ⚠ Notez bien que la requête Power Query qui contient la fonction s\u0026rsquo;appelle fctNasaFeed, comme on le voit dans la liste des requêtes. Le nom de la fonction readFeed utiliser dans le code Power Query n\u0026rsquo;est accessible que dans le code Power Query, les appels de la fonction par d\u0026rsquo;autres requêtes Power Query ce fera par le nom de la requête fctNasaFeed.\nTestez toujours votre fonction une fois pour être sûr que cette dernière fonctionne.\nNous allons maintenant utiliser notre fonction sur un ensemble d\u0026rsquo;URL, pour faire cela nous avons besoin de créer une table contenant les données sur lesquelles on souhaite appliquer la fonction.\nLe code brut Power Query est le suivant :\n1 2 3 4 5 let Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\u0026#34;jc0xD8IgEIbhv0KYG9gdjQ4uOjiS5nIKpUQ9TI9K+PdiqyZOdf3y3nPGyPXg8BLIi73LLBvZp3TnldY5Z0XIqHx86IFZ20L69I6BaqzqKtvGyK0dz5hCpL8M96l/jN0NvROxE6l3YoNlSbl6CK8TiB3UE7BYvtaBJuWYpjdLUqQa89xCHWemfQI=\u0026#34;, BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type nullable text) meta [Serialized.Text = true]) in type table [Nom = _t, Feed = _t]), #\u0026#34;Type modifié\u0026#34; = Table.TransformColumnTypes(Source,{{\u0026#34;Nom\u0026#34;, type text}, {\u0026#34;Feed\u0026#34;, type text}}) in #\u0026#34;Type modifié\u0026#34; Cette table, créer avec via l\u0026rsquo;option Entrer des données du menu Accueil, contient 2 colonnes, une avec le nom du flux et la seconde avec l\u0026rsquo;URL.\nNous allons maintenant appliquer la fonction sur chaque ligne de cette table :\nSélectionnez la requête FeedList créer avec le code ci-dessus. Allez dans le menu Ajouter une colonne. Sélectionnez Colonne personnalisée. Saisissez un nom pour votre nouvelle colonne. Rentrez le code fctNasaFeed([Feed]) pour appeler la fonction. Appuyez sur OK pour valider. La fonction sera appelée pour chaque ligne de la table en utilisant la valeur de la colonne [Feed] comme paramètre de la fonction.\nLe résultat est donc le suivant :\nVous pouvez constater que nous avons une table dans chaque cellule de la nouvelle colonne. C\u0026rsquo;est le résultat de la fonction. Pour voir le contenu d\u0026rsquo;une table, cliquer dans la zone blanche de la cellule, pas sur le texte table. Vous voyez en bas le contenu de la table. Si vous cliquez sur le texte table, une nouvelle étape et automatiquement créer avec le contenu de la table et vous devrez supprimer manuellement cette étape pour continuer cet exemple.\nNous allons maintenant obtenir une table avec l\u0026rsquo;ensemble des valeurs.\nAppuyez sur le bouton développé à droite de l\u0026rsquo;entête de la colonne. Sélectionner les champs que vous souhaitez récupérer. Désélectionner Utiliser le nom de la colonne d\u0026rsquo;origine comme préfixe car nous n’avons pas de risque de nom de colonne en double. Appuyez sur OK pour valider. Vous avez maintenant une table avec l\u0026rsquo;ensemble des données en provenance des différents flux RSS, le tout appelé à l\u0026rsquo;aide d\u0026rsquo;une seul fonction Power Query.\nLe code final brut Power Query est le suivant :\n1 2 3 4 5 6 7 let Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\u0026#34;jc0xD8IgEIbhv0KYG9gdjQ4uOjiS5nIKpUQ9TI9K+PdiqyZOdf3y3nPGyPXg8BLIi73LLBvZp3TnldY5Z0XIqHx86IFZ20L69I6BaqzqKtvGyK0dz5hCpL8M96l/jN0NvROxE6l3YoNlSbl6CK8TiB3UE7BYvtaBJuWYpjdLUqQa89xCHWemfQI=\u0026#34;, BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type nullable text) meta [Serialized.Text = true]) in type table [Nom = _t, Feed = _t]), #\u0026#34;Type modifié\u0026#34; = Table.TransformColumnTypes(Source,{{\u0026#34;Nom\u0026#34;, type text}, {\u0026#34;Feed\u0026#34;, type text}}), #\u0026#34;Personnalisée ajoutée\u0026#34; = Table.AddColumn(#\u0026#34;Type modifié\u0026#34;, \u0026#34;Retour de la fonction\u0026#34;, each fctNasaFeed([Feed])), #\u0026#34;Retour de la fonction développé\u0026#34; = Table.ExpandTableColumn(#\u0026#34;Personnalisée ajoutée\u0026#34;, \u0026#34;Retour de la fonction\u0026#34;, {\u0026#34;title\u0026#34;, \u0026#34;link\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;enclosure\u0026#34;, \u0026#34;guid\u0026#34;, \u0026#34;pubDate\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;http://purl.org/dc/elements/1.1/\u0026#34;}, {\u0026#34;title\u0026#34;, \u0026#34;link\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;enclosure\u0026#34;, \u0026#34;guid\u0026#34;, \u0026#34;pubDate\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;http://purl.org/dc/elements/1.1/\u0026#34;}) in #\u0026#34;Retour de la fonction développé\u0026#34; Aller plus loin avec les fonctions Table utiliser pour appeler la fonction Comme nous l\u0026rsquo;avons vu dans l\u0026rsquo;exemple ci-dessus, pour utiliser une fonction, il faut avoir une table sur laquelle l\u0026rsquo;appliquer. Classiquement cette table peut être :\nLa liste des fichiers d\u0026rsquo;un dossier et ses sous-dossiers, locaux ou dans SharePoint. La liste des onglets d\u0026rsquo;un classeur Excel, cette liste est disponible dans l\u0026rsquo;étape suivant l\u0026rsquo;ouverture du fichier Excel dans Power Query (le résultat de la fonction Power Query Excel.Workbook). La liste des tables d\u0026rsquo;une base de données. Bien entendu cette liste n\u0026rsquo;est pas exhaustive.\nValeur en sortie de la fonction Dans notre exemple, la fonction a renvoyé une table. Bien entendu, une fonction peut renvoyer tous les types de données supportées en sortie d\u0026rsquo;une requête Power Query, comme une chaine de caractère, une date ou une valeur numérique.\nMerci de votre attention.\n","date":"2023-05-21T00:00:00Z","image":"https://blog.ddata.fr/p/power-query-function/chinh-le-duc-poK5c-QWy1E-unsplash_hu1bbec3486f773b072d538975f65a80c0_2311333_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-query-function/","title":"Power Query le pouvoir des fonctions."},{"content":"Il est temps d\u0026rsquo;apprendre à mal utiliser le service Power BI !\nJe me propose de vous présenter un florilège de mauvaises pratiques à appliquer dans votre service Power BI afin de maximiser les chances d\u0026rsquo;échec de votre projet.\nNe formez pas vos utilisateurs finaux Vos utilisateurs finaux, ceux qui consomme les rapports n\u0026rsquo;ont pas besoin d\u0026rsquo;apprendre à utiliser le filtrage interactif des visuels ni de découvrir toutes les fonctions qui font d\u0026rsquo;un rapport Power BI un produit agréable à utiliser. D\u0026rsquo;ailleurs un simple PDF devrait leur suffire plutôt qu\u0026rsquo;un outil de self-service BI.\nNe mettez pas de page de documentation dans vos rapports Pas besoin d\u0026rsquo;ajouter des pages de documentation dans vos rapports, ces pages sont une perte de temps, car vos utilisateurs savent de manière innée qu\u0026rsquo;elles sont les règles de calcul derrière chaque mesure de votre modèle. Et s’ils ne savent pas, ils vous demanderont, répéter 50 fois la même chose à 50 personnes vous permettra de justifier de votre temps de travail.\nPartager vos rapports en direct Partager directement vos rapports, un par un depuis votre espace de travail. Pas besoin de structurer cela au sein d\u0026rsquo;une application Power BI ni de pouvoir monitor facilement qui à accès à quel rapport.\nDiffuser vos rapports en partageant les espaces de travail Vous avez besoin de partager plusieurs rapports d\u0026rsquo;un coup, inviter vos consommateurs dans vos espaces de travail, pas besoin des applications Power BI et plus on est de fou plus on rit ! En plus vous pouvez leur donner des droits pour qu\u0026rsquo;ils modifient eux-mêmes les rapports. De plus ils peuvent voir tous les travaux en cours et consulter des rapports non finalisés.\nMettez tout le monde administrateur des espaces de travail Il existe 4 rôles différents pour les membres d\u0026rsquo;un espace de travail, pourquoi vous embêter à comprendre à quoi cela peut servir ? Le plus simple est de mettre le niveau de droit maximum à tout le monde. Tous administrateurs comme cela pas de jaloux.\nLaissez les paramétrages globaux par défaut Dans l\u0026rsquo;administration du service Power BI on retrouve un grand nombre de paramétrages vous permettant d\u0026rsquo;adapter le service à vos besoins, mais vous pouvez faire confiance à l\u0026rsquo;éditeur pour que la valeur par défaut soit en cohérence avec votre politique de sécurité d\u0026rsquo;entreprise, après tout c\u0026rsquo;est vous le client.\nAffecter les droits au niveau utilisateur C\u0026rsquo;est toujours plus simple de voir la liste des utilisateurs qui ont des droits alors n\u0026rsquo;utilisez pas les groupes 365 ou de sécurité, car cela pourrait simplifier l\u0026rsquo;administration de vos projets Power BI. Au moins quand vous avez 500 utilisateurs à affecter manuellement cela justifie votre journée de travail.\nNe versionnez pas les fichiers publiés Quand vous publiez un rapport, il remplace le précédent, comme vous ne faites jamais d\u0026rsquo;erreur pas besoin de conserver un historique des anciennes versions pour revenir en arrière si besoin.\nNe monitor pas le service Power BI Pourquoi mettre en place un outil de monitoring de Power BI alors que le rapport par défaut vous apporte toutes les informations nécessaires pour ne pas monitor correctement votre service.\nLaissez tous le monde installer des passerelles Les passerelles permettent d\u0026rsquo;extraire des données de vos réseaux d\u0026rsquo;entreprise pour les exposer dans le service Power BI. Pas besoin de surveiller qui sort quoi comme données et comment elles sont utilisées ? Laissez vos utilisateurs libres d\u0026rsquo;installer les passerelles.\nNe personnalisez pas le service Power BI Ne prenez pas 2 minutes pour ajouter votre logo et choisir une couleur par défaut.\nEn effet quelle est l\u0026rsquo;utilité d\u0026rsquo;une identité d\u0026rsquo;entreprise ?\nN\u0026rsquo;allez donc pas dans cet écran :\nUtiliser la fonction \u0026ldquo;Publier sur le web public\u0026rdquo; pour diffuser vos données d\u0026rsquo;entreprise sans payer de licence La fonction Publier sur le web public vous permet de publier un rapport Power BI qui est consultable sans payer de licence pour tous les utilisateurs qui consomme ce rapport.\nN\u0026rsquo;hésiter pas a utiliser cette méthode pour diffuser de manière publique vos données comptables et financières. Pensez à ajouter les salaires des employées.\nBien sûr l\u0026rsquo;adresse Web de ce rapport n\u0026rsquo;est connue que de vous seul, mais aussi de tous les utilisateurs avec qui vous l\u0026rsquo;avez partagé et de tous ceux avec qui ils ont eux-mêmes partagé. Cerise sur le gâteau, une adresse Web passe en clair dans tous les équipements réseau qui traitent votre demande.\nGrâce à cette méthode vous avez la presque certitude de voir exposé vos données privées à n\u0026rsquo;importe qui, alors n\u0026rsquo;hésitez pas à l\u0026rsquo;utiliser pour autre chose que la diffusion de données publique c\u0026rsquo;est gratuit.\nBien entendu je ne garantis pas l\u0026rsquo;échec de votre projet même si vous appliquez toutes ces mauvaises pratiques : l\u0026rsquo;informatique n\u0026rsquo;est pas une science exacte, on n’est pas à l\u0026rsquo;abri d\u0026rsquo;un succès!\nMerci de votre attention.\n","date":"2023-05-14T00:00:00Z","image":"https://blog.ddata.fr/p/power-bi-service-bad-practice/jas-min-egqR_zUd4NI-unsplash_hu3d03a01dcc18bc5be0e67db3d8d209a6_1313326_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-bi-service-bad-practice/","title":"Guide des mauvaises pratiques du service Power BI."},{"content":"Perdu dans les licences de Power BI ? Faisons le point ensemble.\nContexte Power Bi n\u0026rsquo;est pas gratuit. Pourtant, beaucoup de fonctionnalités sont accessibles gratuitement. C\u0026rsquo;est ce que j\u0026rsquo;appelle la technique du pot de confiture, on vous laisse mettre les doigts dedans et quand vous aimez vous devez acheter.\nLes produits Power BI Il existe 2 types de produit Power BI que vous pouvez acheter\nLes licences utilisateur que l\u0026rsquo;on affecte à des personnes. Les capacités premium qui permettent de créer des espaces de travail premium, c\u0026rsquo;est de l\u0026rsquo;infrastructure que vous louez auprès de Microsoft. Les informations importantes à retenir pour les licences :\nEn général vous devez payer à partir du moment où vous diffuser vos rapports Power BI. Dès que vous accédez au service Power BI vous devez avoir une licence. Un niveau de licence vous octroie des usages en fonction des types d\u0026rsquo;espace de travail. Les usages peuvent être répartis sur 2 profils utilisateur : Créateur : ce sont les créateurs de contenus, ils accèdent aux espaces de travail pour collaborer et publier des contenus Consomateur : ce sont les utilisateurs qui consomment les contenus au travers des applications. Les licences Power BI Voici un résumé des usages possibles par niveau de licence.\nLicence gratuite Coût : 0 €\nCréateur :\nMon espace de travail Consommateur :\nEspace de travail Premium Licence Pro Coût : 9,4 € par utilisateur / mois\nA les mêmes usages que la licence gratuite avec en plus :\nCréateur :\nEspace de travail Espace de travail Premium Consommateur :\nEspace de travail Licence Premium par utilisateur Coût : 18,7 € par utilisateur / mois\nA les mêmes usages que la licence Pro avec en plus :\nCréateur :\nEspace de travail Premium par utilisateur Consommateur :\nEspace de travail Premium par utilisateur Les capacités Premium À partir de 4 675,60 € par capacité / mois\nUne capacité Premium permet de créer des espaces de travail Premium.\nIl existe différents types de capacité Premium :\nCapacité de type P : Capacité permettant de consulter vos rapports depuis le portail Power BI. Capacité de type EM : Capacité permettant de consulter vos rapports depuis Sharepoint. Capacité de type A : Capacité permettant de consulter vos rapports depuis vos applications métiers (Power BI Embeded). Les capacités ont un numéro après leur lettre qui donne le niveau de puissance, vous retrouverez plus d\u0026rsquo;information ici. Ce qu\u0026rsquo;il faut retenir c\u0026rsquo;est que l\u0026rsquo;on multiplie par 2 la puissance et le prix entre chaque niveau (P3 = 2 x P2 = 4 x P1).\nVous pouvez louer plusieurs capacités de type et de puissance différente.\nLes types d\u0026rsquo;espace de travail Comme nous venons de le voir, les différents produits Power BI permettent l\u0026rsquo;accès à différent niveau d\u0026rsquo;espace de travail. Je vais maintenant vous présenter les principales différences entre ces espaces de travail\nMon espace de travail Permets seulement le partage direct de contenu Normal Est un espace de travail collaboratif. Il supporte le partage de contenu via les applications. Il supporte les flux de données. Les modèles de données sont limités à 1 Go. Permets 8 rafraichissements d\u0026rsquo;un jeu de données par jour. 10 Go de stockage multiplié par le nombre de licences Pro. Premium Est un espace de travail collaboratif. Il supporte le partage de contenu via les applications. Il supporte les fonctions avancées des flux de données. Les modèles de données sont limités à 400 Go. Permets 48 rafraichissements d\u0026rsquo;un jeu de données par jour 100 To de stockage par capacité. Support des datamarts, des fonctions IA avancées et d\u0026rsquo;un point de terminaison XMLA. Premium par utilisateur Est un espace de travail collaboratif. Il supporte le partage de contenu via les applications. Il supporte les fonctions avancées des flux de données. Les modèles de données sont limités à 100 Go. Permets 48 rafraichissements d\u0026rsquo;un jeu de données par jour. 100 To de stockage au global pour ce type d\u0026rsquo;espace de travail . Support des datamarts, des fonctions IA avancées et d\u0026rsquo;un point de terminaison XMLA. Merci de votre attention.\n","date":"2023-05-07T00:00:00Z","image":"https://blog.ddata.fr/p/comprendre-les-licences-power-bi/cristina-matos-albers-Ltv7a5m8i4c-unsplash_hu9694657f3c697ac35f9080995597d89a_217872_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/comprendre-les-licences-power-bi/","title":"Comprendre les licences Power BI"},{"content":"Et si vous pouviez maintenir un référentiel d\u0026rsquo;entreprise dans le service Power BI et le consommer dans Excel ?\nContexte Dans votre modèle de données, vous avez des données qui sont des référentiels d\u0026rsquo;entreprise. Avec le service Power BI vous avez la possibilité d\u0026rsquo;utiliser ces référentiels directement depuis Excel.\nJe vous montre comment faire.\nCréation d\u0026rsquo;une table recommandée Pour créer une table recommandée, cela se passe dans Power BI Desktop.\nPour cette démonstration j\u0026rsquo;ai créé une simple table directement dans Power BI Desktop, via la fonction Entrer des données.\nLe résultat est le suivant :\nMaintenant que vos données sont disponibles, nous allons créer la table recommandée.\nAllez dans Vue de modèle. Sélectionnez la table que vous souhaitez proposer. Allez dans les propriétés. Basculez le sélecteur Est une table proposée sur Oui. Un écran de configuration apparait.\nSaisissez une description. Sélectionnez la colonne des étiquettes de données. Il s\u0026rsquo;agit de la colonne qui va permettre à vos utilisateurs de retrouver une ligne de données depuis Excel. Sélectionnez la colonne de clé. Il s\u0026rsquo;agit de la colonne qui va permettre à Excel d\u0026rsquo;identifier une ligne de manière unique. Appuyez sur Enregistrer pour terminer et enregistrerez vos modifications. Ne publiez pas tout de suite votre rapport.\nUtilisation des tables recommandées Ouvrez Excel et rendez-vous dans le bandeau de menu Données.\nOuvrez l\u0026rsquo;élément de menu Type de données. Vous pouvez constater que vous n\u0026rsquo;avez pas la table recommandée que vous venez de créer. Fermer votre Excel.\nSi cela ne fonctionne pas, essayer avec Excel Web car vous n\u0026rsquo;avez peut être pas une version d\u0026rsquo;Excel 365 sur votre poste de travail.\nNous allons maintenant publier notre rapport dans le service Power BI.\nUne fois la publication faite, nous allons réouvrir Excel et retourner dans le bandeau de menu Données.\nOuvrez l\u0026rsquo;élément de menu Type de données. Vous pouvez constater que vous avez maintenant une section De votre organisation. Dans cette section, vous retrouvez les données en provenance de votre jeu de données publié dans le service Power BI. Il se peut qu\u0026rsquo;il y ait un délai entre la publication et la visibilité dans Excel. N\u0026rsquo;hésitez pas à fermer et réouvrir Excel si nécessaire. De plus votre Excel doit être connecté au service Office 365, votre profil doit apparaitre en haut à droite de la barre de titre d\u0026rsquo;Excel.\nMaintenant que votre référentiel est visible dans Excel, voilà comment vous allez pouvoir l\u0026rsquo;utiliser.\nDans votre feuille Excel, saisir dans des cellules des valeurs correspondant aux étiquettes de ligne.\nSélectionnez vos cellules. Ouvrez l\u0026rsquo;élément de menu Type de données. Sélectionnez le type de données que vous avez créé. Les données dans vos cellules apparaissent maintenant avec une icône du type de données.\nVous pouvez maintenant créer des formules Excel référençant vos données d\u0026rsquo;entreprise. Les autres champs de la table recommandée sont disponibles. Voici le résultat final des formules appliqué à chaque ligne :\nLes données étant liées à votre jeu de données Power BI, elles pourront donc être rafraichies directement dans Excel.\nParamétrages nécessaires Pour que les tables recommandées fonctionnent, il est nécessaire de vérifier que dans les paramètres du client de l\u0026rsquo;administration du service Power BI, que la fonctionnalité Autoriser les connexions aux tables proposées soit activée.\nLimitations Vous retrouverez l\u0026rsquo;ensemble des limitations dans la documentation officielle https://learn.microsoft.com/fr-fr/power-bi/collaborate-share/service-excel-featured-tables#considerations-and-limitations.\nMerci de votre attention.\n","date":"2023-04-30T00:00:00Z","image":"https://blog.ddata.fr/p/tables-recommandees-powerbi/micheile-dot-com-SoT4-mZhyhE-unsplash_hucaac4cc3511e7b1acfab0972cfec5559_699224_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/tables-recommandees-powerbi/","title":"Vos référentiels d'entreprise directement dans Excel."},{"content":"Vous souhaitez mettre à jour vos données d\u0026rsquo;entreprise dans le service Power BI ?\nVous avez donc besoin d\u0026rsquo;une passerelle de données Power BI.\nContexte Dans un projet Power BI vous pouvez intégrer 2 types de données :\nLes données cloud, type SharePoint Online ou Azure SQL. Les données de votre réseau d\u0026rsquo;entreprise non accessible depuis Internet. Quand vous publiez un rapport dans le service Power BI vous souhaitez pouvoir automatiser le rafraichissement de vos données. Pour que le service Power BI puisse accéder à vos données d\u0026rsquo;entreprise, vous devez installer une passerelle de données Power BI.\nComment marche la passerelle de données Les jeux de données et les flux de données créent des demandes de requêtes à exécuter sur la passerelle lors du rafraichissement des données. Le service Azure Service Bus crée un message crypter contenant la requête et les informations liées aux connexions aux sources de données à utiliser. La passerelle Power BI installée sur votre réseau interne lit la file d\u0026rsquo;attente des demandes dans le service Azure Service Bus. Les données sont décryptées par la passerelle. Il s\u0026rsquo;agit donc seulement de flux sortant HTTPS de votre réseau. Ce n\u0026rsquo;est pas Azure ou Power BI qui entre dans votre réseau, c\u0026rsquo;est la passerelle qui écoute les demandes. Les requêtes sont envoyées par la passerelle et exécutées sur vos sources de données locales. Le résultat est renvoyé vers le service Power BI sous forme cryptée, où il sera intégré dans le jeu de données ou le flux de données demandeur. Type de passerelle Il existe 2 versions de la passerelle de données Power BI. Les 2 versions sont gratuites.\nStandard La passerelle standard est la passerelle à utiliser dans un cadre professionnel, elle permet de partager vos données d\u0026rsquo;entreprise avec Power BI mais aussi d\u0026rsquo;autres services de Microsoft.\nElle est utilisable par tous les utilisateurs de l\u0026rsquo;entreprise autorisés.\nIl est possible de créer un cluster de passerelle, plusieurs machines faisant tourner la même passerelle, afin d\u0026rsquo;adresser des scénarii de haute disponibilité.\nPersonnel La passerelle personnelle permet d\u0026rsquo;accéder aux données depuis un profil utilisateur unique. Sauf cas d\u0026rsquo;usage particulier, évitez de l\u0026rsquo;utiliser et privilégiez la passerelle standard qui est adaptée à un usage professionnel.\nLes cas d\u0026rsquo;usages particuliers peuvent être :\nPermettre l\u0026rsquo;accès à des données sur un poste de travail particulier. Créer un poc rapide avec des données d\u0026rsquo;entreprise. Dans tous les cas, posez-vous la question du bon usage de vos données dans un cadre professionnel.\nInstallation de la passerelle Pour installer la passerelle standard, téléchargez le programme à l\u0026rsquo;adresse suivante https://powerbi.microsoft.com/fr-fr/gateway/ puis lancez-le.\nLors de la phase d\u0026rsquo;installation, vous devrez fournir 2 informations essentielles :\nl\u0026rsquo;utilisateur enregistrant la passerelle dans Power BI, en vous authentifiant avec son profil, cet utilisateur n\u0026rsquo;est pas forcément votre profil local. Il sera l\u0026rsquo;administrateur par défaut de la passerelle. Une clé de récupération, cette clé vous permettra d\u0026rsquo;installer la passerelle sur une nouvelle machine ou un nœud supplémentaire dans le cluster de passerelle. Cette clé permet d\u0026rsquo;accéder aux connexions enregistrées pour cette passerelle. Entrez l\u0026rsquo;adresse de l\u0026rsquo;administrateur par défaut de la passerelle. Appuyez sur Se connecter, pour vous identifier avec le profil administrateur. Choisissez le type d\u0026rsquo;installation çà faire, vous avez le choix entre créer une nouvelle passerelle et interagir avec une déjà enregistrer dans le service Power BI. Appuyez sur Suivant pour continuer. Saisir le nom de votre passerelle Cochez cette case pour ajouter la machine sur laquelle vous installez dans un cluster de passerelle existant. Par défaut une passerelle est un cluster à 1 nœud. Tapez la clé de récupération. Cette clé permet l\u0026rsquo;encodage/décodage des messages échangés entre la passerelle et le service Power BI. Si vous perdez cette clé, vous ne pourrez plus décrypter les sources de données liées à la passerelle. Appuyez sur Configurer pour terminer la configuration de votre passerelle. L\u0026rsquo;écran suivant vous permet d\u0026rsquo;accéder à des paramètres avancés et des informations concernant la passerelle. On va tout laisser par défaut.\nUne fois la passerelle installée, rendez-vous dans l\u0026rsquo;administration des passerelles depuis le portail du service Power BI.\nCliquez sur la roue crantée. Cliquez sur Gérer des connexions et des passerelles. Allez dans Passerelle de données locales. Vous trouvez la liste des passerelles installées. Cliquez sur les 3 points pour gérer les utilisateurs et paramètres de la passerelle ou supprimer la passerelle. Comprendre les sources de données L\u0026rsquo;accès aux données est réalisé à l\u0026rsquo;aide de sources de données. Cette notion de sources de données permet de réaliser une bonne ségrégation des informations de sécurité.\nCelui qui créer la source de données est le sachant des informations de connexion notamment le login et mot de passe. En général c\u0026rsquo;est une personne de l\u0026rsquo;IT. Celui qui utilise la source de données est le propriétaire du jeu de données, il n\u0026rsquo;a pas besoin de connaitre le login et mot de passe utilisé pour accéder aux données. Une personne avec le rôle administrateur de la passerelle ou créateur de connexion crée une source de données pour une passerelle donnée. Une personne avec le rôle administrateur de la passerelle ou créateur de connexion avec repartage autorise des utilisateurs à utiliser cette source de données. Le propriétaire d\u0026rsquo;un jeu de données affecte les sources de données qu\u0026rsquo;il peut utiliser à son jeu de données. Créer une source de données Pour créer une source de données, rendez-vous dans l\u0026rsquo;administration des passerelles depuis le portail du service Power BI.\nCliquez sur la roue crantée. Cliquez sur Gérer des connexions et des passerelles. Allez dans Sources de données. Appuyez sur Nouveau. Choisissez la passerelle pour laquelle vous créer la source de données. La passerelle doit avoir accès à cette source sur votre réseau local. Donnez un nom à votre source de données. Sélectionnez le type de source de données dans la liste. En fonction du type de la source de données renseignez les informations permettant de s\u0026quot;y connecter. Cette partie est différente pour chaque type de source de données. Donnez le niveau de confidentialité de la source de données. Appuyez sur Créer pour ajouter cette source de données. Partager une source de données Une fois la source de données créer, vous pouvez déléguer son utilisation aux personnes en ayant besoin.\nAllez dans Sources de données. Cliquez sur les 3 points pour gérer les utilisateurs. Recherchez les utilisateurs à ajouter. Sélectionnez un nouvel utilisateur ou un utilisateur existant. Configurez le niveau de droit souhaité. Appuyez sur Partager pour valider vos modifications. Configurer votre jeu de données pour utiliser la passerelle Pour configurer l\u0026rsquo;utilisation de la passerelle sur un jeu de données, vous devez utiliser un jeu de données consommant des données sur l\u0026rsquo;infrastructure locale de l\u0026rsquo;entreprise.\nRendez-vous dans les paramètres du jeu de données pour configurer la passerelle. Pour vous y rendre, allez sur les 3 points de la ligne du jeu de données dans votre espace de travail et choisissez Paramètres.\nDans ce bloc, vous avez toutes les informations concernant la passerelle. Si ce bloc n\u0026rsquo;apparait pas, vous ne devez pas avoir le droit d\u0026rsquo;utiliser les sources de données nécessaires. Si ce bloc apparait plusieurs fois c\u0026rsquo;est que vous avez plusieurs passerelles compatibles avec votre source de données. Vous trouverez ici la liste des sources de données de la passerelle compatible avec la signature (3) de votre source de données dans Power Query. Ceci est la signature de votre source de données dans Power Query. Appuyez sur Appliquer pour enregistrer vos modifications, puis tester le rafraichissement de votre jeu de données. Le service Power BI va rechercher des sources de données configurées dans votre passerelle correspondant à la signature de votre source de données dans votre jeu de données. Cette signature est constituée :\nDu type de source de données. Des informations de connexions spécifiques au type de données. Vous pouvez rendre cela dynamique en utilisant des paramètres dans vos requêtes Power Query. Cela vous permet de variabiliser des éléments spécifiques, par exemple modifié après publication du jeu de donner le nom de serveur SQL.\nUn autre point d\u0026rsquo;attention, vous pouvez remplacer des sources de données de type fichier dans Power Query par source de données de type dossier dans la passerelle à condition que le chemin de dossier soit un parent de celui du fichier.\nLimitation Une limitation importante à connaitre est que vous ne pouvez utilisez qu\u0026rsquo;une même passerelle pour un jeu de données. Cela à pour conséquence que toutes les connexions dont vous avez besoin doivent être disponible sur cette passerelle.\nMerci de votre attention.\n","date":"2023-04-23T00:00:00Z","image":"https://blog.ddata.fr/p/power-bi-gateway/jocelyn-allen-GfujWJ6TGj0-unsplash_hu81c52007d2e7fffec5d58774b441b07a_2250510_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-bi-gateway/","title":"Installer et configurer votre passerelle de données pour Power BI."},{"content":"L\u0026rsquo;informatique devient cloud. Mais que se cache-t-il derrière ce mot ?\nJe vous propose de passer en revue les points qu\u0026rsquo;il me semble important à comprendre autour du Cloud.\nContexte Depuis quelques années l\u0026rsquo;informatique d\u0026rsquo;entreprise classique bascule vers le Cloud.\nPour repartir des bases, ce que l\u0026rsquo;on appelle informatique d\u0026rsquo;entreprise est un ensemble de serveurs (ordinateur spécialisé) délivrant des services aux utilisateurs de l\u0026rsquo;entreprise.\nLes services délivrés sont ceux de vos logiciels métiers et sont composé d\u0026rsquo;un sous-ensemble de service informatique de base, tel que :\nles serveurs web les serveurs de messagerie les serveurs de fichiers les serveurs de base de données En général ces serveurs sont stockés dans des datacenters qui sont des lieux sécurisés en terme :\nd\u0026rsquo;accès d\u0026rsquo;alimentation électrique de connectivité réseau de climatisation. Un service informatique peut être décomposé ainsi :\nRéseau : permets au service de communiquer. Stockage : permets de stocker de l\u0026rsquo;information, du code des programmes aux données utilisateur. Serveur : l\u0026rsquo;ordinateur lui-même. Virtualisation : permets de partager une machine physique en plusieurs machines virtuelles. Système d\u0026rsquo;exploitation : en général Linux ou Windows Server. Middleware : les logiciels tels que les serveurs web ou les serveurs de bases de données. Runtime : vos programmes métiers. Données utilisateur : vos données. Application : votre application utilisant toutes les couches précédentes. Le cloud est l\u0026rsquo;appellation générique des prestataires qui se propose de prendre en charge pour vous tout ou parti de ces composantes.\nAs a service Dans le monde du cloud, tout est considéré comme des services fournis à votre entreprise par votre prestataire de Cloud. Comme vous pouvez le voir ci-dessous, chaque service cloud sera affublé d\u0026rsquo;un acronyme permettant de comprendre quelle est la répartition des responsabilités entre vous et le prestataire.\nIaaS, PaaS ou SaaS sont 3 acronymes indispensables à comprendre dans le cloud.\nIaaS : Infrastructure as a Service PaaS : Platform as a Service SaaS : Software as a Service Vous trouverez d\u0026rsquo;autres acronymes correspondants à d\u0026rsquo;autre répartition, mais je ne vous présente que les 3 plus importante.\nAfin de digérer cela, voici ces mêmes concepts expliqués avec le monde de la pizza.\nChangement de paradigme Le passage d\u0026rsquo;une informatique d\u0026rsquo;entreprise gérer en interne à l\u0026rsquo;utilisation massive de service Cloud est un vrai changement de paradigme pour l\u0026rsquo;entreprise et la DSI.\nJe vais en détailler certains maintenant.\nAchat vs location Le premier changement majeur est le passage d\u0026rsquo;un modèle d\u0026rsquo;achat à un modèle de location. En effet, lorsqu\u0026rsquo;une entreprise gère ses propres serveurs, en général, ils sont achetés.\nÀ partir du moment où vous allez dans le Cloud, le matériel n\u0026rsquo;est plus la propriété de l\u0026rsquo;entreprise, cela a d\u0026rsquo;ailleurs un impact très important d\u0026rsquo;un point de vue financier, on passe d\u0026rsquo;un modèle CAPEX (les dépenses sont des coûts d\u0026rsquo;investissement) à un modèle OPEX (les dépenses sont des coûts d\u0026rsquo;exploitation).\nComme on ne gère pas de la même manière un véhicule en location qu\u0026rsquo;un véhicule que l\u0026rsquo;on possède, on doit garder cela à l\u0026rsquo;esprit pour mieux comprendre les changements de paradigme suivants.\nDes services managés Les prestataires de service Cloud proposent avant tout des services managés, c\u0026rsquo;est-à-dire des services informatiques, dont la complexité d\u0026rsquo;implémentation, et d\u0026rsquo;exploitation est masquée à l\u0026rsquo;utilisateur de ce service sous la forme d\u0026rsquo;un ensemble d\u0026rsquo;options simplifié.\nPar exemple, utiliser un service de base de données en ligne est beaucoup plus simple que d\u0026rsquo;installer et exploiter vous-même ce type de service.\nLes services managés permettent aux entreprises d\u0026rsquo;accéder à des services informatiques complexes sans pour autant disposer en interne des ressources pour exploiter ces services.\nAnimal de compagnie vs Bétail Les équipes informatiques ont souvent un attachement particulier à leurs serveurs locaux, en effet ils les ont installés, les ont fait évoluer. Il y a un attachement comme avec un animal de compagnie.\nDans une approche cloud, on aura plus la même approche, on va devoir pouvoir reproduire automatiquement des configurations, automatiser des déploiements, cloner des systèmes, en résumé, industrialiser nos processus. Cette approche empêche d\u0026rsquo;avoir un attachement aux ressources informatiques lié on travail maintenant avec du bétail.\nIndustrialisation L\u0026rsquo;industrialisation de vos processus informatique est donc une phase importante a mettre en œuvre lors d\u0026rsquo;un passage vers des services Cloud.\nEn général, les équipes projet souhaitent déployer rapidement pour mettre à disposition aux utilisateurs les nouveautés et les équipes d\u0026rsquo;exploitation ont besoin de garantir un usage stable des plateformes et préfère donc éviter les changements.\nUne approche DevOps de vos projets informatiques devra être mise en place. Cette approche permet de mieux faire communiquer vos équipes de projets (Dev), de développement interne ou de déploiement de solution logiciel externe, avec vos équipes d\u0026rsquo;exploitation (Ops) en charge du bon fonctionnement quotidien des applications.\nParmi les points clés d\u0026rsquo;une approche DevOps on retiendra la mise en œuvre des éléments suivant :\nInfrastructure As Code : On défini l\u0026rsquo;infrastructure cible et on la déploie à l\u0026rsquo;aide d\u0026rsquo;outils d\u0026rsquo;automatisation, comme Terraform. L\u0026rsquo;infrastructure est maintenue via des fichiers de configuration comme le code informatique. Continous integration / Continous delvery (CI/CD) : On met en place des processus automatisés permettant de préparer et réaliser les évolutions logiciels de vos environnements. Cette automatisation permet d\u0026rsquo;augmenter la fiabilité de ce processus et permet donc des mises à jour plus fréquentes. La clé de la réussite Vous pouvez mettre en place tout ce que vous voulez, mais il y a à mon sens une seule clé de la réussite d\u0026rsquo;un projet Cloud : la maitrise des coûts.\nCette maitrise des coûts passe par une bonne compréhension des paradigmes présentés ci-dessus, mais aussi une forte sensibilisation des toutes vos équipes, on ne fait pas de l\u0026rsquo;informatique dans le cloud comme on fait de l\u0026rsquo;informatique localement.\nQuelques exemples pour mieux comprendre :\nVous avez un serveur virtualisé (IAaS) que vous utilisé 12 h par jour. En informatique traditionnelle ce serveur est disponible en permanence. En Cloud stoppé ce serveur quand il n\u0026rsquo;est pas utilisé pour arrêté la facturation et redémarré le quand vous en avez besoin. Vous avez un serveur de base de données qui consomme régulièrement entre 60 et 80 % de ses ressources. En informatique traditionnelle, vous allez penser à augmenter les capacités matérielles pour prévoir l\u0026rsquo;avenir, les délais de commande de matériel peuvent être longs. En Cloud vous cherchez à maximiser la consommation des ressources louées, en cas de besoin vous pourrez l\u0026rsquo;augmenter en quelques secondes et même automatiser ce processus. Merci de votre attention.\n","date":"2023-04-14T00:00:00Z","image":"https://blog.ddata.fr/p/les-bases-du-cloud/jakob-bartenschlager-GaTZTnODU_Y-unsplash_huec165e5e5d9302a0ae5db3cdc02c6df6_758416_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/les-bases-du-cloud/","title":"Les bases du cloud"},{"content":"Dans cet article, je vous présente les principales transformations des données que vous ferez dans un projet data.\nContexte Lorsque vous traitez les données d\u0026rsquo;un projet data vous allez faire une succession de transformation pour obtenir votre résultat final.\nPar exemple la chaine de traitement dans le cadre d\u0026rsquo;un projet datawarehouse est la suivante :\nOn retrouve un ensemble de traitement permettant la transformation des données d\u0026rsquo;une zone à l\u0026rsquo;autre :\nDans la zone bronze, un maelstrom de données brut. Dans la zone silver, des données préparées sous forme de table structurée. Dans la zone gold, des données à forte valeur ajoutée comme par exemple des tables modéliser en étoile pour de reporting avec potentiellement des données pré agrégé. Mais, quels que soient les outils que vous utiliserez pour votre projet, vous utiliserez presque toujours les mêmes transformations de données.\nJe vous propose de passer en revue ces transformations.\nStructuration des données La phase de structuration des données va vous permettre de transformer vos données en données structurées sous forme de table.\nLa table La table est la structure de données que vous allez utiliser pour toutes vos transformations de données.\nUne table a les caractéristiques suivantes :\nLes données sont stockées en ligne Chaque colonne d\u0026rsquo;une table contient des valeurs cohérentes De nature : une colonne contient des données métier de même nature. Par exemple dans une colonne contenant le nom d\u0026rsquo;une personne on ne met pas sa date de naissance. De type : une colonne ne contient que des valeurs ayant le même type ou pas de valeur. Par exemple une colonne de date ne contient que des dates. De format : une colonne de date ne contient que des dates au même format, par exemple on ne stocke pas le 31 décembre 2022 sous ces 2 formes dans la même colonne : 31/12/2022 2022-12-31 Nommage des colonnes Le nommage des colonnes d\u0026rsquo;une table est une phase très importante dans un projet BI. En général vous avez des données dont le nom n\u0026rsquo;est pas adapté à la création d\u0026rsquo;un modèle métier. La phase de renommage des colonnes d\u0026rsquo;une table vous permet d\u0026rsquo;apporter cette cohérence métier.\nPrenez grand soin à avoir un nommage cohérent et d\u0026rsquo;utiliser le bon vocabulaire du modèle métier cible, en effet si votre modèle de données doit être utilisé par des personnes d\u0026rsquo;un métier parlé leur langage et masquer tous les éléments techniques inutiles.\nTypage de données Le typage des données permet de conserver la cohérence d\u0026rsquo;une table. Parmi les types de données disponibles dans votre outil, choisissez toujours des types de données vous apportant une forte garantie sur le contenu de la colonne.\nPar exemple, si vous stockez vos dates dans une colonne stockant du texte, vous n\u0026rsquo;aurez pas la garantie d\u0026rsquo;avoir des dates dedans.\nLes principaux types de données que l\u0026rsquo;on utilise sont :\nLe texte ou chaine de caractères Les nombres Entier Décimaux Le temps Date Date/Heure Date/Heure/Fuseau horaire Les booléens (vrai/faux) Attention, chaque type de données a ses propres pièges et on rencontre souvent les mêmes problèmes. Par exemple :\nLes formats de date ou de nombre décimaux différents entre les systèmes français et américain L\u0026rsquo;encodage du texte, si les termes ASCII, UTF-8 ou UNICODE ne vous sont pas familiers vous risquez des ennuis lors du traitement de données avec des caractères spéciaux. Sélection des colonnes Ne sélectionnez que les colonnes utiles d\u0026rsquo;un point de vue métier ou technique à votre modèle de données. Il est en général plus simple d\u0026rsquo;ajouter une nouvelle colonne à un modèle existant pour répondre à un nouveau besoin que de créer un modèle cohérent avec un grand nombre de colonnes inutile.\nPréparation des données Nous allons maintenant passer en revue les principales transformations que vous ferez sur vos données lors d\u0026rsquo;un projet data.\nFiltrage des données Un filtre permet de sélectionner les lignes que l\u0026rsquo;on souhaite conserver en fonction de critères prédéfinis.\nRemplacer des données Le remplacement de données permet de remplacer une valeur par une autre valeur.\nAttention, certains outils sont sensibles à la casse, il différencie minuscules et majuscules. Par exemple, rouge sera différent de Rouge.\nTri des données Le tri des données permet d\u0026rsquo;imposer l\u0026rsquo;ordre des lignes les unes par rapport aux autres. Le tri peut être ascendant ou descendant et peu porter sur plusieurs colonnes afin de forcer le tri des lignes sur des sous-critères.\nAttention, tant que vous n\u0026rsquo;avez pas trié vos données ne préjugé pas du tri de celle-ci.\nPartitionnement des données Le partitionnement des données permet de séparer physiquement vos données. On utilise cette technique sur les gros volumes de données en général pour améliorer les performances.\nPivoter Dépivoter et Transposer Ces 3 opérations permettent de faire des transformations sur la structure de la table. Elles sont souvent utilisées pour transformer des données formatées pour la saisie utilisateur dans Excel en données sous la forme d\u0026rsquo;une table utilisable.\nPivoter Permets de basculer des valeurs stockées dans les lignes d\u0026rsquo;une colonne en valeur stockée en colonne. Une opération d\u0026rsquo;agrégation de données peut aussi être réalisée lors du pivot pour regrouper les valeurs sources identiques en une seule valeur cible.\nDépivoter Permets de basculer des valeurs stockées dans les noms de colonne d\u0026rsquo;une table en valeur stockée en ligne. C\u0026rsquo;est l\u0026rsquo;opération inverse de pivoter.\nTransposer Permets de réaliser une rotation à 90° de votre table.\nSi vous transposez une table déjà transposée vous retrouvez la table initiale.\nCréer des colonnes Par fractionnement, vous divisez une colonne en plusieurs autres colonnes à l\u0026rsquo;aide d\u0026rsquo;un séparateur. Par calcul, vous créer une colonne en vous basant sur des informations en provenance d\u0026rsquo;autres colonnes de la même ligne. Par extraction d\u0026rsquo;un terme, vous créer une colonne en extrayant un sous-ensemble d\u0026rsquo;information d\u0026rsquo;une colonne. Les jointures Les jointures sont une transformation de données très puissantes. Elles peuvent avoir plusieurs usages, notamment :\nL\u0026rsquo;enrichissement d\u0026rsquo;une table via une autre table. La suppression de ligne d\u0026rsquo;une table par une autre table. La création de ligne dans une table. Jointure externe gauche La jointure externe gauche permet d\u0026rsquo;intégrer dans la table de gauche toutes les valeurs de la table de droite correspondant à la relation de jointure.\nAvec ce tpe de jointure on à la garantie de conserver TOUTES les lignes de la table de gauche.\nPar contre s\u0026rsquo;il existe plusieurs occurrences d\u0026rsquo;une même valeur dans la table de droite pour la relation, cela va créer des lignes dans la table de gauche.\nLa valeur null représente par convention une absence de valeur dans la table de droite.\nJointure externe droite La jointure externe droite permet de filtrer les lignes de table de gauche à l\u0026rsquo;aide des données de la table de droite.\nJointure externe entière La jointure externe entière permet de conserver toutes les lignes des 2 tables.\nJointure interne La jointure interne permet de ne conserver que les lignes appartenant aux 2 tables.\nL\u0026rsquo;union L\u0026rsquo;union permet de cumuler les lignes de 2 tables ou plus en une seule table. Suivant les outils les tables doivent avoir la même structure de colonnes ou non.\nLe dédoublonnage Le dédoublonnage est une opération qui permet de supprimer les lignes identiques. On à deux types d\u0026rsquo;opération de dédoublonnage :\nDédoublonage pur pour les lignes complètement identiques. Dédoublonage partiel, sur un critère choisi d\u0026rsquo;une ou plusieurs colonnes, dans ce cas il faut faire attention à quelle valeur est conservé. Agrégation L\u0026rsquo;agrégation est une opération qui permet de réduire le nombre de lignes d\u0026rsquo;une table. On supprime certaines colonnes, et en perdant ce détail, on peut regrouper les lignes en faisant une opération mathématique sur les colonnes numériques, en général une somme.\nLes opérations d\u0026rsquo;agrégations classiques sont :\nSomme Minimum Maximum Moyenne Écart type Compter le nombre de lignes agrégé L\u0026rsquo;opération d\u0026rsquo;agrégation est une opération qui modifie la granularité de la table. La table résultante a un niveau de détail moins important que la table source, on perd donc de l\u0026rsquo;information.\nMerci de votre attention.\n","date":"2023-04-09T00:00:00Z","image":"https://blog.ddata.fr/p/datalake-transformation/katrin-hauf-sD-WFeC8WEs-unsplash_hue23c27c3ff840c33b0fdb3aeccafd101_470448_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/datalake-transformation/","title":"Projet data : les principales transformations des données"},{"content":"Excel est votre premier amour ? Et s’il rencontré votre service Power BI.\nContexte On vous demande fréquemment d\u0026rsquo;avoir les données de votre joli rapport dans Excel, mais vous ne voulez pas perdre la connexion aux données rafraichie automatiquement par le service Power BI ?\nIl existe une solution : Analyser dans Excel\nPrincipe de fonctionnement Le principe de fonctionnement est plutôt simple comme le montre le schéma ci-dessous :\nLe service Power BI expose le jeu de données et Excel peut se connecter directement dessus. L\u0026rsquo;avantage par rapport à un export de données classique est qu\u0026rsquo;Excel peut rafraichir la source de données. Il profite ainsi d\u0026rsquo;une source de données vivante plutôt que morte dans le cas d\u0026rsquo;un export.\nAnalyser dans Excel Il existe plusieurs manières d\u0026rsquo;accéder à cette fonctionnalité.\nDans tous les cas le service Power BI va créer un fichier Excel qui inclut la connexion vers le jeu de données.\nDirectement depuis un jeu de données Vous pouvez créer une connexion vers Excel directement depuis un jeu de données, pour cela rendez-vous dans l\u0026rsquo;espace de travail contenant le jeu de données.\nCliquez sur les 3 points de la ligne du jeu de données pour ouvrir le menu contextuel. Sélectionnez Analyser dans Excel. Vous pouvez voir en haut à droite de votre écran que le service Power BI prépare votre fichier.\nUne fois le fichier prêt, il est déposé dans votre OneDrive\nSi le service n\u0026rsquo;arrive pas à accéder à votre OneDrive le fichier est téléchargé en local.\nVous pouvez donc ouvrir le fichier Excel et activer le contenu.\nVous retrouvez votre jeu de données. Vous pouvez l\u0026rsquo;exploiter comme un tableau croisé dynamique. Directement depuis un rapport Une autre solution si vous n\u0026rsquo;avez pas accès à l\u0026rsquo;espace de travail contenant le jeu de données est de passer par le rapport pour créer le fichier Excel se connectant aux données.\nCliquez sur le menu Exporter. Sélectionnez Analyser dans Excel. Le même processus que celui décrit pour le jeu de données créer votre fichier Excel.\nExporter des données vers Excel avec une connexion dynamique Cette option va produire un fichier Excel différent des 2 cas précédents.\nPour l\u0026rsquo;utiliser, pointer sur un visuel de votre rapport.\nSur votre visuel, sélectionnez les 3 points pour ouvrir le menu contextuel. Sélectionnez Exporter des données. Choisissez le type d\u0026rsquo;export souhaité parmi les choix actifs. Sélectionnez .xlsx (Excel) avec connexion active. Appuyez sur le bouton Exporter pour télécharger le fichier Excel localement. Vos données sont directement disponibles sans avoir à les reconstruire avec un tableau croisé dynamique. Vous pouvez les rafraichir à l\u0026rsquo;aide du bouton Actualiser tout du menu Données. Vous pouvez ouvrir le panneau de requêtes et connexions à l\u0026rsquo;aide du bouton Requêtes et connexions du menu Données. Vous pouvez constater que vous avez une connexion active vers votre jeu de données. Pour aller plus loin, faites un clic droit sur la connexion et choisissez Propriétés\u0026hellip;.\nAllez sur l\u0026rsquo;onglet Définition. Vous pouvez voir la chaine de connexion à votre jeu de données Power BI. Vous retrouvez ici la requête DAX permettant d\u0026rsquo;alimenter le tableau Excel. Bien entendu vous pouvez modifier ces éléments pour les adapter à vos besoins.\nParamétrages nécessaires Pour que la fonction Analyser dans Excel fonctionne, il est nécessaire de vérifier que vous êtes autorisé à l\u0026rsquo;utiliser.\nLes autorisations nécessaires sont à la fois globales et au niveau du jeu de données.\nParamétrages nécessaires au niveau global Au niveau de service Power BI, deux autorisations doivent être actives dans les paramètres du client du portail d\u0026rsquo;administration Power BI :\nL\u0026rsquo;option Les utilisateurs peuvent utiliser Power BI jeux de données dans Excel à l’aide d’une connexion active doit être active. L\u0026rsquo;option Autoriser les points de terminaison XMLA et Analyser dans Excel avec les jeux de données locaux doit être active. Paramétrages nécessaires au niveau du jeu de données Pour utiliser la fonction analysée dans Excel, vous devez avoir le droit d\u0026rsquo;accéder au jeu de données en lecture, mais aussi d\u0026rsquo;avoir le droit de créer du contenu sur le jeu de données. Ces droits peuvent être affectés :\nAutomatiquement pour les membres de l\u0026rsquo;espace de travail du jeu de données. Spécifiquement pour les audiences de l\u0026rsquo;application Spécifiquement sur le jeu de données lui-même Bien entendu si de la sécurité niveau ligne est défini sur le jeu de données, elle s\u0026rsquo;applique.\nLimitations Un certain nombre de limitations sont applicables comme le précise la documentation.\nMerci de votre attention.\n","date":"2023-04-02T00:00:00Z","image":"https://blog.ddata.fr/p/analyser-dans-excel/matcha-co-pC16vUGYCL4-unsplash_hu8221600bfe6cac469e7179ce07dc706c_5513366_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/analyser-dans-excel/","title":"Analyser dans Excel, la rencontre d'Excel et du service Power BI"},{"content":"Dans cet article je vous présente les principaux formats de fichier que l\u0026rsquo;on trouve dans un datalake.\nContexte Lorsque vous travaillez avec un datalake vous allez manipuler un certain nombre de formats de fichier. Je vous présente ici ceux que vous rencontrerez le plus souvent.\nVous trouverez dans mes articles précédents les explications concernant :\nLes architectures de type datalakehouse : Datawarehouse ou datalakehouse, quelle source pour vos reportings ?. Les datalakes :Datalake : la base d\u0026rsquo;un projet data moderne. Qu\u0026rsquo;est-ce qu\u0026rsquo;un fichier ? La question peut sembler triviale pour certain j\u0026rsquo;ai rencontré suffisamment de personnes pour qui cela n\u0026rsquo;était pas clair pour prendre le temps de l\u0026rsquo;expliquer.\nEn informatique on utilise des fichiers pour stocker de l\u0026rsquo;information et des dossiers pour ranger nos fichiers dans des structures hiérarchiques.\nUn fichier peut contenir tous types d\u0026rsquo;informations, du code binaire de vos programmes aux données des documents que vous réalisez avec ces programmes.\nOn appelle un format de fichier l\u0026rsquo;extension du fichier.\nL\u0026rsquo;extension du fichier est simplement la suite de caractères se trouvant après le dernier point du nom complet du fichier. Les extensions sont souvent masquées dans l\u0026rsquo;explorateur de fichier de Windows.\nPar exemple, MonFichier.json est un fichier de type json. Attention l\u0026rsquo;extension du fichier ne garantit en aucun cas que le contenu du fichier est bien celui prétendu par l\u0026rsquo;extension. Il s\u0026rsquo;agit simplement d\u0026rsquo;une convention, pas d\u0026rsquo;une contrainte. Enfin on peut distinguer 2 grands types de contenus dans nos fichiers :\nles contenus texte, les fichiers sont lisibles et éditables avec un éditeur de texte. les contenus binaires, les fichiers sont utilisables avec des programmes particuliers. JSON Les fichiers JSON :\non en général l\u0026rsquo;extension .json ils contiennent du texte ils sont produits par des systèmes automatisés comme des API. Un fichier JSON permet de stocker de l\u0026rsquo;information hiérarchique, en effet ce format provient de la programmation est est utilisé pour stockée des objets complexes.\nPour faire simple, dans un JSON vous stockez des couples clé/valeur sous la forme \u0026ldquo;MaClé\u0026rdquo;: \u0026ldquo;MaValeur\u0026rdquo; chaque couple est séparé par une virgule.\nLa valeur peut être :\nUne chaine de caractères, dans ce cas elle est entre quote \u0026quot;. une valeur numérique, dans ce cas elle est saisie directement après les 2 points. On utilise le point comme séparateur décimal. un valeur booléenne, dans ce cas on saisie directement true ou false sans quote. un objet, qui n\u0026rsquo;est défini comme un ensemble de clés/valeur correspondant à une clé. Dans ce cas la valeur se trouve entre accolades. un tableau, qui peut contenir plusieurs valeurs de n\u0026rsquo;importe quel type. Voici un exemple de fichier JSON :\n1 2 3 4 5 6 7 8 { \u0026#34;MonObjet\u0026#34;: {\u0026#34;Nom\u0026#34; : \u0026#34;Nom de mon objet\u0026#34;, \u0026#34;Quantite\u0026#34; : 10}, \u0026#34;MonTableauDeNombre\u0026#34;: [1,2,3], \u0026#34;MonTableauDObjet\u0026#34;: [ {\u0026#34;Titre\u0026#34;: \u0026#34;Mon livre\u0026#34;, \u0026#34;Auteur\u0026#34;: \u0026#34;Max\u0026#34;}, {\u0026#34;Titre\u0026#34;: \u0026#34;Mon second livre\u0026#34;, \u0026#34;Auteur\u0026#34;: \u0026#34;Max\u0026#34;} ] } CSV Les fichiers CSV :\non en général l\u0026rsquo;extension .csv ils contiennent du texte ils sont produits par des systèmes automatisés comme des exports de logiciel métier. le format peut différer en fonction des informations régionales utilisées pour le produire. Un fichier CSV permet de stocker de l\u0026rsquo;information sous la forme d\u0026rsquo;un tableau de données.\nOn retrouve une ligne dans le fichier par ligne dans la table, toutes les lignes du fichier doivent avoir la même structure.\nPour chaque fichier CSV on doit connaitre les informations le constituant afin de pouvoir le lire sans erreur, notamment :\nQuel est le séparateur de colonne, le point-virgule en français ou la virgule en américain. Quel est le séparateur décimal, la virgule en français ou le point en américain. Qu’elle est de délimiteur de colonne s’il existe, en général la double quote \u0026quot; qui permet d\u0026rsquo;utiliser les séparateurs de colonnes dans du texte sans qu\u0026rsquo;il soit interprété. Si la première ligne est l\u0026rsquo;en tête des colonnes ou une ligne de données. Voici un exemple de fichier CSV américain :\n1 2 3 Titre,Auteur,Quantité Mon livre, Max, 10 Mon second livre, Max, 25 Excel Les fichiers Excel :\non en général l\u0026rsquo;extension .xls ou .xlsx. ils contiennent des données binaires. ils sont produits par Microsoft Excel ou des systèmes d\u0026rsquo;export de données. le nombre de lignes d\u0026rsquo;une feuille de calcul est limité (1 048 576 lignes pour les fichiers xlsx) Les fichiers Excel sont en général une source de problème, car étant à la main des utilisateurs, ils ont tendance à modifier la structure de leur fichier sans connaitre les conséquences sur les traitements de données utilisant leurs fichiers.\nAttention on trouve encore beaucoup de fichier Excel 97-2003, ceux avec l\u0026rsquo;extension xls, ce format de fichier est vieillissant. Évitez-les si vous le pouvez.\nParquet Les fichiers Parquet :\non en général l\u0026rsquo;extension .parquet. ils contiennent des données binaires. ils sont inmutables, une fois le fichier créer vous ne pouvez pas modifier sont contenu. Les fichiers parquet sont le format roi des datalakes. Ils stockent des tables et apportent grand nombre de services :\nPrésence des métadonnées de la table nom de colonne type de données Stockage colonne optimisée pour les traitements analytiques. Compression des données. Étant inmutable, l\u0026rsquo;utilisation de ces fichiers demande une bonne stratégie d\u0026rsquo;alimentation pour optimiser vos traitements. En général on trouve les scénarii suivant :\nRemplacement intégral des fichiers avec la nouvelle version des données. Remplacement partiel des fichiers avec un sous-ensemble des données, on ne remplace que certains fichiers, chaque fichier correspond à une partition de données connue. Ajout uniquement des nouvelles données dans de nouveaux fichiers. Le format parquet est un format open source géré par la fondation Apache.\nDelta lake La gestion des suppression et modification de données dans le système source n\u0026rsquo;est pas simple à gérer avec les fichiers parquets.\nAfin de simplifier cela, il existe le format Delta lake qui permet de gérer ces problèmes. Le résultat est un ensemble de fichiers parquet et json qui représente l\u0026rsquo;état des données. Chaque modification créée de nouveaux fichiers. Vous trouverez plus d\u0026rsquo;information sur le site de delta lake.\nMerci de votre attention.\n","date":"2023-03-26T00:00:00Z","image":"https://blog.ddata.fr/p/datalake-file-format/david-bruno-silva-Z19vToWBDIc-unsplash_hu7b96a42f0cce3c959105b317d2a6b238_2704901_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/datalake-file-format/","title":"Datalake : les principaux formats de fichiers"},{"content":"Power Query est la brique de Power BI qui permet l\u0026rsquo;ingestion et la transformation de vos sources de données. Si vous débutez, je vous propose de vous montrer les principales options avancées.\nContexte Il y a 2 manières d\u0026rsquo;utiliser Power Query :\navec les assistants quand on débute en modifiant directement le code produit. En effet, lorsque vous utilisez Power Query via les assistants, ce dernier produit un code correspondant aux différentes étapes des transformations demandées, ce sont les requêtes Power Query.\nListe des requêtes Power Query Liste des étapes de la requête sélectionnée. Le code des requêtes Power Query est écrit dans le langage M.\nAfficher le code M dans l\u0026rsquo;éditeur Power Query Pour pouvoir comprendre comment fonctionne le code M, la première chose à faire est d\u0026rsquo;afficher le code généré dans l\u0026rsquo;éditeur Power Query.\nDans Power Query, sélectionnez le menu Affichage. Cochez Barre de formule. Vous pouvez voir le code de l\u0026rsquo;étape dans la barre de formule. Vous pouvez modifier directement le code M depuis la barre de formule, parfois c\u0026rsquo;est plus simple et rapide que de repasser par les assistants.\nOuvrir l\u0026rsquo;éditeur avancé La barre de formule permet de modifier une transformation précise. Chaque transformation correspond à une étape de votre requête Power Query, vous pouvez éditer toutes les transformations d\u0026rsquo;un coup à l\u0026rsquo;aide de l\u0026rsquo;éditeur avancé.\nPour ouvrir l\u0026rsquo;éditeur avancé, vous devez :\nPremière manière de faire, sélectionnez le menu Accueil. Appuyez sur Éditeur avancé. Seconde manière, faire un clic droit sur une requête. Appuyez sur Éditeur avancé. L\u0026rsquo;éditeur avancé s\u0026rsquo;affiche :\nPersonnaliser l\u0026rsquo;affichage Vous pouvez modifier l\u0026rsquo;affichage de l\u0026rsquo;éditeur en activant certaines options pratiques.\nAppuyez sur Option d\u0026rsquo;affichage. Sélectionnez ou désélectionnez les options qui vous intéressent. L\u0026rsquo;option Afficher les numéros de ligne permet d\u0026rsquo;y voir plus clair dans le code affiché, surtout si on doit en parler avec quelqu\u0026rsquo;un. Vous pouvez mettre des commentaires dans votre code Pour les commentaires sur une ligne : // Pour les commentaires multiligne : /* */ Les commentaires apparaissent en vert dans l\u0026rsquo;éditeur.\nLes commentaires apparaissent dans les étapes de l\u0026rsquo;éditeur Power Query.\nPointez-les avec la souris pour les lire. Comprendre les requêtes Power Query Il faut comprendre la relation qui existe entre le code et les étapes de votre requête Power Query.\nPour commencer, vous pouvez remarquer que chaque étape dans le code ressemble à ce pattern : NomDeMonEtape = FamilledeFonction.NomdeLafonction(Mes, Paramètres)\nLes lignes se terminent par une virgule pour séparer les étapes (sauf la dernière étape).\nDonc ce que l\u0026rsquo;on trouve avant le symbole égal dans l\u0026rsquo;éditeur avancé est juste le nom de l\u0026rsquo;étape. Si le nom de cette étape contient des espaces, elle est entourée des symboles #\u0026quot;\u0026quot;, par exemple #\u0026ldquo;Mon nom d\u0026rsquo;étape avec des espaces\u0026rdquo;.\nJe vais maintenant vous expliquer plus en détail les autres choses que l\u0026rsquo;on trouve dans l\u0026rsquo;éditeur Power Query.\nL\u0026rsquo;étape Type modifié présente dans la liste des étapes est définie sur la ligne 8 de notre exemple, elle se voit affecter le résultat de la fonction se trouvant après le symbole égal de la ligne 8. Cette fonction renvoie une table. L\u0026rsquo;étape suivante sur la ligne 9, Année insérée, utilise une fonction qui prend en paramètre la table résultat de l\u0026rsquo;étape Type modifié (en jaune) On retrouve la même chose pour les étapes suivantes. Une étape correspond donc a un résultat pouvant être réutilisé. L\u0026rsquo;étape n\u0026rsquo;est pas forcément réutilisée dans l\u0026rsquo;étape suivante et les fonctions ne renvoient pas forcément des tables. Vous pouvez voir que les étapes des lignes 3 et 4, DateDebut et DateFin, sont utilisées dans l\u0026rsquo;étape 5. Vous remarquerez que le code M appelle toujours des fonctions dont le nom est du type FamilledeFonction.NomdeLafonction.\nAller plus loin avec le langage M Aide du langage Une grande partie des fonctions du langage M ne sont pas disponibles via les assistants, mais sont utilisables directement dans le code.\nPour les fonctions disponibles, on trouve souvent un grand nombre d\u0026rsquo;options non disponible via les assistants. Pour connaitre les détails du langage, rendez-vous sur la documentation de Microsoft. Elles sont classées par famille, il n\u0026rsquo;est pas inutile de passer un peu de temps à explorer pour vos futurs besoins.\nTaper sans les points Lorsque vous saisissez une fonction M, dans la barre de formule ou l\u0026rsquo;éditeur avancé, l\u0026rsquo;aide à la saisie se déclenche. Pour en profiter au maximum, ne saisissez pas le point entre la famille de fonction et le nom de la fonction, quand le focus est sur la fonction que vous souhaitez utiliser la touche Tabulation (à gauche au-dessus du verrouillage numérique) pour terminer automatiquement la saisie.\nLes fonctions avancées Le code M permet des usages très avancés, ce sont les Helper, certaines de ces fonctions sont documentées par Microsoft ici.\nMerci de votre attention.\n","date":"2023-03-19T00:00:00Z","image":"https://blog.ddata.fr/p/power-query-advanced-editor/anastasiia-kamil--uQZPtoJ8nk-unsplash_huf15fa593426db9a90f89c3ece54be71a_1377284_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-query-advanced-editor/","title":"Bien utiliser l'éditeur avancé de Power Query"},{"content":"Vous voulez commencer avec Power BI ? Faisons ensemble un rapide tour de l\u0026rsquo;écosystème du produit.\nQue faire avec Power BI ? BI est l\u0026rsquo;acronyme de Buisiness Inteligence ou Informatique décisionnelle en bon français. C\u0026rsquo;est la branche de l\u0026rsquo;informatique spécialisée dans l\u0026rsquo;analyse de données. Le résultat final sera un rapport ou un tableau de bord, mais Power BI est bien plus que cela. Il va vous permettre :\nDe collecter vos données depuis différentes sources De modéliser vos données et calculer des KPI/mesures/Indoicateurs De présenter vos données dans des rapports ou tableaux de bord Power BI va vous accompagner tout au long de votre projet BI.\nQui peut utiliser Power BI ? Cette question est de loin la plus importante. Power BI n\u0026rsquo;est pas un outil destiné aux informaticiens, mais aux métiers. Qu\u0026rsquo;on les nomme Power User, Citizen developper ou autrement, le concept de base du produit est de permettre à des non-informaticiens de réaliser par eux même des projets BI, sacré challenge !\nComment faire un projet BI ? Pour répondre à cette question, je vous invite à lire mon article Comment faire un projet BI..\nUn projet BI ne sera jamais linéaire. Vos données sont vivantes. Votre projet BI doit être itératif. Avec Power BI vous allez pouvoir obtenir rapidement un premier résultat, puis retravailler entièrement vos phases de projets pour obtenir de nouveaux résultats.\nLa famille Power BI Power BI Desktop Power BI Desktop est l\u0026rsquo;outil de création des projets Power BI. Les rapports produits à l\u0026rsquo;aide de Power BI Desktop sont prévus pour être utilisés sur des écrans de manière interactive. Power BI Desktop est un outil de la gamme Office 365, il cible aussi bien un public d\u0026rsquo;utilisateurs métier voulant prendre en main leur BI qui des professionnels de la BI.\nPower BI Desktop est composé de 3 éléments.\nL\u0026rsquo;éditeur Power Query qui permet de gérer la phase ETL du projet BI (1). Des outils de modélisation de données (2) \u0026amp; (3). Des outils de visualisation des données (4) \u0026amp; (5). Le fichier PBIX produit par Power BI desktop contiennent l\u0026rsquo;ensemble des éléments de votre projet.\nUne fois votre projet terminé dans Power BI Desktop, vous allez publier ce projet dans le service Power BI pour le diffuser auprès de vos utilisateurs.\nPower BI Report Builder Power BI Report Builder est l\u0026rsquo;outil de création de rapport paginé. Contrairement aux rapports créer avec Power BI Desktop, les rapports paginés sont conçus pour être consommés sous forme statique, PDF ou impression papier par exemple.\nEn résumé :\nLes rapports créés avec Power BI desktop sont destiné à être consommé sur un écran Les rapports créés avec Power BI Report Builder sont destinés à être consommés sur papier (PDF inclus) Il est facile de comprendre le besoin de 2 outils différents : un tableau avec un ascenseur marche très bien sur écran, mais pas du tout sur papier.\nContrairement à Power BI Desktop c\u0026rsquo;est un outil visant un public technique.\nPower BI Service Power BI Service est un service disponible sur le web à l\u0026rsquo;adresse https://app.powerbi.com, c\u0026rsquo;est là que vous allez publier vos rapports et les partager avec vos utilisateurs.\nPower BI Service est composée des éléments suivants\nEspace de travail (Workspace) Un espace de travail est l\u0026rsquo;endroit où vous publiez vos projets Power BI. Lors de la publication, un fichier PBIX est séparé en 2 entités, le jeu de données et le rapport.\nVous trouverez plus d\u0026rsquo;information à ce sujet dans mes précédents articles :\nJeu de données et rapport : la rupture ! Voyage au cœur de la BI self-service Jeux de données (Datasets) Le jeu de données contient les requêtes Power Query, vos données ainsi que le modèle de données avec vos mesures.\nDans le service vous pourrez paramétrer votre jeu de données :\nEn définissant les informations de connexion aux sources de données Cloud ou OnPremise via la passerelle Power BI. En gérant les droits d\u0026rsquo;accès. En modifiant la valeur des paramètres Power Query En définissant les rôles de sécurité si vous avez de la sécurité niveau ligne ou RLS pour Row Level Security en anglais. Rapports (Reports) Les rapports dans le service Power BI sont la partie datavisualisation de votre projet BI de Power BI Desktop.\nVous avez aussi la possibilité de créer directement des rapports dans l\u0026rsquo;espace de travail sur un jeu de données déjà existant.\nUn rapport ne peut avoir qu\u0026rsquo;un seul jeu de données comme source.\nRapports paginés (Paginated Reports) Les rapports dans le service Power BI sont les rapports créés avec Power BI Report Builder.\nTableau de bord (Dashbord) Les tableaux de bord vous permettent d\u0026rsquo;épingler des visuels en provenance de différents rapports dans un endroit unique.\nVous trouverez plus d\u0026rsquo;information dans mon article Power BI : Créer un tableau de bord Power BI.\nFlux de données (Dataflows) Les flux de données sont essentiellement du Power Query Online, ils permettent d\u0026rsquo;exécuter des requêtes Power Query et de stocker le résultat dans le service Power BI.\nVous trouverez plus d\u0026rsquo;information dans mon article Mutualiser vos données avec les dataflows.\nApplications (Apps) Les applications sont le bon moyen de diffusion de vos rapports et tableaux de bord à vos audiences.\nVous trouverez plus d\u0026rsquo;information dans mon article Les applications Power BI ?!? A quoi ça sert ?.\nPower BI Mobile Power BI Mobile est une application disponible dans les magasins d\u0026rsquo;applications (Apple ou Android) qui vous permet de consulter les rapports publiés dans le service Power BI directement depuis votre téléphone mobile.\nPasserelle Power BI (Power BI Gateway) La passerelle Power BI est une application qui s\u0026rsquo;installe sur vos serveurs d\u0026rsquo;entreprise afin de créer un lien entre le service Power BI et vos données dans votre réseau d\u0026rsquo;entreprise. Elle permet au service Power BI de rafraichir des données se trouvant dans votre réseau d\u0026rsquo;entreprise.\nElle existe en 2 versions :\nStandard : la version que l\u0026rsquo;on utilise en entreprise, elle permet de connecter différents services Microsoft à vos données d\u0026rsquo;entreprise. Elle est utilisable par toutes personnes autorisées. Personnel : utilisable uniquement par la personne l\u0026rsquo;ayant installé et uniquement pour Power BI Vous trouverez plus de détail ici.\nPower BI Embeded Power BI Embeded est l\u0026rsquo;appellation de l\u0026rsquo;utilisation de rapports publiés dans le service Power BI au sein d\u0026rsquo;application destiné à des utilisateurs internes ou externes à votre entreprise. Son usage est soumis à des licences spécifiques en fonction des cas d\u0026rsquo;utilisations.\nPower BI Report Server Power BI Report Server est une application s\u0026rsquo;installant dans votre réseau d\u0026rsquo;entreprise équivalente au service Power BI mais avec beaucoup moins de fonctionnalités. Une version spécifique de Power BI Desktop doit être utilisé pour créer et publier du contenu dans Power BI Report Server.\nCette solution est intéressante si vos données ne doivent pas être hébergées dans le cloud.\nMerci de votre attention.\n","date":"2023-03-12T00:00:00Z","image":"https://blog.ddata.fr/p/l-ecosysteme-power-bi/no-revisions-sVrEI2myiv4-unsplash_huf6962dbb77908a28d639e0c5b85edf9e_1022577_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/l-ecosysteme-power-bi/","title":"Présentation de l'écosystème Power BI."},{"content":"Dans cet article je vous présente les principaux services permettant de traiter les données d\u0026rsquo;un datalake dans Azure.\nRappel Vous trouverez dans mes articles précédents les explications concernant :\nLes architectures de type datalakehouse : Datawarehouse ou datalakehouse, quelle source pour vos reportings ?. Les datalakes :Datalake : la base d\u0026rsquo;un projet data moderne. Les services de données spécialisés Azure Data Factory Azure Data Factory est un outil de type ETL/ELT (Extract Transform Load / Extract Load Transform), il permet de réaliser des déplacements de données d\u0026rsquo;un point à un autre en réalisant des transformations sur les données. Parmi les usages classiques, on retrouve :\nAlimentation de la zone Bronze d\u0026rsquo;un datalake avec des données en provenance de service cloud et de vos réseaux d\u0026rsquo;entreprise (via un programme installé sur vos serveurs) Transformation et structuration des données : Création de fichiers structurés de type Parquet Re modélisation des données applicative vers un modèle en étoile Transformation de données semi-structurées de type JSON en données structuré de type Parquet Suivi de l\u0026rsquo;exécution des traitements de transformation Vous retrouverez les informations sur le produit ici.\nAzure Synapse Analytics Azure Synapse Analytics est avant tout une suite d\u0026rsquo;outils comprenant :\nun outil de type ETL/ELT, qui est simplement une version d\u0026rsquo;Azure Data Factory des moteurs de traitements de données : SQL Pools provisionnés : Vous louez des capacités de traitements SQL dédiés. Serverless : Vous payez vos traitements SQL à la requête. Apache Spark : le moteur de traitement de données Apache Spark qui permet de créer des traitements de données fortement scalable. Vous pouvez écrire des traitements dans différents langages (Python/R/Scala/Java) et les exécuter sur la plate-forme Spark. Data Explorer : un moteur de traitement spécialisé dans les traitements des logs et des séries temporelles. un outil de monitoring un espace de création regroupant tous les services ci-dessus. Azure Synapse Analytics est le service phare de la suite Azure pour les traitements de données.\nVous retrouverez les informations sur le produit ici.\nAzure Databricks Azure Databricks est l\u0026rsquo;implémentation de Databricks dans Azure.\nDatabricks est un outil basé sur Apache Spark tout en incluant des services complémentaires. Au même titre qu\u0026rsquo;Azure Synapse Analytics, il permet de créer des traitements d\u0026rsquo;ingestion et de préparation de données.\nContrairement à Azure Synapse Analytics, Databricks est prévu pour être multicloud, vous retrouverez ce service dans AWS (Amazon) ou GCP (Google).\nVous retrouverez les informations sur le produit ici.\nAzure Data Explorer Azure Data Explorer est un moteur de traitement spécialisé dans les traitements des logs et des séries temporelles.\nMicrosoft a créé un langage de requête dédié nommé KQL (Kusto Query Language) permettant de plonger dans les puits de logs comme le commandant Cousteau dans la mer.\nVous retrouverez les informations sur le produit ici.\nLes services cognitifs Azure Cognitive Services Azure Cognitive Services est un ensemble de service de type IA préentrainé que vous pouvez consommer directement sous forme d\u0026rsquo;API.\nParmi les principaux services proposés, on retrouve :\nLa reconnaissance de texte La reconnaissance faciale La reconnaissance d\u0026rsquo;entité sur des images ou vidéos Vous pouvez intégrer ces services dans vos traitements de données pour enrichir ces dernières.\nVous retrouverez les informations sur le produit ici.\nAzure Machine Learning Azure Machine Learning est un outil de création d\u0026rsquo;expérience de Machine Learning, il prend en charge de bout en bout ce type de projet de la création du modèle à son exploitation en production.\nLes modèles ainsi créés pourront enrichir vos traitements de données.\nVous retrouverez les informations sur le produit ici.\nLes services de calcul utilisable pour traiter les données Azure Functions Azure Functions permet d\u0026rsquo;exécuter du code à la demande dans Azure, sans louer de ressources en permanence. Vous êtes facturé à l\u0026rsquo;usage.\nVous pouvez créer des traitements s\u0026rsquo;exécutant à la demande, sur des évènements Azure ou de manière planifiée.\nLes principaux langages disponibles sont :\nC# JavaScript Java PowerShell Python TypeScript Par défaut les Azure Functions sont limitées à 5 minutes d\u0026rsquo;exécutions, 10 maximum. Si vous avez besoin de plus regarder les fonctions durables.\nVous retrouverez les informations sur le produit ici.\nAzure Logic Apps Azure Logic Apps est un outil de création de traitement low code. Il est très proche en termes d\u0026rsquo;interface et d\u0026rsquo;utilisation de Power Automate, mais contrairement à ce dernier vous payez les traitements à l\u0026rsquo;exécution, quels que soient les connecteurs utilisés.\nVous pouvez créer des traitements s\u0026rsquo;exécutant à la demande, sur des évènements Azure ou de manière planifiée.\nVous retrouverez les informations sur le produit ici.\nAutres services Azure à intégrer Azure Key Vault Azure Key Vault est un coffre-fort pour vos mots de passe, est-il besoin de rappeler qu\u0026rsquo;un mot de passe n\u0026rsquo;a rien à faire en clair dans un code ?\nAzure Key Vault vous permettra de sécuriser vos mots de passe et autres secrets et le mettra à disposition des applications ayant le droit de les utiliser. Vos différentes briques de services Azure ou autre pourront ainsi utiliser des secrets en toute discrétion.\nVous retrouverez les informations sur le produit ici.\nMicrosoft Purview Microsoft Purview est en ensemble d\u0026rsquo;outils permettant de mettre en place une gouvernance de données globale. Il vous permet aussi de mettre en place un catalogue de données d\u0026rsquo;entreprise.\nVous retrouverez les informations sur le produit ici.\nArchitecture type Tous les outils présentés ci-dessus peuvent travailler ensemble dans des architectures data moderne, ma préférée étant celle autour de Synapse pour un usage BI:\nVous trouverez les références de cette architecture ici et une version plus globale ici.\nMerci de votre attention.\n","date":"2023-03-05T00:00:00Z","image":"https://blog.ddata.fr/p/datalake-compute/deepmind-ZJKE4XVlKIA-unsplash_huabd43efd91f25ddf48b57b8258cba1be_7710978_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/datalake-compute/","title":"Datalake : comment traiter les données d'un datalake dans Azure"},{"content":"Dans cet article je vous présente le concept de Datalake ou lac de données en bon français. Un datalake est la fondation d\u0026rsquo;un projet data moderne, vous le retrouverez dans la plupart de vos futurs projets.\nContexte Comme je vous l\u0026rsquo;ai présenté dans ce précédent article, le datalake est un élément clé des projets data moderne. Je me propose maintenant de vous présenter plus en détail ce qu\u0026rsquo;est un datalake, comment on le structure et comment on peut s\u0026rsquo;en servir.\nContrairement à ce que l\u0026rsquo;on pourrait croire, les datalakes ne sont pas réservés aux gros projets de data et leur philosophie d\u0026rsquo;usage peut-être reprise dans tout projet data.\nLe stockage Lorsque l\u0026rsquo;on parle datalake on parle avant tout d\u0026rsquo;un stockage de données pouvant accepter tout type de format de données.\nEn général lorsque l\u0026rsquo;on parle de datalake on pense stockage cloud, l\u0026rsquo;avantage de ce type de stockage est sa capacité à être étendue en fonction de vos besoins et de vos finances. Bien que cette solution est la plus souple, on peut très bien créer un datalake en dehors du cloud.\nLes principaux types de stockages pour vos datalakes peuvent être :\nEn cloud dans l\u0026rsquo;écosystème Microsoft Azure Datalake Gen 2 Sharepoint Sur vos infrastructures locales Un partage réseau Un disque dur L\u0026rsquo;organisation Une fois la solution de stockage choisie, il est indispensable de réfléchir à l\u0026rsquo;organisation de votre datalake.\nSans rigueur votre lac de données deviendra un marais de données (dataswamp).\nLes zones Bien que chacun puisse créer l\u0026rsquo;organisation qu\u0026rsquo;il souhaite, on retrouve en général les 3 zones suivantes :\nBronze : Cette zone sert de landing zone pour les données, elle permet d\u0026rsquo;écrire les données brutes reçues dans le datalake. Aucune transformation n\u0026rsquo;est réalisée sur les données. Silver : cette zone permet de raffiner les données de la zone Bronze et d\u0026rsquo;effectuer les traitements de préparation de données parmi lesquels on retrouve : La transformation de données en un format de table avec Le nommage des colonnes Le typage des colonnes La détection des anomalies La détection des problèmes de qualité Gold : cette zone contient les données avec le plus haut niveau de raffinage, on retrouve par exemple Des données modélisées pour le reporting (modèle en étoile). Des données préaggrégées. Le nom de ces zones peut varier, mais leur usage reste. D\u0026rsquo;autres zones peuvent être créées en fonction de vos besoins spécifiques, par exemple une zone sandbox pour des expérimentations.\nLes coûts de stockage étant relativement bas, en général, les traitements permettant le raffinage des données d\u0026rsquo;une zone à l\u0026rsquo;autre ne suppriment pas les données traitées. Ainsi si vous traitez des données sources en utilisant 10 champs sur 50 dans le fichier brut et que vos besoins évoluent, vous aurez toujours la possibilité de retraiter l\u0026rsquo;ensemble des données brut pour répondre à vos nouveaux besoins.\nLes partitions Le partitionnement des données va vous permettre de ranger vos données de manière précise. Quand on parle de partitionnement, il s\u0026rsquo;agit simplement de mettre vos fichiers dans des dossiers et sous dossier différent. L\u0026rsquo;avantage du partitionnement est notamment de pouvoir accéder rapidement à des données en ne lisant que les fichiers présents dans les partitions qui nous intéressent.\nVous pouvez mixer les partitions comme vous le souhaitez.\nEn général, on crée des partitions en créant des dossiers sous la forme NomPartition=ValeurPartition.\nLes principaux partitionnements que vous utiliserez sont :\nPar source de données avec par exemple des dossiers sous la forme /service=rh/application=monapplication Par version de traitement ou de format de fichier avec par exemple des dossiers sous la forme /version=2.0 Par date avec par exemple des dossiers sous la forme /year=2023/month=02/day=26 Les principaux formats de données Votre datalake va contenir des fichiers, voici quelque format classique que vous y trouverez :\nParquet : C\u0026rsquo;est le format roi des datalakes, il s\u0026rsquo;agit d\u0026rsquo;un format de fichier dédier aux données analytiques, il conserve le nom et le type de données des colonnes et compresse les données contenues dans le fichier. CSV : Format classique d\u0026rsquo;extraction de données, il contient le nom des colonnes, mais les séparateurs utilisés peuvent varier en fonction de l\u0026rsquo;origine régionale du fichier (les séparateurs US et FR sont différents). Excel : Est-ce besoin de le présenter ? Un incontournable dans tous projets data. json, yaml, xml : Il s\u0026rsquo;agit en général de fichiers issus de traitement automatisé tel des API. Ces fichiers sont semi-structuré. log : les fichiers de log pourront être stockés longtemps et conserver une possibilité de traitement dans un datalake. Ces fichiers sont en général semi-structuré. Binaire : les vidéos et images sont des formats de données binaires, ils peuvent contenir certaines métadonnées exploitables directement (coordonnée GPS pour une photo par exemple), mais en général un traitement de type IA et nécessaire pour qualifier le contenu du fichier. La sécurité La sécurité est un point clé de votre datalake, elle est portée par les possibilités offertes par la solution de stockage que vous avez choisie.\nDans tous les cas, penser à créer plusieurs datalakes si vous avez des besoins forts de sécurité, une séparation des données peut prémunir contre des niveaux de droits inadéquats accordés à certain utilisateur.\nTraitement des données Le traitement des données d\u0026rsquo;un datalake fera l\u0026rsquo;objet d\u0026rsquo;un prochain article.\nMerci de votre attention.\n","date":"2023-02-26T00:00:00Z","image":"https://blog.ddata.fr/p/datalake-la-base/ticka-kao-o87vASD6Ksk-unsplash_hu0aa3f93b2631ba2a32af5171088bce89_2286693_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/datalake-la-base/","title":"Datalake : la base d'un projet data moderne"},{"content":"Besoin de partager des données entre vos jeux de données ? Découvrez la puissance des flux de données.\nContexte Je vous propose ici de découvrir une de mes fonctionnalités préférées du service Power BI, les flux de données ou dataflows.\nLes flux de données sont ni plus ni moins que du Power Query Online :\nPower Query : c\u0026quot;est l\u0026rsquo;outil de préparation des données au cœur de Power BI Desktop. Online : les transformations sont exécutées dans le service Power BI et le résultat est aussi stocké dans le service. Création de flux de données Rendez-vous dans le service Power BI et accédez à un espace de travail. Les flux de données ne sont pas disponibles dans Mon espace de travail.\nOuvrez le menu Nouveau de votre espace de travail. Sélectionnez Flux de données. Sélectionnez Ajouter de nouvelles tables pour créer un nouveau flux de données. Le bouton Ajouter des tables liées, permet de créer un flux de données basé sur le résultat d\u0026rsquo;un autre flux de données. Cette option demande un espace de travail avec une licence Premium. Le bouton Importer le modèle vous permet d\u0026rsquo;importer un modèle préalablement exporter au format JSON, cette option est disponible sur un flux de données existant. Le bouton Créer et attacher permet de créer un nouveau flux de données dont le stockage sera dans un de vos Azure Datake Gen2, ainsi vous pourrez accéder aux fichiers résultant du traitement du flux de données. Vous pouvez constater que l\u0026rsquo;on est bien dans Power Query. Dans cette zone, vous avez accès aux différentes sources de données pour votre flux de données. Pour notre exemple, nous allons appuyer sur Classeur Excel. Sélectionnez Parcourir OneDrive\u0026hellip; pour sélectionner un de vos fichiers Excel. Une fois la sélection faite, vous pouvez voir l\u0026rsquo;adresse OneDrive du fichier Une connexion à OneDrive était automatiquement créée par l\u0026rsquo;assistant. Appuyez sur Suivant pour continuer. Sélectionnez un classeur ou une table dans votre fichier. Appuyez sur Transformer les données pour continuer. Vous retrouvez tous les éléments de votre éditeur Power Query :\nLes menus. La liste des requêtes. L\u0026rsquo;aperçu du résultat de l\u0026rsquo;étape en cours de la requête. La liste des étapes de la requête. Une fois votre requête terminée, appuyez sur Enregistrer et fermer pour sauvegarder votre flux de données. Lorsque vous enregistrez votre flux de données, un écran de validation apparait une fois terminé, vous avez l\u0026rsquo;écran d\u0026rsquo;enregistrement.\nDonnez un nom à votre flux de données. Appuyez sur Enregistrer pour finir l\u0026rsquo;enregistrement. Paramétrages des flux de données Vous pouvez agir sur le flux de données dans l\u0026rsquo;espace de travail.\nAppuyez sur les 3 points pour ouvrir le menu du flux de données. Modifier vous permet d\u0026rsquo;ouvrir le flux de données dans l\u0026rsquo;éditeur Power Query Online pour apporter des modifications. Exporter .json vous permet de sauvegarder en local le flux de données au format json. Paramètres vous permet de modifier les paramètres du flux de données notamment Les informations de connexion aux sources de données. La planification des actualisations du flux de données. Cas d\u0026rsquo;usage Les scénarii d\u0026rsquo;usage des flux de données sont variés, on peut par exemple :\nMutualiser les transformations d\u0026rsquo;une source en un unique point réutilisable. Le flux de données devient la source de plusieurs jeux de données. La maintenance des transformations est centralisée. Le temps de transformation n\u0026rsquo;est pas démultiplié pour chaque jeu de données. Permettre l\u0026rsquo;accès à des données sensibles que l\u0026rsquo;on exposera après avoir enlevé une partie de leur sensibilité. Par exemple, accéder à des données financières détaillées en entrée et fournir en sortie des données agrégées. Permettre l\u0026rsquo;historisation de données ancienne. Par exemple, mettre à disposition les données des années passées sans les rafraichir tous les jours comme les données de l\u0026rsquo;année en cours. Cela allège de fait les temps de traitement. Limitations des flux de données Il existe des limitations liées aux flux de données ou au niveau de licence de l\u0026rsquo;espace de travail dans lequel est stocké le flux de données.\nSource de données Cloud ou On premise : un flux de données peut être composé de plusieurs requêtes et de plusieurs sources de données différentes, mais ne peux pas mixer des sources de données cloud et On Premise (utilisant une passerelle) dans le même flux. Fonctionnalitées Premium : certaines fonctionnalités des flux de données sont réservés aux espaces de travail premium. Dès que vous croisez des requêtes dans un flux de données, vous risquer d\u0026rsquo;être confronté à cette limitation sur un espace de travail non premium. Testez toujours le cycle complet de votre flux de données avant d\u0026rsquo;appuyer une architecture de données dessus. Conclusion Les flux de données sont un outil très intéressant dans le cadre d\u0026rsquo;une self service BI , vous pouvez mettre à disposition de vos utilisateurs des sources de données partagées dans le cloud. Il ne remplace pas un datawarehouse ou un datalakehouse, mais un complément permettant de répondre simplement à certains besoins.\nMerci de votre attention.\n","date":"2023-02-17T00:00:00Z","image":"https://blog.ddata.fr/p/mutualiser-vos-donnees-avec-les-dataflow/toa-heftiba-oQvESMKUkzM-unsplash_hu0f99de747ff578d771608bd9e3bcfcad_2719124_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/mutualiser-vos-donnees-avec-les-dataflow/","title":"Mutualiser vos données avec les dataflows"},{"content":"Méconnu des débutants, ils permettent de modifier la configuration d\u0026rsquo;un jeu de données après sa publication dans Power BI Service. Ce sont les paramètres Power Query.\nContexte Lorsque vous réalisez un projet Power BI, dans la phase d\u0026rsquo;intégration de données, vous allez utiliser Power Query et très certainement utiliser des éléments de configuration que vous répétez dans plusieurs requêtes. Par exemple si vous faites 3 requêtes sur le même dossier Sharepoint vous aurez 3 fois l\u0026rsquo;adresse de site utilisé dans vos requêtes, idem pour les informations de connexion à une base de données.\nSi vous devez modifier ces informations, cela va rapidement être fastidieux à corriger.\nLes paramètres Power Query sont là pour vous simplifier la vie. Vous allez pouvoir créer autant de paramètres que nécessaire dans Power Query et les utilisez afin de variabiliser vos autres requêtes Power Query.\nEn termes de maintenance les paramètres Power Query permettent de simplifier la réutilisation d\u0026rsquo;éléments dans plusieurs requêtes du même projet.\nVous allez aussi pouvoir variabiliser des informations de connexion aux sources de données et les modifier dans le service Power BI. Dans ce genre de scénarii vous pouvez avoir des sources de données que vous utilisez pour le développement, une fois le jeu de données publié vous modifier le paramètre afin que votre source de données pointe vers une source de production.\nCréation et utilisation de paramètre dans Power Query Création du paramètre Pour créer un paramètre Power Query, ouvrez votre projet Power BI et rendez-vous dans l\u0026rsquo;éditeur Power Query.\nAppuyez ici pour ouvrir l\u0026rsquo;éditeur Power Query. Une fois, dans l\u0026rsquo;éditeur Power Query, ouvrez le panneau de gestion des paramètres.\nAppuyez ici pour ouvrir le panneau de gestion des paramètres. L\u0026rsquo;écran de gestion des paramètres apparait, vous pouvez maintenant créer un nouveau paramètre :\nAppuyez sur Nouveau pour ajouter un paramètre. Définir un nom et donner une description à votre paramètre. Définir si la valeur du paramètre est obligatoire (interdire les valeurs vides). Donner un type de donnée au paramètre. Donner une valeur au paramètre. Appuyez sur OK pour enregistrer vos modifications. Vous pouvez voir la liste de tous les paramètres ici. Vous pouvez créer plusieurs paramètres avant d\u0026rsquo;appuyer sur OK.\nDans Valeurs suggérées vous pouvez aussi choisir une liste de valeur possible saisie manuellement ou étant le résultat d\u0026rsquo;une requête.\nUne fois votre ou vos paramètres créer vous les retrouver dans le panneau de requête.\nModification d\u0026rsquo;une requête Power Query Une fois votre paramètre créer vous pouvez l\u0026rsquo;utiliser dans vos requêtes Power Query où vous. En fonction de son type, vous pourrez :\nPour les paramètres texte, les utiliser dans une concaténation ou un filtre. Pour des nombres, les utiliser comme multiplicateur ou autres opérations mathématiques. Pour les dates, les utiliser pour filtrer vos requêtes. Voici comment par exemple remplacer l\u0026rsquo;adresse d\u0026rsquo;un site SharePoint dans une requête par un paramètre.\nFaite un clic droit sur la requête. Sélectionnez Editeur avancé. Vous pouvez aussi appeler l\u0026rsquo;éditeur avancé depuis le menu Accueil.\nL\u0026rsquo;éditeur de requête avancé s\u0026rsquo;ouvre.\nL\u0026rsquo;adresse du site SharePoint est présente 2 fois dans mon code en dehors du nom des étapes.\nVous pouvez donc remplacer la valeur textuelle par la valeur de votre paramètre. Pour rappel la concaténation de chaine de caractère se fait à l\u0026rsquo;aide de l\u0026rsquo;opérateur \u0026amp; en Power Query.\nAttention n\u0026rsquo;oubliez pas que Power Query est sensible à la casse (distinction minuscule/majuscule) c\u0026rsquo;est une grosse source d\u0026rsquo;erreur quand on édite une requête Power Query.\nLe résultat de l\u0026rsquo;exemple précédent est donc :\nAppuyez sur OK pour enregistrer vos modifications. Modification du paramètre dans le service Une fois que votre fichier PBIX sera publié dans le service Power BI vous pourrez accéder aux paramètres est agir sur leur valeur.\nRendez-vous dans l\u0026rsquo;espace de travail où se trouve le rapport.\nSur votre jeu de données, sélectionnez les 3 points. Cliquez sur Paramètres. Il s\u0026rsquo;agit ici des paramètres du jeu de données, ils contiennent les paramètres Power Query mais pas seulement.\nOuvrez la section Paramètres. Modifiez les valeurs de paramètres si besoin. Appuyez sur Appliquer pour enregistrer vos modifications. Ça y est vos requêtes Power Query vont se rafraichir avec de nouveaux paramètres au prochain rafraichissement du jeu de données.\nScénarii d\u0026rsquo;usage des paramètres Les paramètres répondent à de nombreux scénarii d\u0026rsquo;usage, notamment :\nLa modification dans le service des informations de connexion aux sources de données, cela permet d\u0026rsquo;avoir un seul PBIX pour des environnements DEV / UAT / PROD par exemple. La centralisation d\u0026rsquo;élément variable dans plusieurs requêtes d\u0026rsquo;un projet. La mise en place d\u0026rsquo;un filtre dynamique sur nos données pour par exemple ne prendre qu\u0026rsquo;un sous-ensemble des valeurs de notre base de données en développement, mais pas en production. Merci de votre attention.\n","date":"2023-02-12T00:00:00Z","image":"https://blog.ddata.fr/p/parametres-power-qwery/moises-de-paula-HPZZHJ-LuDI-unsplash_hu792653f7b7326a670abe032c0e66ac08_1629682_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/parametres-power-qwery/","title":"Les paramètres Power Query"},{"content":"Dans cet article, je vous présente les différentes sources possibles pour vos reportings. Les solutions natives de vos applications, un datawarehouse ou un datalakehouse ? Je vous propose de passer en revue ces solutions.\nLe reporting de données sans la BI Vos applications sont en général fournies avec des possibilités de reporting dédié fournies par l\u0026rsquo;éditeur de la solution.\nLes avantages de ces solutions de reporting natives sont principalement :\nLe coût normalement inclut dans la solution. Des rapports en lien très fort avec les données de la solution. Par contre les inconvénients sont en général :\nUne faible capacité de customisation et d\u0026rsquo;intégration de règle spécifique à votre usage. L\u0026rsquo;impossibilité de croiser avec d\u0026rsquo;autres sources de données extérieures à la solution. Si l’on faisait une analogie avec la cuisine, nous avons affaire à un plat industriel prêt à être consommé.\nOn peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui délivre des rapports aux utilisateurs via des outils de reporting différents les uns des autres.\nLa BI à la rescousse ! Comment faire mieux que cette solution de données en silo ? La réponse est simplement l\u0026rsquo;informatique décisionnelle souvent appelée BI acronyme de son nom anglais Business Inteligence.\nL\u0026rsquo;objectif de la BI est de vous permettre d\u0026rsquo;utiliser un seul outil pour tous vos reportings, basé sur une source de données consolidées permettant le croisement des données de vos différents silos de données grâce à la mise en relation des données communes.\nLes services attendus d\u0026rsquo;une couche data Pour réaliser cet objectif, les solutions proposées par la BI ont évoluée dans le temps, mais les services attendus par cette couche intermédiaire reste les mêmes :\nCapacité de traitement et de stockage dédié distinct de l\u0026rsquo;infrastructure de l\u0026rsquo;application Centralisation des données Modélisation des données optimisée pour le reporting (modèle en étoile) Historisation des données Pour certaine dimension, il est nécessaire de conserver la trace des changements, on parle en général de dimension à variation lente (Slow changing dimension). Pour certaines tables de faits, comme les stocks, on souhaite conserver des photos à un instant T des données afin de les assembler en un film de données permettant de suivre l\u0026rsquo;évolution des changements dans le temps. Réaliser des transformations des données sources comme par exemple passées de données cumulées (non agrégable) en données journalière (agrégable). Cette liste n\u0026rsquo;est pas exhaustive des services attendus par les datatrucs.\nLes datawarehouses La solution que l\u0026rsquo;on retrouve le plus fréquemment dans les solutions BI historiques des entreprises est le datawarehouse ou entrepôt de données en bon français.\nUn datawarehouse est basiquement une base de données dédiée au reporting. Cette base de données est chargée via un ETL (acronyme d\u0026rsquo;Extract Transform Load). Le chargement est en général réalisé par des traitements en mode batch, en opposition à un traitement en temps réel, qui charge les données à une fréquence prédéfinies, classiquement une fois par jour.\nLes données subissent des transformations lors du chargement afin que les tables résultantes soient optimisées pour le reporting, on passe en général d\u0026rsquo;une modélisation OLTP (celle de l\u0026rsquo;application) à une modélisation en étoile.\nLes datawarehouses sont des projets techniques en général réalisés par l\u0026rsquo;IT de l\u0026rsquo;entreprise.\nSi l’on faisait une analogie avec la cuisine, nous avons affaire à un plat fait maison, mais l\u0026rsquo;œuf utilisé pour la tarte ne peut plus servir à faire des crêpes.\nOn peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui sont chargées dans une base intermédiaire dédié au reporting, le datawarehouse.\nLe reporting peut être réalisé par un outil unique ou pas en fonction des besoins des utilisateurs.\nLes datalakehouses Les datawarehouses ont certains inconvénients et ne sont pas optimisés pour répondre à certains besoins modernes :\nLes données étant stockées dans une base de données, ils supportent très mal les données non structurées et les changements de structure des données dans le temps. Les données de type streaming (données en temps réel comme un capteur ou flux de données twitter) sont difficiles à intégrer dans un datawarehouse. Pour répondre à ces besoins modernes tout en gardant les services fournis par les datawarehouses, la BI moderne propose comme réponse les datalakehouses.\nUn datalakehouse est la fusion des datalakes et des datawarehouses. J\u0026rsquo;ai expliqué ce qu\u0026rsquo;est un datawarehouse ci-dessus, je vais donc expliqué le concept de datalake.\nLes datalakes sont la réponse au stockage de données répondant aux critères suivants :\nIl accepte les formats multiples Données structurées en provenance des bases de données Données semis structuré comme des fichiers Excel, JSON, CSV Données non structurées comme des images ou vidéos, qui pourront subir des traitements de type IA pour en extraire de l\u0026rsquo;information exploitable. Il supporte des fréquences de chargement variables allant du mode batch de datawarehouse au temps réel. Ce stockage permettant tous ces cas d\u0026rsquo;usages n\u0026rsquo;est ni plus ni moins qu\u0026rsquo;un stockage avec les mêmes services qu\u0026rsquo;un disque dur, mais disponible en général dans le cloud.\nLe plus important avec un datalake est la rigueur avec laquelle on range ces données afin de ne pas finir avec un dataswamp ou marais de données.\nUn datalakehouse est donc une architecture de raffinage de données composé d\u0026rsquo;un datalake et de divers outils permettant le raffinage des données. Différents outils peuvent être utilisés aux différentes phases de raffinage en fonction des besoins.\nLes principales étapes de raffinage des données sont :\nL\u0026rsquo;alimentation du datalake avec les données brutes, sans transformation, dans une zone que l\u0026rsquo;on nomme couramment Bronze. Le raffinage des données brut à l\u0026rsquo;aide de divers traitements en fonction des besoins, on stocke le résultat dans une zone que l\u0026rsquo;on nomme couramment Silver. La mise à disposition des données de reporting dans une zone que l\u0026rsquo;on nomme couramment Gold. Les données peuvent par exemple être préaggrégées pour améliorer le reporting. On conserve les données de chaque étape ainsi on peut, si besoin, ajouter de nouveaux traitements sur d\u0026rsquo;anciennes données.\nLes données du datalake du dalakehouse seront donc disponibles pour d\u0026rsquo;autres usages tels que :\nDu machine learning. Un référentiel d\u0026rsquo;entreprise mis à disposition d\u0026rsquo;autres applications. Si l’on faisait une analogie avec la cuisine, nous avons affaire à un garde-manger où on en prend les éléments que l\u0026rsquo;on souhaite cuisiner en fonction des besoins.\nOn peut donc représenter cette solution avec le schéma ci-dessous avec des applications en silo qui sont chargées dans un stockage intermédiaire sans usage spécifique puis préparé en fonction des besoins, notamment le reporting.\nJ\u0026rsquo;espère que maintenant vous comprenez mieux l\u0026rsquo;utilisation de ces différents datatrucs.\nMerci de votre attention.\n","date":"2023-02-05T00:00:00Z","image":"https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/antoine-petitteville-RIdYHUISNuM-unsplash_hu8a7a29d89b178f9f3af884e9f947f179_1540306_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/reporting-datawarehouse-datalakehouse/","title":"Datawarehouse ou datalakehouse, quelle source pour vos reportings ?"},{"content":"Dans cet article je vous présente pbi-tools, l\u0026rsquo;outil qui vous permet de faire du suivi de version sur vos projets Power BI et bien plus.\nContexte Dans tout projet informatique, on est confronté aux problèmes de suivi des changements dans le code source. Un projet Power BI n\u0026rsquo;échappe pas à ce problème et à l\u0026rsquo;heure actuelle Microsoft ne propose pas de solution In the box pour répondre à ce problème.\nL\u0026rsquo;outil Power BI tools va vous permettre de diviser un fichier Power BI en une multitude de fichiers et vous pourrez ensuite utiliser un outil de gestion de configuration tel que Git pour suivre les changements dans votre projet Power BI.\nLe but du présent article n\u0026rsquo;est pas de vous expliquez le fonctionnement des outils de gestion de configuration tels que Git, vous trouverez beaucoup de ressources d\u0026rsquo;apprentissage sur Git comme par exemple celle de korben.info.\nInstallation Pour installer pbi-tools, cliquez sur le lien de téléchargement de la version pbi-tools Desktop CLI de la page d\u0026rsquo;accueil du site de pbi-tools pour tester l\u0026rsquo;outil. Téléchargez et installez l\u0026rsquo;outil tel que décrit sur la page dédiée du site. Dans un contexte d\u0026rsquo;entreprise, vous aurez probablement besoin de l\u0026rsquo;autorisation et/ou le support de l\u0026rsquo;IT pour faire cette opération.\nUsage pbi-tools est un outil technique, il ne possède pas d\u0026rsquo;interface graphique utilisateur, il s\u0026rsquo;utilise en ligne de commande.\nAu-delà de son interface austère, pbi-tools peut vous apporter un grand service, en effet il vous permet de décomposer un fichier PBIX en un ensemble de fichiers texte, un pour chaque sous composant notamment :\nDans le dossier Model on retrouve : un dossier query contenant un fichier par requête Power Query. Les fichiers ont l\u0026rsquo;extention m mais contienne du texte. Un dossier tables contenant un dossier par table du modèle Power BI, chaque dossier contient (si besoin) : un fichier table.json contenant les données techniques de la table. un dossier columns avec un fichier json décrivant chaque colonne de la table et un fichier dax contenant le code DAX de chaque colonne calculée du modèle. un dossier hierarchies avec un fichier json décrivant chaque hiérarchie du modèle. un dossier measures avec un fichier json décrivant chaque mesure et un fichier dax contenant le code DAX de la mesure. Un dossier Report contenant un dossier section par table du modèle Power BI, chaque dossier contient (si besoin) : un fichier config.json et report.json contenant des informations sur les pages. un dossier reports avec un dossier par page de vos rapports contenant 3 fichiers par visuel (config.json / filters.json / visualContainer.json) Votre fichier PBIX est donc fragmenté en une multitude de fichiers.\nÀ quoi cela peut-il servir ? Avec un outil tel que Git vous allez pouvoir monitorer les changements dans chaque fichier et vous pourrez comparer chaque version que vous aurez dans Git l\u0026rsquo;une par rapport à l\u0026rsquo;autre.\nPrenons un exemple, vous cherchez à comprendre quelle modification a subie une mesure DAX dans les 10 dernières versions, sur les 6 derniers mois ? Si votre fichier PBIX est décomposé par pbi-tools et suivi régulièrement dans Git vous ne mettrez que quelques secondes à répondre à la question. Dans le cas contraire, je vous laisse évaluer la charge de travail\u0026hellip;\npbi-tools vous permet aussi de recréer votre fichier PBIX à partir de l\u0026rsquo;ensemble des fichiers. On peut imaginer des scénarii d\u0026rsquo;automatisation dans lesquels vous réalisez des modifications manuelles sur les fichiers avant de les recomposer.\nCe genre de scénario avancé demande de procéder avec la plus grande précaution et beaucoup de tests pour valider que tout ce passe bien jusqu\u0026rsquo;à la mise en production.\nMerci de votre attention.\n","date":"2023-01-29T00:00:00Z","image":"https://blog.ddata.fr/p/pbitools/amy-shamblen-eT8vx7LLorY-unsplash_hu82c25fa98cd4e9a8103220017be4b8cb_577052_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/pbitools/","title":"pbi-tools : diviser pour mieux régner"},{"content":"Dans cet article je vous présente ALM Toolkit, l\u0026rsquo;outil qui vous permet de comparer 2 jeux de données Power BI.\nContexte Parmi les outils externes de Power BI, ALM Toolkit fait partie des outils à connaitre. Cet outil va vous permettre de comparer la structure de deux jeux de données Power Bi, qu\u0026rsquo;ils soient locaux dans votre Power BI Desktop ou publiés dans le service.\nInstallation Pour installer ALM Toolkit, cliquez sur le lien Download latest version de la page d\u0026rsquo;accueil du site d\u0026rsquo;ALM Toolkit. Téléchargez et installez le fichier msi. Dans un contexte d\u0026rsquo;entreprise, vous aurez probablement besoin de l\u0026rsquo;autorisation et/ou le support de l\u0026rsquo;IT pour faire cette opération.\nUne fois installé vous retrouverez ALM Toolkit dans le menu Outils externes de Power BI. Si Power BI était ouvert lors de l\u0026rsquo;installation, fermez-le et réouvrez-le.\nSi ALM Toolkit ne s\u0026rsquo;ouvre pas, je vous invite à installer la version x86 de Visual C++ Redistributable Packages for Visual Studio 2013 comme décris dans l\u0026rsquo;article de blog ALM Toolkit would not open.\nUsage Vous pouvez comparer 2 jeux de données se trouvant au choix :\nOuvert dans votre Power BI Desktop. Dans un espace de travail Premium. Dans un fichier PBIT. Choisissez le modèle de référence à comparer. Choisissez le modèle cible. Appuyez sur OK pour lancer la comparaison. Synthèse des différences entre les 2 jeux de données. Si vous n\u0026rsquo;avez pas d\u0026rsquo;espaces de travail Premium pour utiliser la fonctionnalité de synchronisation entre les jeux de données, ALM Toolkit vous permettra \u0026ldquo;seulement\u0026rdquo; de voir les différences entre 2 jeux de données, à vous de réaliser les modifications manuellement.\nMerci de votre attention.\n","date":"2023-01-22T00:00:00Z","image":"https://blog.ddata.fr/p/almtoolkit/laura-ockel-UQ2Fw_9oApU-unsplash_hu272b112faa2067d59f4abbc2a04e4757_1908708_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/almtoolkit/","title":"ALM Toolkit"},{"content":"Dans cet article je vous présente Measure Killer, l\u0026rsquo;outil qui vous permet de nettoyer votre jeu de données Power BI.\nContexte Votre projet Power BI à bien grandit et vous vous demandez comment faire le tri entre les éléments de votre modèle que vous utilisez et les éléments inutiles. Measure Killer va vous aider à faire le tri.\nInstallation Il existe une version gratuite et 2 versions payantes de cet outil.\nDans cet article je vais exclusivement parler de la version gratuite.\nPour installer Measure Killer, cliquez sur un des liens dans la section *Current version *de la page d\u0026rsquo;accueil du site de Measure Killer. Téléchargez et installez la version admin ou portable. Dans un contexte d\u0026rsquo;entreprise, vous aurez probablement besoin de l\u0026rsquo;autorisation et/ou le support de l\u0026rsquo;IT pour faire cette opération.\nUne fois installé vous retrouverez Measure Killer dans le menu Outils externes de Power BI. Si Power BI était ouvert lors de l\u0026rsquo;installation, fermez-le et réouvrez-le.\nUsage Ouvrez le rapport que vous souhaitez traiter, dans le cadre de test, une copie sera plus appropriée.\nDans le menu Outils externes de Power BI, sélectionnez Measure Killer pour lancer le programme.\nSélectionnez Start (Free version) pour démarrer la version gratuite Sélectionnez Single report/dataset. Sélectionnez Execute pour démarrer l\u0026rsquo;analyse et indiquer au programme où se trouve le fichier source dans l\u0026rsquo;écran suivant. Vous pouvez explorer le résultat de l\u0026rsquo;analyse ici. Vous pouvez supprimer automatiquement les mesures et colonnes non utilisées. Vous pouvez exporter le résultat dans Excel pour documenter votre modèle. Cet outil est très utile pour vous permettre de nettoyer vos projets Power BI mais faite très attention, les mesures non utilisées le sont dans le rapport en cours, dans le cadre de self-service BI et de jeu de données partagé où ces mesures sont peut être utilisé. Votre expertise du modèle doit primer sur la suppression automatique.\nMerci de votre attention.\n","date":"2023-01-15T00:00:00Z","image":"https://blog.ddata.fr/p/measurekiller/jeshoots-com-__ZMnefoI3k-unsplash_hu7280f2b0e20a4865b2928faa11c122f2_668657_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/measurekiller/","title":"Measure Killer"},{"content":"Vous avez décidé de vous lancer dans un projet Power BI et vous vous demandez comment vous y prendre ? Je vous propose de vous présenter, dans les grandes lignes, comment mener un projet BI.\nContexte Vous débutez dans la réalisation d\u0026rsquo;un projet BI ?\nBonne nouvelle, quel que soit l\u0026rsquo;outil que vous utiliserez un projet BI se réalise toujours de la même manière. Je vous propose dans cet article une présentation simple de la réalisation d\u0026rsquo;un projet BI, dans le cadre de l\u0026rsquo;utilisation de Power BI.\nLes phases d\u0026rsquo;un projet BI Avant tout, un projet BI n\u0026rsquo;est pas un projet statique, mais un projet en perpétuel enrichissement. Vous allez commencer en intégrant certaines données, puis au fil des itérations du projet vous allez améliorer votre projet en répétant les différentes phases du projet qui sont les suivantes :\nPhase 0 : identification des sources de données Lors de cette phase, vous identifiez les sources de données que vous souhaitez intégrer dans votre projet. Pensez déjà à la méthode que vous utiliserez pour rafraichir la donnée en privilégiant les solutions automatisables.\nPhase 1 : préparation des données Lors de cette phase, vous allez mettre en place un processus de traitement de données que l\u0026rsquo;on appelle ETL dans les projets BI, c\u0026rsquo;est l\u0026rsquo;acronyme anglais de Extract Transform Load. Dans le cadre d\u0026rsquo;un projet Power BI vous utiliserez l\u0026rsquo;outil Power Query, intégrer à Power BI, pour réaliser cette phase du projet. Voici les tâches à réaliser :\nExtraire les données : l\u0026rsquo;objectif est de lire les données depuis la source de données (Base de données, fichier Excel, API Web ou autres). Un projet BI doit TOUJOURS être en lecture seule sur la source. On ne réalise pas de modifications dans la source, mais dans une copie lors de l\u0026rsquo;étape de transformation. Transformer les données : Lors de cette phase nous allons transformer les données sources notre objectif est de créer une tables de faits ou une table de dimension pour notre modèle de données en étoile. Les principales étapes consisteront à : Définir les colonnes que l\u0026rsquo;on conserve Nommer les colonnes dans le langage métier. Mettre le bon type de données aux colonnes (texte, entier, date \u0026hellip;). Nettoyer les données dans les lignes si besoin. Restructurer les données si besoin afin de créer des tables de faits ou de dimensions. Enrichir notre table avec d\u0026rsquo;autres sources de données. Charger les données : Maintenant que nos données sont prêtes nous allons les charger dans Power BI, pour cela on utilise le bouton Fermer et appliquer de Power Query. À la fin de cette étape, Power BI contient un ensemble de tables chargées que vous allez utiliser lors de la prochaine phase.\nPhase 2 : modélisation des données Vos données sont maintenant disponibles sous forme de table dans Power BI. L\u0026rsquo;objectif est maintenant de réaliser un modèle de données métier qui sera utilisé par vos utilisateurs dans le cadre d\u0026rsquo;une self-service BI par exemple.\nLe modèle de données est essentiel, il est composé :\nDes tables de données reçues de Power Query. De relations entre les tables. De mesures et colonnes calculées créées en DAX. Un bon modèle de données est un modèle que les utilisateurs comprennent rapidement. Il doit être documenté afin que la définition des mesures soit partagée et comprise. Idéalement il ne doit pas y avoir d\u0026rsquo;éléments techniques visibles de l\u0026rsquo;utilisateur de modèle.\nLe langage DAX s\u0026rsquo;appuie sur un modèle en étoile afin d\u0026rsquo;exprimer toute sa puissance, si vous ne comprenez pas de quoi je parle, documentez-vous avant 😁.\nPhase 3 : présentation des données On arrive à la phase visible du projet. Vous allez utiliser le modèle de données créer lors de la phase précédente pour alimenter les différentes visualisations de votre futur rapport. Naturellement vous ne trouverez pas immédiatement le bon indicateur ou vous découvrirez qu\u0026rsquo;il vous manque des données. Vous devrez alors repartir dans le cycle afin de\u0026rsquo;enrichir votre modèle.\nSelf-service BI vs BI d\u0026rsquo;entreprise, quelles différences ? Si un projet BI se fait toujours de la même manière, quelle est donc la différence entre la self-service BI et BI d\u0026rsquo;entreprise ?\nC\u0026rsquo;est principalement une question d\u0026rsquo;approche projet : qui fait quoi ? Mais aussi cela dépend aussi souvent de la source de données.\nResponsabilité de réalisation Un projet BI d\u0026rsquo;entreprise est souvent mené par l\u0026rsquo;IT de l\u0026rsquo;entreprise. Il s\u0026rsquo;agit de mettre à disposition des modèles de données et des rapports respectant des standards de qualité. La méthodologie de projet et les techniques utilisées pour le développement de logiciel sont utilisées pour la réalisation du projet BI d\u0026rsquo;entreprise. L\u0026rsquo;IT mène ce genre de projet dans le cadre de ses autres activités, et comme pour tout projet informatique vous rencontrerez notamment les inconvénients suivants :\nMauvaise compréhension de besoin. Mauvaise expression du besoin. Modification du besoin entre la collecte de ce dernier et la mise à disposition de résultat. Délai de réalisation (toujours) trop long. Et comme la loi de Murphy s\u0026rsquo;applique, vous avez de grande chance de tous les rencontrer à chaque fois. Un projet en self-service BI est réalisé par un service ou une personne ayant de bonnes connaissances du métier et des données, car elle travaille déjà avec régulièrement. Cette personne doit apprendre l\u0026rsquo;utilisation d\u0026rsquo;un outil comme Power BI et réalise le projet BI par elle-même ou avec de l\u0026rsquo;accompagnement.\nOn peut représenter simplement cette différence ainsi :\nNature des sources de données Un projet BI d\u0026rsquo;entreprise a pour objectif de mettre à disposition du plus grand nombre des données d\u0026rsquo;entreprise. En général il permet d\u0026rsquo;accéder à l\u0026rsquo;ensemble des données dans un seul modèle intégrant les contraintes de sécurité d\u0026rsquo;accès aux données. De plus les sources primaires de type base de données ou API demandent des compétences spécifiques que l\u0026rsquo;on retrouve dans les services IT. Enfin ce type de projet utilise fréquemment des couches intermédiaires de données permettant une première modélisation. Cette couche est fréquemment sous la forme d\u0026rsquo;un datawarehouse ou d\u0026rsquo;un datalakehouse pour les architectures modernes dans le cloud.\nUn projet en self-service BI est en général fait avec les moyens du bord, les données utilisées sont souvent des données spécifiques à un service en termes de scope. Vous n\u0026rsquo;avez pas forcément l\u0026rsquo;accès à la donnée source primaire en termes de sécurité ou de connaissance. Dans ce cas des extractions limitées à un sous-ensemble de données sont utilisées pour ces projets qui de fait ne pourront pas répondre à des besoins d\u0026rsquo;entreprise.\nAu-delà de cette organisation Entre self-service BI et BI d\u0026rsquo;entreprise on peut imaginer d\u0026rsquo;autres niveaux de partage de responsabilité, on parle parfait de self-service managed BI pour des scénarii dans lesquels l\u0026rsquo;IT développe et modélise les couches bases de données et les mets à la disposition des métiers qui peuvent être en responsabilité de l\u0026rsquo;enrichissement, par exemple l\u0026rsquo;ajout des objectifs, mais aussi de la réalisation des rapports. Libre à vous d\u0026rsquo;adapter le produit à votre mode de fonctionnement.\nConclusion Retenez qu\u0026rsquo;un projet BI n\u0026rsquo;est jamais un projet statique. Vos données bougent, vos besoins changent et plus vous exposez de la donnée, plus de nouveaux besoins apparaissent comme par magie. Vous devez donc être agile, quand vous réalisez une tâche pensez à celui qui devra venir après vous dans quelque temps pour changer des choses. Votre Moi futur vous remerciera, car cette personne c\u0026rsquo;est probablement vous même.\nMerci de votre attention.\n","date":"2023-01-08T00:00:00Z","image":"https://blog.ddata.fr/p/comment-faire-un-projet-bi/un-liu-5t9qbyCslIs-unsplash_hu633350e3efb25b0fa50f31c8f536a31f_1331748_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/comment-faire-un-projet-bi/","title":"Comment faire un projet BI."},{"content":"Dans cet article je vous présente Power BI Model Documenter, l\u0026rsquo;outil qui extrait automatiquement des informations de vos fichiers Power BI pour créer une documentation technique.\nContexte La documentation est une tâche fastidieuse sur vos créations et encore plus compliquée à faire sur des fichiers Power BI que vous n\u0026rsquo;avez pas créés. Power BI Model Documenter est là pour vous aider à réaliser cette étape de vos projets BI.\nInstallation Pour installer Power BI Model Documenter, cliquez sur le lien Power BI Model Documenter v2.1.0 de la page d\u0026rsquo;accueil du site de Power BI Model Documenter. Le numéro de version peut changer après la publication de cet article. Téléchargez et installez la version disponible. Dans un contexte d\u0026rsquo;entreprise, vous aurez probablement besoin de l\u0026rsquo;autorisation et/ou le support de l\u0026rsquo;IT pour faire cette opération.\nUne fois installé vous retrouverez Power BI Model Documenter dans le menu Outils externes de Power BI. Si Power BI était ouvert lors de l\u0026rsquo;installation, fermez-le et réouvrez-le.\nUsage Pour utiliser Power BI Model Documenter commencer par ouvrir le fichier PBIX que vous souhaitez documenter, puis rendez-vous dans le menu Outils externes de Power BI et appuyez sur Model Documenter. Une fenêtre noire de script apparait un instant, l\u0026rsquo;outil est en train d\u0026rsquo;extraire les informations du fichier actuel de Power BI puis ouvre une nouvelle fenêtre de Power BI pour importer les informations extraites dans un nouveau fichier Power BI qui contiendra la documentation de votre premier fichier.\nDans cette documentation vous trouverez :\nLes paramètres Power Query. Les tables. Les partitions Les colonnes. Les groupes de calcul. Les paramètres de champs. Les mesures. Les relations. Les informations de sécurité RLS et OLS. Bref, toutes les informations concernant votre modèle de données dans un Power BI.\nMerci de votre attention.\n","date":"2023-01-01T00:00:00Z","image":"https://blog.ddata.fr/p/powerbimodeldocumenter/bernd-klutsch-nE2HV5AUXFo-unsplash_hu7b4b3bff9c5502582b0d9cd34cf4487c_862909_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/powerbimodeldocumenter/","title":"Power BI Model Documenter"},{"content":"Dans cet article je vous présente Power BI Gateway Monitor, l\u0026rsquo;outil qui vous permet de centraliser les logs de vos différentes passerelles Power BI et de les analyser.\nContexte Si vous utilisez des sources de données, on premise, vous utilisez une passerelle Power BI. Rapidement dans le cadre d\u0026rsquo;une bonne gouvernance de votre Power BI vous allez devoir mettre en œuvre des outils pour suivre l\u0026rsquo;activité des différents composants de Power BI.\nPower BI Gateway Monitor, un outil gratuit, va vous permettre de centraliser les logs de vos passerelles, soit sur un stockage Azure, soit sur un partage réseau on premise, et vous pourrez ainsi avoir un suivi centralisé de vos passerelles.\nInstallation Power BI Gateway Monitor est un projet que vous trouverez sur Github : https://github.com/RuiRomano/pbigtwmonitor.\nCe projet est composé d\u0026rsquo;un script PowerShell qui va automatiser l\u0026rsquo;extraction de données de log de vos passerelles pour vous et de template Power BI afin de mettre ces données en forme.\nLe guide d\u0026rsquo;installation sur la page d\u0026rsquo;accueil est très complet, vous avez 2 modes d\u0026rsquo;utilisation des scripts : en local ou via un stockage Azure afin de centraliser les logs dans un espace directement accessible à Power BI.\nUsage Le script Run.ps1 doit être installé et planifié sur chaque passerelle.\nModifiez le fichier de configuration Config.json pour chaque passerelle. Utilisez le template Power BI associé à votre choix de stockage pour avoir une bonne vision de vos passerelles Power BI.\nMerci de votre attention.\n","date":"2022-12-24T00:00:00Z","image":"https://blog.ddata.fr/p/pbigwtmonitor/matt-artz-4KaY-etsnKc-unsplash_hu758a226a991c2b16a831eb60265cd660_934145_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/pbigwtmonitor/","title":"Power BI Gateway Monitor"},{"content":"Dans cet article je vous présente Power BI Monitor, l\u0026rsquo;outil indispensable dans le cadre d\u0026rsquo;une bonne gouvernance de Power BI.\nContexte Dès que vous utilisez le service Power BI régulièrement dans le cadre d\u0026rsquo;une self-service BI d\u0026rsquo;entreprise, vous allez avoir besoin de pouvoir monitorer en détail ce qu\u0026rsquo;il se passe dans votre service Power BI, cela fait partie de la gouvernance Power BI.\nMicrosoft vous met à disposition un grand nombre d\u0026rsquo;informations, tel que les logs d\u0026rsquo;activité Power BI, mais pas d\u0026rsquo;outils pour les exploiter facilement. C\u0026rsquo;est ici qu\u0026rsquo;intervient Power BI Monitor. Cet outil gratuit va vous permettre d\u0026rsquo;avoir une vision globale de votre service Power BI en exploitant diverses informations déjà disponibles.\nInstallation Power BI Monitor est un projet que vous trouverez sur Github : https://github.com/RuiRomano/pbimonitor.\nCe projet est composé d\u0026rsquo;un ensemble de scripts PowerShell qui vont automatiser un certain nombre de tâches d\u0026rsquo;extraction de données pour vous et de template Power BI afin de mettre ces données en forme.\nLe guide d\u0026rsquo;installation sur la page d\u0026rsquo;accueil est très complet, vous avez 2 modes d\u0026rsquo;utilisation des scripts : en local ou via des services Azure afin de tout automatiser.\nLes scripts PowerShell s\u0026rsquo;appuient sur le module PowerShell Power BI qui doit être installé. Vous trouverez ce module dans le projet Github : https://github.com/Microsoft/powerbi-powershell. Vous pouvez simplement utiliser la commande d\u0026rsquo;installation suivante dans un fenêtre PowerShell (en mode Administrateur) :\n1 Install-Module -Name MicrosoftPowerBIMgmt -RequiredVersion 1.2.1026 Vous pouvez utiliser ces scripts avec un service principal pour vous authentifier (recommandé) ou avec votre compte utilisateur.\nUsage Pour lancer le script, vous devez modifier le fichier Config.json :\nPour vous authentifier avec un compte utilisateur, supprimé la section ServicePrincipal. Pour vous authentifier avec un service principal, renseigné la section ServicePrincipal. Le script Fetch - Run.ps1 sert de lanceur pour les autres scripts. Vous pouvez choisir les scripts que vous lancez en mettant en commentaire les appels du tableau *scriptsToRun * de la ligne 4. Les 4 scripts appelés ont chacun une fonction différente :\nFetch - Activity.ps1 : Permets de lire et de stocker les logs du service Power BI. Attention seul les 30 derniers jours de log sont disponible dans le service, pensez à exécuter ce script régulièrement. Fetch - Catalog.ps1 : Permets d\u0026rsquo;obtenir l\u0026rsquo;ensemble des objets présents dans le service Power BI via les API admin WorkspaceInfo. Fetch - DataSetRefresh.ps1 : Permets d\u0026rsquo;obtenir les informations des refreshs des jeux de données, ne fonctionne que pour les espaces de travail dont l\u0026rsquo;utilisateur ou le service principal sont membres. Fetch - Graph.ps1 : Permets d\u0026rsquo;obtenir les informations de l\u0026rsquo;API Graph notamment l\u0026rsquo;état des licences Power BI. Si vous lancez les scripts avec vos logins utilisateur vous devrez avoir un niveau Administrateur Power BI pour les 2 premiers.\nUne fois les données collectées, utilisez les templates Power BI pour exploiter les données. Changer le paramètre DataLocation en indiquant le dossier contenant les données.\nMaintenant vous avez une bonne vision de votre service Power BI.\nMerci de votre attention.\n","date":"2022-12-17T00:00:00Z","image":"https://blog.ddata.fr/p/pbimonitor/tamanna-rumee-FtJEat_S7Q4-unsplash_hu38957ebb2a36eb160a7da9484a087a83_913911_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/pbimonitor/","title":"Power BI Monitor"},{"content":"Dans cet article je vous présente Tabular Editor, l\u0026rsquo;outil qui vous permet d\u0026rsquo;allez plus loin avec vos modèles de données Power BI.\nContexte Dans la galaxie des outils externe de Power BI, Tabular Editor est sans conteste sur le podium. Cet outil va vous permettre d\u0026rsquo;entrer dans les entrailles de vos modèles Power BI et d\u0026rsquo;étendre vos possibilités.\nAutre point important, si vous passez la certification DP-500, vous aurez des questions sur l\u0026rsquo;utilisation de cet outil.\nInstallation Il existe 2 versions de Tabular Editor :\nLa version 2, open source et gratuite La version 3 payante avec de nombreuses nouveautés Dans cet article je vais exclusivement parler de la version gratuite.\nPour installer Tabular Editor, cliquez sur le lien Download de la page d\u0026rsquo;accueil du site de Tabular Editor. Téléchargez et installez la version avec l\u0026rsquo;extension msi, qui est un installeur Windows facile à utiliser. Dans un contexte d\u0026rsquo;entreprise, vous aurez probablement besoin de l\u0026rsquo;autorisation et/ou le support de l\u0026rsquo;IT pour faire cette opération.\nUne fois installé vous retrouverez Tabular Editor dans le menu Outils externes de Power BI. Si Power BI était ouvert lors de l\u0026rsquo;installation fermez-le et réouvrez-le.\nUsage Tabular Editor est avant tout un outil technique, c\u0026rsquo;est-à-dire qu\u0026rsquo;il a une interface visuelle plus brute que Power BI et que son usage est pour un public averti. En effet il est très simple d\u0026rsquo;endommager votre modèle de données si vous faites n\u0026rsquo;importe quoi. Vous devez impérativement travailler sur des copies de vos originaux pour vos phases de formation et de tests avec cet outil.\nLa liste des fonctionnalités de Tabular Editor est impressionnante. Vous allez pouvoir :\nÉditer toutes les propriétés de vos tables. Créer et éditer vos mesures DAX. Voir les dépendances de vos mesures DAX. Analyser votre modèle par rapport aux bonnes pratiques. Formater vos mesures DAX avec l\u0026rsquo;intégration de DAX Formatter. Implémenter les groupes de calcul. Scripteur la création et le renommage de mesure DAX. Implémenter la sécurité au niveau des objets (OLS). Les modifications réalisées dans l\u0026rsquo;interface de Tabular Editor sont automatiquement synchronisées avec votre fichier Power BI.\nLaissez-moi un commentaire si vous voulez un article détaillant l\u0026rsquo;usage de certaines fonctionnalités.\nMerci de votre attention.\n","date":"2022-12-11T00:00:00Z","image":"https://blog.ddata.fr/p/tabulareditor/dan-cristian-padure-noOXRT9gfQ8-unsplash_huded2a096120cee03ed9ff78bdbe4646d_2227739_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/tabulareditor/","title":"Tabular Editor - Allez plus loin avec Power BI"},{"content":"Vous cherchez un outil pour faire des captures d\u0026rsquo;écran ?\nDécouvrons ensemble ShareX.\nShareX est un outil gratuit permettant notamment :\nDe faire une copie de votre écran, d\u0026rsquo;une application ou d\u0026rsquo;une zone définie. D\u0026rsquo;enrichir l\u0026rsquo;image capturer. De copier, sauvegarder ou publier le résultat. Pour installer ShareX, rendez-vous sur (https://getsharex.com/)[https://getsharex.com/] pour le télécharger et l\u0026rsquo;installer.\nUne fois installé, vous pourrez déclencher une capture en appuyant sur le raccourci clavier ctrl + imp. ecran. Une mire apparait, vous avez 4 possibilités :\nCliquez quand le cadre en pointillé fait le tour de votre écran. Pointez une application, quand le cadre en pointillé est autour de la zone que vous voulez, cliquez pour déclencher la capture. Positionnez la mire ou vous voulez commencer la capture, maintenez le bouton enfoncé pour créer un rectangle représentant la zone de capture. Appuyez sur la touche Echap pour annuler la capture. Les actions que vous réalisez après la capture sont modifiables.\nFaite un clic sur l\u0026rsquo;icône de ShareX en bas à droite à côté de l\u0026rsquo;heure. Choisissez Tâches après la capture. Sélectionnez les tâches que vous voulez réaliser après une capture d\u0026rsquo;écran. Les tâches actives sont encadrées. Si vous avez choisi la tâche Modifier l\u0026rsquo;image, ce que je vous recommande, un éditeur d\u0026rsquo;image apparait après la capture. Voici les principales options que j\u0026rsquo;utilise :\nLes fonctions Enregistrer, Enregistrer sous et copier. L\u0026rsquo;outil pour ajouter des rectangles sur la capture. L\u0026rsquo;outil permettant d\u0026rsquo;ajouter des cercles avec des nombres pour pointer des éléments dans la capture. L\u0026rsquo;outil permettant de flouter une zone de la capture. Beaucoup d\u0026rsquo;autres options sont disponibles que je vous laisse découvrir.\nMerci de votre attention.\n","date":"2022-12-04T00:00:00Z","image":"https://blog.ddata.fr/p/sharex/patrick-OIFgeLnjwrM-unsplash_hu86800f9cc15b64238fdba3d87ec8d9f3_1386551_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/sharex/","title":"ShareX : faite des copies d'écran comme moi."},{"content":"Vous travaillez avec des données ? Il est temps de parler modélisation de données.\nAvertissement Je commence ici une série d\u0026rsquo;articles portant sur la modélisation de données.\nMon objectif est d\u0026rsquo;initier toutes personnes devant travailler avec les données aux notions de base de la modélisation, je m\u0026rsquo;adresse donc aux personnes ayant peu ou pas du tout de notion dans ce domaine.\nJe vais donc me permettre d\u0026rsquo;être incomplet, voire même simpliste, le but n\u0026rsquo;étant pas de faire de vous des maitres de la modélisation, mais juste de vous apporter les bases afin de comprendre les enjeux et l\u0026rsquo;importance de la modélisation quand on travaille avec les données.\nPourquoi modéliser les données Commençons par une analogie avec le monde animal, vos données sont des ressources et vous pouvez soit faire comme le bousier, accumuler vos ressources pour en faire une boule qui aura son utilité, mais principalement en surface, soit organiser vos ressources comme une abeille et les transformer en nectar.\nLa modélisation de données va vous permettre de passer du bousier à l\u0026rsquo;abeille 🤩.\nLa modélisation c\u0026rsquo;est quoi ? La modélisation des données c\u0026rsquo;est simplement le fait d\u0026rsquo;organiser et de structurer vos données d\u0026rsquo;une manière spécifique afin de répondre à votre besoin. En général le résultat de la modélisation est un ensemble de tables organiser selon une certaine logique.\nLes 3 grands types de modélisation Il existe plusieurs manières de modéliser les données. La première chose à comprendre c’est pourquoi nous avons différentes manières de faire. La réponse est simple : nous avons des besoins différents :\nQuand vous réaliser une application classique, avec Power apps par exemple, vous allez faire beaucoup d\u0026rsquo;écriture de données, à chaque enregistrement et beaucoup de lecture unitaire, à chaque fois qu\u0026rsquo;un utilisateur ouvre un enregistrement. Cet usage demande pour être performant d\u0026rsquo;avoir des tables le plus segmentées possible afin de ne pas avoir de redondance de données afin d\u0026rsquo;optimiser les multiples lecture et écriture. Il en ressort en général un modèle avec beaucoup de tables reliées entre elles avec des relations. Quand vous faite du reporting de données, avec Power BI par exemple, vous allez faire une seule écriture de données, lors du chargement du modèle et beaucoup de lecture massive voir totale des données. Dans ce cas un grand nombre de tables avec un grand nombre de relations pénalise les performances. Pour résoudre ce problème, nous allons modéliser nos données différemment que pour l\u0026rsquo;application source de la donnée. Pour les applications classiques Pour les applications classiques, nous allons faire une modélisation de données ayant pour but d\u0026rsquo;éviter la redondance de données et garantir la cohérence de nos données. On parle en général de modèle relationnel. En termes de méthodologie de modélisation, vous retrouverez principalement la notion de formes normales et de modélisation selon la méthode Merise pour les plus anciens.\nL\u0026rsquo;objectif de cette modélisation est de garantir le caractère ACID des transactions d\u0026rsquo;écriture des données :\nA - Atomicité : La propriété d\u0026rsquo;atomicité assure qu\u0026rsquo;une transaction se fait au complet ou pas du tout. C - Cohérence : La propriété de cohérence assure que chaque transaction amènera le système d\u0026rsquo;un état valide à un autre état valide. I - Isolation : Toute transaction doit s\u0026rsquo;exécuter comme si elle était la seule sur le système. D - Durabilité : La propriété de durabilité assure que lorsqu\u0026rsquo;une transaction a été confirmée. Source Wikipédia\nPour le reporting Schématiquement, lorsque vous faites du reporting, vous ne faites qu\u0026rsquo;une seule écriture en masse de votre base, comme pour le chargement d\u0026rsquo;un datawarehouse par un ETL ou le mode import de Power BI. Puis vous réalisez exclusivement des lectures sur un très gros volume de données.\nVous allez donc optimiser votre modélisation pour être performante en lecture et croisement de données, or ce qui est couteux pour ce genre d\u0026rsquo;opération c\u0026rsquo;est un grand nombre de relations entre les tables.\nC\u0026rsquo;est pour cette raison que nous modélisons les données pour Power BI suivant le modèle en étoile. Ce modèle est centré sur des tables de faits et des tables de dimensions. Généralement les tables de faits vont provenir de systèmes différents et nous allons faire communiquer entre elles les différentes tables de fait grâce aux dimensions qu\u0026rsquo;elles ont en commun.\nSource Microsoft Learn\nPour les applications distribuées Pou les applications distribuées, application dans le cloud nécessitant de fortes montées en charge et/ou des croissances exponentielles, nous allons faire une modélisation de données ayant pour but de permettre la monter en charge. Un moteur de base de données classique ne peut que faire de la croissance verticale. Si vous avez besoin de plus de puissance, vous utilisez une machine plus puissante. Ce modèle a des limites aussi bien physiques qu’économique, plus une machine est puissante, plus elle est chère.\nPour absorber des montées en charge exponentielles, le cloud propose des architectures permettant une montée en charge horizontale. Pour montée en puissance vous ajoutez des machines peu chères puis vous les retirez quand vous n\u0026rsquo;en avez plus besoin. Cela fonctionne bien avec les bases NoSQL (Not Only SQL). Ces bases proposent une modélisation différente des 2 précédentes. Pour faire simple, ces bases ne vont pas avoir le même niveau de service que les bases de données relationnelles traditionnelles et vont par exemple être optimisées pour des usages très particuliers. Par exemple une base comme MongoDB va proposer de stocker l\u0026rsquo;ensemble des données d\u0026rsquo;un client pour un site de e-commerce dans un seul document de type JSON. Ce document aura des données redondantes par rapport aux autres documents en base pour les autres clients. Mais chaque client pourra modifier son propre document, lors d\u0026rsquo;une nouvelle commande par exemple, sans créer une contrainte sur l\u0026rsquo;ensemble de la base, mais uniquement sur son document.\nParcour Microsoft Learn - Implémenter un modèle de données non relationnelles\nMerci de votre attention.\n","date":"2022-11-27T00:00:00Z","image":"https://blog.ddata.fr/p/modelisation-de-donnees-les-bases/robert-anasch-McX3XuJRsUM-unsplash_hu24d5abd74ebb0a153cabc62bc39d7d45_1806570_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/modelisation-de-donnees-les-bases/","title":"C'est quoi la modélisation de données ?"},{"content":"Vous avez des connaissances dans l\u0026rsquo;écosystème Microsoft ?\nValider vos acquis avec les certifications et ajouter de jolis badges à votre CV.\nJe vais vous expliquer comment cela se passe.\nPrérequis Je vais ici parler exclusivement des certifications Microsoft. Avant de commencer, vérifiez que vous avez bien un compte Microsoft Learn associé à votre email personnel. Cela est très important, car vous pourrez toujours y associer plus tard votre compte email professionnel, mais ce compte Microsoft Learn vous suivra au long de votre parcours professionnel.\nPour vous inscrire, rendez-vous sur le portail de Microsoft Learn pour créer un compte gratuit.\nVotre compte Microsoft Learn vous permettra de suivre vos avancées dans les différents parcours de formation et conservera la trace de vos futures certifications.\nChoisir une certification Vous trouverez l\u0026rsquo;ensemble des certifications Microsoft sur le portail des certifications.\nCe que vous devez savoir sur les certifications :\nLes certifications avec le chiffre 900 sont sur les fondamentaux d\u0026rsquo;un thème. Les certifications sont classées en fonction de leur code : AZ : Azure DP : Data Product PL : Power Plateform SC : Sécurité MS : Microsoft 365 AI : Inteligence Artificielle MB : Dynamics 365 Les certifications doivent être renouvelées tous les ans, sauf les 900 qui sont acquises définitivement. Choisissez une certification en fonction de vos besoins et noter bien le code (par exemple AZ-900) cela vous servira de mot clé pour trouver des ressources autour de cette certification.\nSe préparer à la certification Il existe une multitude de ressources pour préparer une certification, je vais vous présenter celles qui me paraissent indispensables.\nConsultez le programme de l\u0026rsquo;examen Sur la page dédiée à votre examen, vous trouverez un guide d\u0026rsquo;étude au format PDF. Ce guide en anglais vous donnera en détail les compétences testées avec le pourcentage de question sur ce sujet (une fourchette en général) et vous aurez aussi le détail des compétences testées en dessous. Cela vous permet de comprendre sur quoi portera l\u0026rsquo;examen.\nParcours Microsoft Learn Sur la page dédiée à votre examen, vous trouverez un parcours Microsoft Learn associé à cette certification. N\u0026rsquo;hésitez pas à explorer Microsoft Learn pour trouver d\u0026rsquo;autres contenus sur les thèmes de votre certification.\nYoutube Recherche sur Youtube le code de votre certification, vous trouverez certainement beaucoup de ressource de formations gratuite.\nPour Azure et la sécurité je vous recommande chaudement la chaine de John Savill c\u0026rsquo;est en anglais, mais c\u0026rsquo;est le meilleur.\nS\u0026quot;entrainer à la certification L\u0026rsquo;examen peut faire peur, je vous conseille de regarder sur Internet, vous trouverez, en anglais, des exemples de questions de l\u0026rsquo;examen. Vous pouvez commencer par ExamTopics. Par contre attention certaines réponses sont fausse, regardez les commentaires en cas de doute.\nS\u0026rsquo;inscrire à la certification Obtenir des vouchers Un voucher est un bon gratuit pour passer une certification. Vous pouvez en obtenir lors d\u0026rsquo;évènement organisé par Microsoft.\nLes vouchers pour les certifications 900 sont distribués si vous participez à des training day vous trouverez sur le portail dédié. Lisez le descriptif de l\u0026rsquo;évènement pour savoir s’il est éligible à un voucher. Lors des évènements Build et Ignite, Microsoft organise des challenges consistant à valider des parcours Microsoft Learn. Suite à la participation aux challenges, vous pouvez recevoir un voucher pour certaines certifications (la liste change pour chaque challenge) Procédure d\u0026rsquo;inscription La procédure d\u0026rsquo;inscription est plutôt simple, si vous programmez votre examen suffisamment à l\u0026rsquo;avance, vous aurez un grand choix de dates et heures. Vous pouvez passer une certification à 4h00 le matin si vous voulez 😁.\nPar contre, prévoyez d\u0026rsquo;arriver 30 minutes avant pour la procédure d\u0026rsquo;inscription.\nVous retrouverez les informations de l\u0026rsquo;examen dans le portail Microsoft Learn dans votre profil.\nSe préparer pour le jour de la certification C\u0026rsquo;est le grand jour, attention, il reste quelques pièges à éviter.\nPréparez le lieu de votre examen Si vous passez l\u0026rsquo;examen chez vous ou au bureau, prenez le temps de tout enlever et d\u0026rsquo;avoir une zone de travail dégager. Vous allez devoir photographier votre lieu d\u0026rsquo;examen et moins il est chargé plus c\u0026rsquo;est simple lors de la validation.\nPour faire simple :\nPas de papier qui traine sur le bureau, pas de livres techniques dans les étagères ou de posters techniques. Pas de montre, pas de casque, pas d\u0026rsquo;enceinte, pas de téléphone. Vous devrez être en mono-écran. Personnellement je démonte les écrans de mon bureau pour être tranquille. Une connexion Internet stable, car vous serez filmé durant tout l\u0026rsquo;examen via Internet. Pas de visiteurs pendant l\u0026rsquo;examen. Ne pas parler pendant l\u0026rsquo;examen. La liste n\u0026rsquo;est pas exhaustive 😀\nLes types de questions Lors de votre certification, vous allez avoir différents types de questions dont voici les principaux types :\nLe choix simple : Une question une liste de choix, n\u0026rsquo;hésiter pas à éliminer les réponses aberrantes en cas de doute. Le choix multiple : Une question une liste de choix dans lequel on sélectionne plusieurs valeurs. Le choix multiple ordonné : Une question une liste de choix dans lequel on sélectionne plusieurs valeurs que l\u0026rsquo;on ordonne. L\u0026rsquo;exemple de script : Un exemple de script a complété à l\u0026rsquo;aide de liste déroulante. L\u0026rsquo;étude de cas : On vous présente un cas métier puis un ensemble de questions dessus. Une fois répondu, vous ne pourrez pas revenir dessus. La série sans retour : Une série avec la même question une liste de choix, mais pas possible de revenir en arrière après avoir répondu. Conseils pour l\u0026rsquo;examen Lire l\u0026rsquo;écran au début de l\u0026rsquo;examen Au début de l\u0026rsquo;examen, vous aurez un écran qui vous rappelle le temps de l\u0026rsquo;examen, mais qui vous donne aussi le nombre de questions et le nombre d\u0026rsquo;études de cas pour votre examen. Cela peut changer d\u0026rsquo;une session à une autre. Penser à bien conserver du temps pour les études de cas si elles sont en fin de session. Généralement elles sont soit en premier soit en dernier.\nObjectif 700 Vous aurez votre certification si vous atteignez 700 points sur 1000. Que vous ayez 701 ou 990 le résultat est le même : vous être reçu. Alors, si une question vous pose problème, ne perdez pas trop de temps avec. Vous pouvez la marquer pour revenir plus tard dessus.\nBien lire les questions C\u0026rsquo;est bête, mais pour certaines questions on vous demande de choisir plusieurs réponses, en général le nombre de réponses attendues est indiqué dans le texte de la question alors ne cocher pas une seule valeur si deux sont attendus\u0026hellip;.\nRépondre à toutes les questions Il ne semble pas y avoir de point négatif pour les mauvaises réponses. Il est donc conseillé de répondre a TOUTES les questions?\nUne mauvaise pratique n\u0026rsquo;est pas une mauvaise réponse Certaines questions vous font une proposition et vous demande si cela fonctionne pas si c\u0026rsquo;est la bonne manière de faire. J\u0026rsquo;ai vu pas mal de question qui propose des choses qu\u0026rsquo;il ne faut pas faire, mais qui effectivement fonctionne.\nEn VO c\u0026rsquo;est mieux Si cela ne vous dérange pas, préférez la version anglaise de l\u0026rsquo;examen, vous n\u0026rsquo;aurez ainsi pas de traduction parfois très limite des questions.\nLisez \u0026hellip; éliminez Si vous avez un doute sur les réponses, travaillez par élimination. Pour cela un petit truc, jusqu\u0026rsquo;à présent je n\u0026rsquo;ai pas rencontré de réponses pièges, c\u0026rsquo;est-à-dire qu\u0026rsquo;il n\u0026rsquo;y a pas de réponse avec quelque chose d\u0026rsquo;inventé. Par exemple si vous avez dans les propositions une liste de produits Azure vous n\u0026rsquo;aurez pas un nom de produit qui n\u0026rsquo;existe pas au catalogue. Par simple déduction/intuition on arrive à trouver certaines réponses s alors que l\u0026rsquo;on ne connait pas la réponse, mais on sait éliminer les réponses fausses.\nBonne chance à vous Merci de votre attention.\n","date":"2022-11-20T00:00:00Z","image":"https://blog.ddata.fr/p/guide-pour-votre-certification/chuttersnap-VMKsKFSuEg8-unsplash_hubca494d8a8173a55721e430a50cb167a_924717_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/guide-pour-votre-certification/","title":"Certification : attrapez votre badge !"},{"content":"Vous partagez votre écran en réunion Teams et vous voulez mettre en avant quelque chose sur votre écran ?\nDevenez plus percutant avec ZoomIt.\nInstallation ZoomIt est un outil gratuit disponible sur le site de Microsoft https://learn.microsoft.com/en-us/sysinternals/downloads/zoomit.\nPour installer ZoomIt, téléchargez le fichier Zip sur le page de l\u0026rsquo;outil, décompressez-le dans le dossier où vous voulez installer ZoomIt puis lancer le programme ZoomIt64 dans le dossier.\nAu premier démarrage le produit vous demande d\u0026rsquo;accepter la licence, puis vous pouvez le configurer.\nVous pouvez cocher la case pour que ZoomIt se lance au démarrage. Appuyez sur Ok pour terminer. ZoomIt est maintenant en attente avec les icônes à côté de l\u0026rsquo;heure en bas à droite. Utilisation Voici les principales commandes de ZoomIt que j\u0026rsquo;utilise.\nctrl + 1 : permets de déclencher un zoom, le zoom est réglable avec la molette de la souris. Appuyez sur Echap pour sortir du zoom ou faites un clic gauche pour passer en mode dessin comme avec ctrl + 2. ctrl + 2 : permets de passer en mode dessin. Vous pouvez dessiner à main levée avec la souris en faisant un clic gauche. Vous pouvez dessiner un rectangle avec la souris en faisant un clic gauche et en maintenant la touche ctrl enfoncée. Vous pouvez dessiner une flèche avec la souris en faisant un clic gauche et en maintenant la touche ctrl + maj enfoncée. Vous pouvez écrire un texte en appuyant sur la touche T. Vous pouvez changer la couleur des dessins B : bleu (Blue) P : rose (Pink) Y : jaune (Yellow) R : Rouge (Red) O : orange (Orange) Vous pouvez revenir à votre écran normal en appuyant sur Echap. Entrainez-vous avant une première utilisation en live, cet outil va changer votre manière de présenter. Bien entendu cela ne fonctionne pas seulement avec Teams.\nMerci de votre attention.\n","date":"2022-11-13T00:00:00Z","image":"https://blog.ddata.fr/p/zoomit/nick-fewings-zF_pTLx_Dkg-unsplash_huc4ac62ac1a5ec37e7fe0f9c85c73adbd_1488241_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/zoomit/","title":"ZoomIt : devenez plus percutant en réunion Teams !"},{"content":"La BI self-service est une des promesses de Power BI. Mais de quoi parle-t-on exactement ?\nJe vous propose de vous accompagner dans la découverte de l\u0026rsquo;utilisation la plus courante de la BI self-service avec Power BI : la réutilisation d\u0026rsquo;un jeu de données existant.\nContexte Dans mes articles précédents, je vous ai parlé :\nDe la composition d\u0026rsquo;un projet Power BI dans l\u0026rsquo;article Power BI : leçon d\u0026rsquo;anatomie. De la création d\u0026rsquo;un jeu de données distinct des rapports qui le consomme Jeu de données et rapport : la rupture !. Nous allons maintenant mettre tout cela en œuvre pour faire de la BI self-service.\nUtiliser un jeu de données existant L\u0026rsquo;idée de la BI self-service est de rendre nos utilisateurs autonomes dans leur consommation des données de l\u0026rsquo;entreprise. Nous allons dans cet article non concentré sur l\u0026rsquo;usage le plus commun qui est la réutilisation d\u0026rsquo;un jeu de données existant.\nMais pourquoi réutiliser un jeu de données existant ? Il peut y avoir plein de raisons, mais principalement, pour profiter du travail de préparation, modélisation et validation des données fait par un de vos collègues.\nNous allons voir 3 méthodes distinctes pour réutiliser un jeu de données.\nPour cela je vais utiliser le jeu de données créer lors de la publication du rapport simple comme vue dans l\u0026rsquo;article Jeu de données et rapport : la rupture !.\nAvec un nouveau rapport Power BI directement dans le service Dans cette première méthode, nous allons créer un nouveau rapport depuis le service Power BI et utiliser un jeu de données existant.\nRendez-vous dans votre espace de travail personnel ou dans l\u0026rsquo;espace de travail où vous avez publié le jeu de données.\nPassez la souris sur votre jeu de données et appuyez sur les 3 points. Dans le menu contextuel, appuyez sur Créer un rapport. Vous retrouvez la même expérience de création de rapport que dans Power BI Desktop. Pour enregistrer votre rapport, appuyez sur le bouton Enregistrer. Choisissez un nom pour votre rapport. Sélectionnez un espace de travail dans lequel enregistrer votre rapport, le même que celui du jeu de données pour notre exemple. Appuyez sur Enregistrer. Vous pouvez très bien enregistrer votre rapport dans un espace de travail différent de votre jeu de données. Cela vous ouvre des possibilités d\u0026rsquo;organisation très importante. On appelle cela les jeux de données partagés.\nUne fois le rapport enregistrer, revenez dans votre espace de travail.\nPassez en vue Tracabilité, via le menu Affichage en haut à droite de l\u0026rsquo;espace de travail. Vous pouvez constater que le rapport utilise le jeu de données existant. ⚠ Attention, votre rapport existe uniquement dans le service Power BI. Si vous souhaitez en conserver une copie locale vous devez ouvrir le rapport, puis dans le menu Fichier choisir Télécharger ce fichier.\nAvec Excel Dans cette seconde méthode, nous allons utiliser un jeu de données existant et le consommer directement dans Excel.\nRendez-vous dans votre espace de travail personnel ou dans l\u0026rsquo;espace de travail où vous avez publié le jeu de données.\nPassez la souris sur votre jeu de données et appuyez sur les 3 points. Dans le menu contextuel, appuyez sur Analyser dans Excel. Le service Power BI prépare votre fichier Excel et le mets à disposition dans votre OneDrive. Une fois le processus terminé, le message suivant apparait :\nLe fichier se trouve dans la racine de votre OneDrive et porte le nom du jeu de données. Vous pouvez ouvrir le fichier dans Excel Online ou dans votre Excel local, dans ce cas vous devrez activer les modifications et activer le contenu du fichier.\nVotre fichier est directement connecté à votre jeu de données, les données sont analysables via un tableau croisé dynamique. Vous retrouvez votre modèle de données dans la liste des champs du tableau croisé dynamique. Des éléments du modèle sont déposés dans le tableau croisé dynamique. Les données affichées proviennent du jeu de données du service Power BI. Vous pouvez rafraichir vos données dans Excel via le bouton Actualiser di menu Données. Avec Power BI Desktop Dans cette troisième méthode, nous allons créer un nouveau rapport depuis Power BI Desktop et utiliser un jeu de données existant.\nOuvrez Power BI desktop, nous allons nous connecter à votre jeu de données Power BI.\nDans le menu Accueil appuyez sur le bouton Obtenir les données. Sélectionnez Power Platform. Sélectionnez Jeux de données Power BI. Appuyer sur Se connecter pour passer à l\u0026rsquo;étape suivante. Sélectionnez le jeu de données auquel vous voulez vous connecter. Appuyer sur Se connecter. Vous retrouvez votre modèle de données dans la liste des champs. Dans la barre d\u0026rsquo;information, vous pouvez voir que vous êtes connecté au jeu de données. Si vous publiez le rapport dans le service Power BI vous pourrez constater que seul le rapport est publié et si vous passez en affichage Tracabilité vous constaterez qu\u0026rsquo;il est connecté au jeu de données.\nAméliorer un jeu de données existant Jusqu\u0026rsquo;à présent non avons fait que réutiliser simplement un jeu de données existant. C\u0026rsquo;est bien, mais parfois nous avons besoin d\u0026rsquo;enrichir le jeu de données afin de créer un calcul spécifique à un rapport, mais non disponible dans le jeu de données.\nQue fait exactement un rapport avec un jeu de données ? Pour afficher les données dans un visuel, le rapport crée une requête, dans le langage DAX, puis l\u0026rsquo;envoi au jeu de données qui l\u0026rsquo;exécute et renvoie le résultat afin que ce dernier soit affiché.\nJusqu\u0026rsquo;à présent nous nous sommes limités à la réutilisation des éléments disponible dans le jeu de données. Pour rappel un jeu de données est constitué des éléments suivants :\nNous pouvons aller plus loin et créer de nouvelles mesures dans notre rapport. Nous sommes limités aux mesures, car au final une mesure est juste un morceau de requêtes DAX réutilisable.\nLes autres éléments comme les tables ou colonnes calculées influent directement sur le jeu de données et ne sont donc pas modifiables dans le cas d\u0026rsquo;un rapport consommant un jeu de données.\nNote : Il existe un moyen d\u0026rsquo;enrichir un jeu de données existant avec de nouvelles sources de données, mais ce n\u0026rsquo;est pas le sujet du présent article.\nCréer une mesure dans notre rapport Pour ajouter une mesure dans votre rapport, vous pouvez le faire uniquement avec Power BI Desktop. Ouvrez votre fichier PBIX contenant seulement un rapport.\nDans le menu Modelisation, sélectionnez Nouvelle mesure. Vous pouvez constater que les autres options de création du menu sont désactivées. Créez une nouvelle mesure simple. Vous pouvez l\u0026rsquo;utiliser directement dans votre rapport. Code de la mesure :\n1 # Lignes = COUNTROWS(Ventes) Maintenant, publiez votre rapport dans le service Power BI et rendez-vous dans votre espace de travail.\nVous pouvez constater que seul un rapport est publié. En utilisant la vue de traçabilité, vous voyez qu\u0026rsquo;il est connecté au jeu de données.\nJeu de données lié. Rapport avec une mesure en plus. Conclusion Nous venons de faire un tour rapide de la principale fonctionnalité de la BI self-service, le partage de jeu de données entre plusieurs rapports.\nMerci de votre attention.\n","date":"2022-11-06T00:00:00Z","image":"https://blog.ddata.fr/p/self-service-bi-sur-jeu-de-donnees-existant/anna-kolosyuk-D5nh6mCW52c-unsplash_huc8aff4f4bdb05689828a54a86f8617f1_1552202_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/self-service-bi-sur-jeu-de-donnees-existant/","title":"Voyage au coeur de la BI self-service"},{"content":"Dans cet article je vous présente QuiteRSS, l\u0026rsquo;outil qui me permet de suivre l\u0026rsquo;actualité de la Power Platform.\nContexte Rester informer sur les sujets techniques est une tâche chronophage et répétitive. Les sources d\u0026rsquo;informations sont multiples et mises à jour de manière irrégulière.\nBeaucoup de mes sources d\u0026rsquo;informations préférer sont des blogs ou des chaines Youtube.\nAfin de rester informer des sorties sur ces différentes sources, j\u0026rsquo;utilise QuiteRSS, un outil gratuit, permettant d\u0026rsquo;agréger les différents flux RSS en un seul point.\nLes flux RSS Un flux RSS est un élément d\u0026rsquo;un site qui reprend les dernières publications du site. En général il est mis à jour automatiquement par le site lors de la publication d\u0026rsquo;un nouvel article.\nQuite RSS Si les sites que vous suivez mettent à disposition un flux RSS, vous pouvez l\u0026rsquo;ajouter dans QuiteRSS.\nEnsuite QuiteRSS collecte pour vous l\u0026rsquo;ensemble des flux RSS et vous informe des nouveautés parues. Vous pouvez paramétrer QuiteRSS pour qu\u0026rsquo;il réalise un rafraichissement de données à la fréquence que vous voulez.\nCliquez pour ouvrir le menu. Sélectionnez Outils. Sélectionnez Options\u0026hellip;. Allez dans Feeds. Paramétrez la fréquence de rafraichissement. Appuyez sur OK pour enregistrer. Vous pouvez télécharger le produit sur la page du site https://quiterss.org/en/download. Personnellement j\u0026rsquo;utilise la version portable qui évite une installation.\nMes flux Maintenant que vous avez QuiteRSS, je vous propose de vous partager ma liste de flux, elle est disponible sur mon Github, vous pouvez faire un clic droit sur le lien puis Enregistrer sous.\nUne fois le fichier OPML téléchargé, vous pouvez l\u0026rsquo;ajouter dans QuiteRSS.\nCliquez pour ouvrir le menu. Sélectionnez Importer des flux\u0026hellip;, puis sélectionnez le fichier OPML téléchargé. Tous les flux présents dans le fichier OPML ont été mis à jour Positionnez-vous sur les articles non lus. Appuyez sur Marquer tous les articles lus pour ne voir que les futures parutions dans les articles non lus. N\u0026rsquo;hésitez pas à partager en commentaire les blogs que vous jugerez intéressants à ajouter 😀.\nMerci de votre attention.\n","date":"2022-10-30T00:00:00Z","image":"https://blog.ddata.fr/p/quiterss/sai-abhinivesh-burla-WEv76KgEysk-unsplash_hubedd68143f8f3e6370dbbd8400152b05_498986_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/quiterss/","title":"QuiteRSS : suivez l'actualité de vos blogs préférés."},{"content":"Vous avez terminé votre rapport, il est maintenant temps de le partager. Mais quelle est la bonne solution ?\nContexte Dans Power Bi il existe plusieurs options de partage: Partager le fichier pbix, ou publier le rapport dans le service et partager directement le rapport, le partager via un espace de travail ou une application ? Que choisir ?\nDans cet article nous allons passer en revue ces différents modes de partage pour bien comprendre lequel est le plus adapté en fonction de vos besoins.\nRappelez-vous bien qu\u0026rsquo;il n\u0026rsquo;existe pas une manière unique de partager vos rapports, car il n\u0026rsquo;existe pas un besoin unique de diffusion de nos rapports.\nScénarii de partage Nous partageons un rapport pour répondre à différents besoins, parmi lesquels on retrouve :\nLa co création du rapport avec d\u0026rsquo;autres concepteurs. La validation du rapport avec le demandeur. La diffusion du rapport aux utilisateurs consommant notre rapport. Partager son fichier pbix Partager son rapport en partageant le fichier pbix est la manière de partage la plus simple à comprendre et la plus intuitive pour un utilisateur débutant.\nCette méthode est adaptée pour la co création mais certainement pas pour les autres scénarii de partage.\nAvantages Possibilité de partager le travail de conception du rapport en équipe à condition de ne pas travailler en simultané sur un même fichier. Pas besoin de licence Power BI. Pas de partage sur la cloud. Pas besoin de passerelle pour la mise à jour des données locale. Inconvénients Power BI Descktop doit être installée sur le poste de la personne consultant le rapport pour ouvrir le fichier pbix. Le rafraichissement de données du rapport nécessite un accès aux données. Si des copies du fichier sont réalisées pour partager en simultané à plusieurs utilisateurs cela coûte en stockage et en ressources pour rafraichir les données des N rapports identiques. Publier son rapport dans Power BI Service La bonne solution pour partager vos rapports est une publication dans le service Power BI.\nAppuyez sur le bouton Publier dans le menu Accueil de Power BI Desktop. Choisissez un Espace de travail où publier. Appuyez sur le bouton Sélectionner pour démarrer la publication de votre rapport. Lors de la publication, vous avez 2 types d\u0026rsquo;espace de travail disponible :\nVotre espace de travail personnel. Un espace de travail Pro, PPU ou Premium. Note : Quand je parle d\u0026rsquo;espace de travail sans plus de précision, je parle des espaces de travail V2 dans lesquels vous pouvez collaborer avec d\u0026rsquo;autres utilisateurs.\nPour la suite nous allons uniquement envisager la seconde possibilité. Cette action nécessite une licence Power BI Pro ou Power BI Premium Per User pour pouvoir publier vers un espace de travail.\nPartager directement un rapport Vous pouvez partager un rapport de manière unitaire.\nUtilisez le bouton Partager disponible sur la ligne du rapport dans la liste des objets de l\u0026rsquo;espace de travail. Sélectionnez le mode de partage. Sélectionnez avec qui partager. Sélectionnez le mode d\u0026rsquo;envoi du lien de partage. Vous pouvez partager avec :\nUn utilisateur interne ou invité (externe à votre organisation). Un groupe de sécurité ou de distribution. Avantages Cette méthode et simple et rapide. Fonctionne aussi depuis un espace de travail personnel. Inconvénients Ce mode de partage ne fonctionne qu\u0026rsquo;avec un seul rapport à la fois. Si vous voulez partager plusieurs rapports, vous devez refaire l\u0026rsquo;opération pour chaque rapport et vous générer un lien par rapport. Pas de partage avec un groupe Teams (Microsoft 365). Partager un espace de travail Appuyer sur le bouton Accès pour partager votre espace de travail à d\u0026rsquo;autres utilisateurs. Sélectionnez avec qui partager. Sélectionnez le niveau d\u0026rsquo;autorisation. Validez en appuyant sur Ajouter. Vous pouvez partager avec :\nUn utilisateur interne ou invité (externe à votre organisation). Un groupe de sécurité, de distribution ou une équipe Teams (groupe Microsoft 365). Avantages Partage avec une équipe Teams. Possibilité de collaborer pour la conception des rapports. Partage de tous les objets de l\u0026rsquo;espace de travail en une seule opération. Inconvénients Partage de tous les objets de l\u0026rsquo;espace de travail. Oui comme dans les avantages, mais on ne souhaite pas forcément laisser voir les rapports en cours d\u0026rsquo;élaboration à des utilisateurs finaux. En cas d\u0026rsquo;usage de la sécurité niveau ligne (RLS), seuls les utilisateurs avec l\u0026rsquo;autorisation Visionneuse subissent la RLS. Utiliser une application Power BI L\u0026rsquo;accès à vos rapports se fait via l\u0026rsquo;écran Application et non par l\u0026rsquo;accès à l\u0026rsquo;espace de travail. Pour les détails sur la création d\u0026rsquo;une application Power BI et le partage de celle-ci, je vous invite à lire mon article dédié.\nVous pouvez partager avec :\nUn utilisateur interne ou invité (externe à votre organisation). Un groupe de sécurité, de distribution ou une équipe Teams (groupe Microsoft 365). Avantages C\u0026rsquo;est le bon moyen de diffusion des données aux consommateurs de données. Vous avez le contrôle sur les rapports visibles dans l\u0026rsquo;application. Ce n\u0026rsquo;est pas l\u0026rsquo;ensemble de l\u0026rsquo;espace de travail. Tant que vous ne mettez pas à jour l\u0026rsquo;application, les modifications de rapports survenues dans l\u0026rsquo;espace de travail ne sont pas visibles dans l\u0026rsquo;application. Vous pouvez donc travailler sur une nouvelle version de vos rapports sans impacter vos utilisateurs. Inconvénients Le jeu de données reste commun entre l\u0026rsquo;espace de travail et l\u0026rsquo;application, toute mise à jour de ce dernier doit être faite en conservant la compatibilité pour la version des rapports de l\u0026rsquo;application. Choisir son mode de partage Maintenant que nous avons passé en revue les différents modes de partage, la grande question est : lequel utiliser ?\nVoici un arbre de décision :\nflowchart TD\rA{Quel type de permissions est nécessaire ?}\rA -- Edition --\u003e B[\"Partage de l'espace de travail (Adminitrateur, Membre, Contributeur)\"]\rA -- Lecture seule --\u003e C{Quel type de contenu}\rC -- \"Contenu d'entreprise\" --\u003e D[\"Application\"]\rC -- Contenu collaboratif --\u003e E{Combien d'objets doivent être partagés ?}\rE -- \"Tous ceux de l'espace de travail\" --\u003e F[\"Partage de l'espace de travail (Visioneuse)\"]\rE -- Un seul --\u003e G[\"Partage direct\"]\rSynthèse des partages Ce qu\u0026rsquo;il faut bien comprendre pour choisir :\nUn espace de travail est comme son nom l\u0026rsquo;indique un lieu dans lequel vous allez travailler. Vous allez travailler sur les modèles de données, les rapports et les tableaux de bord. Je le répète c\u0026rsquo;est un lieu de travail pas un lieu de diffusion. L\u0026rsquo;application Power BI est le bon moyen de diffusion de vos informations. Le partage d\u0026rsquo;un rapport de manière unitaire est un moyen simple pour partager rapidement un rapport à une personne. Il n\u0026rsquo;est pas utilisable pour un grand nombre de rapports ou pour adresser une audience vaste. Merci de votre attention\n","date":"2022-10-23T00:00:00Z","image":"https://blog.ddata.fr/p/partager-avec-power-bi/tamanna-rumee-CIfgsywk-_4-unsplash_hu59c2f53d89f061f5e30f7c5261dfb40e_568587_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/partager-avec-power-bi/","title":"Bien partager votre travail"},{"content":"C\u0026rsquo;est la fin d\u0026rsquo;une histoire et le début d\u0026rsquo;une autre, le temps de la séparation de votre jeu de données et son rapport arrive\u0026hellip;\nMais pourquoi faire cela ? Est-ce douloureux ?\nContexte Lorsque l\u0026rsquo;on débute avec Power BI on a tendance à tout faire dans le même fichier, préparation de données, modélisation et data visualisation. Mais dès que l\u0026rsquo;on publie notre rapport dans Power BI Service, on s\u0026rsquo;aperçoit que ce dernier divise notre travail en deux parties :\nUn jeu de données (Dataset) qui contient la préparation de données et la modélisation. Un rapport qui contient la data visualisation. Dès que l\u0026rsquo;on va plus avant dans l\u0026rsquo;utilisation de la BI self service, on se rend compte que l\u0026rsquo;on peut créer plusieurs rapports sur un même jeu de données ou ouvrir un jeu de données directement dans Excel, via l\u0026rsquo;option Analyser dans Excel du service Power BI.\nRapidement on comprend que l\u0026rsquo;on a tout intérêt à créer 2 fichiers pbix de travail, un pour le jeu de donnée et un pour la data visualisation.\nVous avez déjà des rapports avec tout dedans et vous ne souhaitez pas repartir de zéro ? Ne vous inquiétez pas je vais vous montrer comment séparer votre fichier en 2 et cela sans douleur !\nPréparation des fichiers Pour notre démonstration, nous allons utiliser un rapport simple disponible sur mon Github.\nTéléchargez le fichier ou utilisez un de vos fichiers de travail, en prenant la précaution de travailler sur une copie.\nCréation du jeu de données Un jeu de données est simplement un PBIX sans élément de visualisation. Vous serez par contre obligé de conserver au minimum une page de rapport. Je vous conseille donc de mettre à profit cette page pour informer les personnes ouvrant ce fichier que c\u0026rsquo;est un fichier dédié à être un jeu de données. Vous pouvez aussi profiter des pages de rapports pour documenter votre jeu de données ou avoir quelques visualisations dédiées aux contrôles de données.\nOuvrez le fichier dans Power BI Desktop.\nPour créer votre fichier dédié au jeu de données, vous devez simplement supprimer toutes les pages de rapport dédié à vos visualisations.\nAjoutez une nouvelle page avec le bouton + en bas. Renommer cette page, en faisant un clic droit sur l\u0026rsquo;onglet puis Renommer la page. Utilisez le menu Insérer. Ajouter une zone de texte. Ajoutez des informations relatives à votre jeu de données Supprimez les pages de rapports avec l\u0026rsquo;ancienne data visualisation, en faisant un clic droit sur l\u0026rsquo;onglet puis Supprimer la page. Un fois cela fait, enregistrez votre fichier PBIX sous un nouveau nom, par exemple blog.ddata.fr-simple-report-dataset.pbix.\nPour faire cela, allez dans le menu Fichier et sélectionnez Enregistrer sous.\nUne fois votre fichier prêt, vous devez le publier dans le service avant de passer à l\u0026rsquo;étape suivante.\nDans le menu Acceuil. Appuyez sur le bouton Publier. Sélectionnez l\u0026rsquo;espace de travail où vous voulez publier. Appuyez sur Sélectionner pour publier le jeu de données dans le service Poxer BI. Une fois publié, le fichier est séparé en 2 morceaux :\nUn jeu de données Un rapport Pour plus d\u0026rsquo;explications sur le contenu de chaque entité, je détaille cela dans l\u0026rsquo;article Power BI : leçon d\u0026rsquo;anatomie.\nVous pouvez supprimer le rapport si vous le souhaitez. Pour cela, positionnez-vous sur la ligne, appuyez sur les 3 points et choisissez Supprimer.\nCréation de la data visualisation Nous allons repartir de notre fichier PBIX initial, celui qui contient tout, et le transformer en un fichier PBIX qui ne contiendra que la partie rapport (la visualisation).\nPour créer votre fichier dédié au rapport, vous devez simplement supprimer tous les éléments relatifs à l\u0026rsquo;intégration de données :\nLes requêtes Power Query Les objets DAX restants Ouvrez le fichier dans Power BI Desktop.\nNous devons accéder à l\u0026rsquo;éditeur Power Query, allez dans le menu Acceuil. Appuyez sur le bouton Transformer les données. Sélectionnez TOUTES vos requêtes Power Query. Vous pouvez le faire en sélectionnant la première requête de la liste, puis en maintenant la touche majuscule appuyée et sélectionnez la dernière de la liste. Faite un clic droit et sélectionnez Supprimer. Appuyez sur Supprimer dans le message suivant pour confirmer. Appuyez sur le bouton Fermer et appliquer pour enregistrer vos modifications. De retour dans l\u0026rsquo;écran principal de Power BI Desktop, tout est en erreur, pas d\u0026rsquo;inquiétude, c\u0026rsquo;est normal. Ouvrez le panneau Champs. Vérifier que rien ne reste dedans. Si ce n\u0026rsquo;est pas le cas, supprimez tous les éléments restant en faisant un clic droit dessus et en sélectionnant Supprimer du modèle. Il est maintenant temps de reconnecter notre rapport à une source de données. Pour cela nous allons le connecter directement au jeu de données que l\u0026rsquo;on a publié précédemment dans le service Power BI.\nDans le menu Acceuil, appuyez sur le bouton Obtenir les données. Sélectionnez Power Platform. Sélectionnez Jeu de données Power BI. Appuyez sur Se connecter. Un écran apparait pour vous permettre de choisir un jeu de données dans le service Power BI :\nSélectionnez votre jeu de données. Appuyez sur Se connecter. Votre rapport affiche de nouveau vos données. Votre modèle de données est de nouveau disponible dans le panneau Champs. Tout est revenu comme avant, car votre rapport est connecté à un jeu de données compatible avec l\u0026rsquo;ancien jeu de données local. Ce qui est normal puisque c\u0026rsquo;est le même.\nUne fois cela fait, enregistrez votre fichier PBIX sous un nouveau nom, par exemple blog.ddata.fr-simple-report-dataviz.pbix.\nPour faire cela, allez dans le menu Fichier et sélectionnez Enregistrer sous.\nVérification dans le service Vous allez maintenant publier le rapport dans le service Power BI puis vous rendre dans l\u0026rsquo;espace de travail dans lequel vous avez publié.\nVous pouvez constater que seul le rapport est publié cette fois, il n\u0026rsquo;y a pas de jeu de données avec le même nom que le rapport. Allez dans le menu Afficher et sélectionnez Traçabilité. Dans cette vue, on peut voir que l\u0026rsquo;on a 2 rapports sur un jeu de données. Le rapport publié en même temps que le jeu de données si vous ne l\u0026rsquo;avez pas supprimé. Le rapport que l\u0026rsquo;on vient de publier. Allez dans le menu Afficher et sélectionnez Liste pour revenir à la vue classique. Bravo, vous avez réussi la rupture entre votre jeu de données et votre rapport.\nMerci de votre attention.\n","date":"2022-10-16T00:00:00Z","image":"https://blog.ddata.fr/p/jeu-de-donnees-et-rapport-la-rupture/massimo-virgilio-7PfWuqT-As0-unsplash_hu4f849c82d4e14f72bfb032f4fe26f62a_6495568_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/jeu-de-donnees-et-rapport-la-rupture/","title":"Jeu de données et rapport : la rupture !"},{"content":"Quand on débute avec Power BI, on n\u0026rsquo;a pas forcément une vision globale de ce que l\u0026rsquo;on fait. Je vous propose de vous accompagner pour découvrir les entrailles d\u0026rsquo;un projet Power BI.\nContexte Power BI permet comme son nom l\u0026rsquo;indique de faire de la BI.\nLa BI, Business Inteliggence ou informatique décisionnelle en bon français, est la branche de l\u0026rsquo;informatique qui vous permet de faire des analyses et du reporting sur vos données passées.\nQuel que soit l\u0026rsquo;outil, un projet BI se fait toujours de la même manière et Power BI ne déroge pas à cela :\nCollecte et préparation des données sources. Modélisation des données et création des indicateurs. Présentation des données. La collecte de donnée dans Power BI Desktop La collecte des données dans Power BI, se fait à l\u0026rsquo;aide de l\u0026rsquo;outil Power Query. Cet outil permet la préparation des données et réalise la phase ETL (Extract Transform Load) classique des projets BI.\nEn sortie de Power Query, des tables sont créées dans la base de données de Power BI.\nC\u0026rsquo;est aussi le processus Power Query qui permet de rafraichir vos données en répétant l\u0026rsquo;ensemble des transformations que vous avez défini pour la création de chaque table.\nLes données que vous transformez sont toujours des copies des données sources, vous n\u0026rsquo;allez pas endommager vous données source avec Power BI.\nLa modélisation des données et création des indicateurs Une fois vos données disponibles dans les tables, vous allez passer à la phase de modélisation. Pour cela Power BI vous fournit 2 outils :\nUn écran pour définir les relations entre vos tables. Il est fortement conseillé de réaliser une modélisation en étoile, mais nous ne détaillerons pas cela dans le présent article. Un langage, le DAX, vous permettant de requêter vos données, avec lequel vous pourrez créer : Des mesures calculées dynamiquement dans le contexte de filtre. Des colonnes calculées dans le contexte de ligne Des tables calculées. Je ne rentrerais pas non plus dans les détails dans cet article. Si la différence entre mesure et colonne calculées vous intéresse, je vous conseille cette excellente vidéo de DataSeito\nUne fois cette phase terminée vous avez un modèle de données utilisable pour créer vos rapports ou partageable avec vos utilisateurs dans le cadre de la self service BI.\nLa présentation des données Après avoir travaillé dans l\u0026rsquo;arrière-boutique, vous allez maintenant mettre en forme vos données au travers des différents visuels disponibles pour présenter vos données à vos consommateurs de données.\nC\u0026rsquo;est la phase la plus visible de votre travail, votre vitrine.\nVision synthétique de votre travail de Power BI Desktop Toutes les étapes précédentes sont généralement réalisées avec l\u0026rsquo;outil Power BI Desktop et l\u0026rsquo;ensemble de votre travail est conservé sous la forme d\u0026rsquo;un fichier avec l\u0026rsquo;extension PBIX.\nNous pouvons représenter le contenu de votre fichier PBIX sous cette forme simplifiée :\nPublication de notre travail Lorsque l\u0026rsquo;on souhaite partager son travail, on utilise le service Power BI. La publication se fait simplement via l\u0026rsquo;icône Publier du menu de Power BI Desktop. Lors de la publication dans le service, votre travail est divisé en 2 objets distincts :\nUn jeu de données Un rapport Les différents éléments décrits ci-dessus sont répartis ainsi entre ces 2 entités :\nAinsi se termine notre tour dans l\u0026rsquo;anatomie d\u0026rsquo;un projet Power BI.\nMerci de votre attention.\n","date":"2022-10-09T00:00:00Z","image":"https://blog.ddata.fr/p/anatomie-de-power-bi/charlesdeluvio-YJxAy2p_ZJ4-unsplash_hu82c25fa98cd4e9a8103220017be4b8cb_324346_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/anatomie-de-power-bi/","title":"Power BI : leçon d'anatomie"},{"content":"Les applications Power BI sont trop souvent méconnues, vous vous demandez à quoi elles servent ?\nCréons ensemble une application pour mieux comprendre leur usage.\nContexte Dans Power BI il y a plusieurs manières de partager ses rapports avec son audience. Dès que vous souhaitez diffuser des contenus à une large audience, les applications Power BI sont la bonne manière de le faire.\nTrop souvent les utilisateurs ne comprennent pas leur utilité ou ignorent même leur existence.\nAnalogie avec la création d\u0026rsquo;une lettre d\u0026rsquo;information sur votre intranet Imaginons que vous deviez créer une lettre d\u0026rsquo;information diffusée sur votre intranet, nous pouvons simplifier le processus de création de cette lettre d\u0026rsquo;information ainsi :\nL\u0026rsquo;équipe en charge de la rédaction travaille sur un fichier Word dans un dossier partagé, ils ont accès en lecture/écriture au dossier. Les personnes en charge de la relecture et de la validation accèdent à ce dossier en lecture seule. Une fois le document validé, ce dernier est transformé en PDF pour être diffusé sur l\u0026rsquo;intranet. Cet exemple de processus simple est analogue au processus de création de contenu dans le service Power BI :\nLes personnes en charge de la réalisation des rapports collaborent dans un espace de travail avec un des droits suivant : administrateur, membre ou contributeur. Les personnes en charge de la relecture et de la validation accèdent à l\u0026rsquo;espace de travail avec le droit Visioneuse. La diffusion des rapports est réalisée au travers d\u0026rsquo;une application Power BI. Bénéfices des applications C\u0026rsquo;est le bon moyen de diffusion des données aux consommateurs de données. Vous avez le contrôle sur les rapports visibles dans l\u0026rsquo;application. Ce n\u0026rsquo;est pas l\u0026rsquo;ensemble de l\u0026rsquo;espace de travail. Tant que vous ne mettez pas à jour l\u0026rsquo;application, les modifications de rapports survenues dans l\u0026rsquo;espace de travail ne sont pas visibles dans l\u0026rsquo;application. Vous pouvez donc travailler sur une nouvelle version de vos rapports sans impacter vos utilisateurs. Créer une application Power BI Attention, cet article ne prend pas en compte la nouvelle expérience de création des applications actuellement en Public Preview. La nouvelle version est prometteuse, car elle permet la gestion de différentes audiences pour une même application.. Une mise à jour de cet article sera faite quand la version définitive de cette nouvelle fonctionnalité sera disponible.\nIl existe une relation un pour un entre un espace de travail et une application. Une application Power BI est liée à un seul espace de travail et depuis un espace de travail on ne peut créer qu\u0026rsquo;une seule application.\nAllez dans l\u0026rsquo;espace de travail depuis lequel vous souhaitez créer une application :\nSélectionner les rapports et tableaux de bord devant être inclus dans l\u0026rsquo;application. Appuyer sur le bouton Créer une\u0026hellip; pour créer une application. Définir un nom pour votre application, ce nom peut-être différent de celui de l\u0026rsquo;espace de travail. Définir une description pour l\u0026rsquo;application. Personnalisez les différents éléments de votre application. Cliquer sur l\u0026rsquo;onglet Navigation pour poursuivre la configuration Définir l\u0026rsquo;ordre et la visibilité des rapports et des tableaux de bord dans le menu principal de l\u0026rsquo;application. Définir les noms des rapports et tableaux de bord dans le menu principal de l\u0026rsquo;application. Cliquer sur l\u0026rsquo;onglet Autorisations pour finir la configuration Définir qui a accès à cette application. Définir le niveau de partage de l\u0026rsquo;application. Cliquer sur Publier l\u0026rsquo;application pour publier l\u0026rsquo;application. Un écran d\u0026rsquo;avertissement apparait, cliquez sur Publier pour terminer le processus Accéder à votre application soit en cliquant sur le bouton Accéder à l\u0026rsquo;application. Soit par le menu application dans le menu de gauche de Power BI. Soit en copiant le lien dans votre navigateur Internet. Utilisation de l\u0026rsquo;application par vos utilisateurs Vos utilisateurs ont plusieurs moyens d\u0026rsquo;utiliser l\u0026rsquo;application.\nAvant d\u0026rsquo;utiliser une application, elle doit être installée dans le profil de votre utilisateur.\nInstallation des applications Si vous avez coché Installez automatiquement cette application dans le panneau autorisation lors de la création de l\u0026rsquo;application, cette dernière s\u0026rsquo;installe automatiquement pour vos utilisateurs.\nPour installer manuellement une application, vous avez 2 moyens :\nFournir le lien de l\u0026rsquo;application à votre utilisateur, l\u0026rsquo;utilisation de ce lien provoquera l\u0026rsquo;installation automatique de l\u0026rsquo;application. Installer l\u0026rsquo;application depuis le menu Application de Power BI. Sélectionnez le menu Application de Power BI. Appuyez sur le bouton Obtenir des applicati. Allez sur l\u0026rsquo;onglet Applications organisationnelles. Sélectionnez l\u0026rsquo;application et appuyez sur Obtenir maintenant pour installer l\u0026rsquo;application. Quand l\u0026rsquo;application est installée, elle apparait dans le menu Application de Power BI. Mise à jour des applications Pour mettre à jour une application et publier la dernière version des rapports et tableau de bord de l\u0026rsquo;espace de travail :\nAllez dans l\u0026rsquo;espace de travail de l\u0026rsquo;application. Appuyer sur le bouton Mettre à j\u0026hellip; pour mettre à jour l\u0026rsquo;application. Suivre le même processus que celui de la création de l\u0026rsquo;application. Merci de votre attention.\n","date":"2022-10-02T00:00:00Z","image":"https://blog.ddata.fr/p/les-applications-power-bi/volodymyr-hryshchenko-V5vqWC9gyEU-unsplash_huae08634171f8756a098e3310c58d8541_2234549_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/les-applications-power-bi/","title":"Les applications Power BI ?!?  A quoi ça sert ?"},{"content":"Vous avez créé une alerte sur vos données dans Power BI et maintenant vous souhaitez informer un canal Teams du problème ?\nSuivez le guide \u0026hellip;\nPréparons-nous Dans cet article je vais aborder comment utiliser une alerte de données Power BI pour envoyer un message dans Teams. Pour faire cela, nous allons utiliser Power Automate. Je vais donc supposer que vous avez lu les articles suivants :\nPower Automate : Créer votre premier flux. Power BI : Créer une alerte sur vos données. Pour cet article nous allons utiliser la persona Narcisse créatrice Power BI.\nCréation d\u0026rsquo;un flux Power Automate Connectez-vous au site de Power Automate.\nAppuyez sur Créer pour créer votre flux Power Automate. Appuyer sur Flux de cloud automatisé. Saisir un nom pour votre flux, par exemple Alerte Poxer BI - Délai de mise à jour. Pour sélectionner le déclencheur, tapez alerte dans la zone de recherche. Puis choisir Lorsqu\u0026rsquo;une alerte est déclenchée - Power BI. Appuyez sur Créer pour créer votre flux. Dans la boite du déclencheur, sélectionnez votre alerte dans ID de l\u0026rsquo;alerte. Appuyez sur Nouvelle étape pour ajouter l\u0026rsquo;étape suivante. Filtrer la liste des connecteurs en tapant teams. Sélectionnez Microsoft Teams. Sélectionnez Publier le message dans une conversation instantanée ou un canal. Dans Publier en tant que choisir Bot de flux. Il s\u0026rsquo;agit de l\u0026rsquo;identité de l\u0026rsquo;émetteur du message dans le canal Teams. Dans Publier dans choisir Channel. On va écrire dans un canal Teams. Dans Team, sélectionnez l\u0026rsquo;équipe dans laquelle vous voulez publier le message. Dans Channel, sélectionnez le canal dans laquelle vous voulez publier le message. Dans Message vous allez pouvoir écrire le message qui apparaitra dans Teams. Vous pouvez mettre le message en forme avec la barre de mise en forme (Couleur du texte, gras, italique \u0026hellip;). Pour rendre votre texte dynamique, vous pouvez ajouter du contenu dynamique en provenance des étapes précédentes. Il suffit de positionner votre curseur à l\u0026rsquo;endroit où vous souhaitez insérer le contenu. Puis sélectionnez le contenu dynamique dans le menu de droite et de cliquer sur le contenu que vous souhaitez pour l\u0026rsquo;insérer. Enregistrez votre flux. Test du flux Pour déclencher l\u0026rsquo;alerte, suivez la procédure Test de l\u0026rsquo;alerte de l\u0026rsquo;article Power BI : Créer une alerte sur vos données.\nPatientez, une fois l\u0026rsquo;alerte déclenchée, vous aurez ce résultat dans le canal Teams choisi :\nAmélioration du message dans Teams Le résultat est fonctionnel, mais pas élégant, on a un lien non cliquable et très long dans le message Teams.\nPour améliorer cela, nous allons devoir modifier le flux.\nConnectez-vous au site de Power Automate.\nAppuyez sur Mes flux pour accéder à vos flux existants. Cliquez sur votre flux pour l\u0026rsquo;ouvrir. Sur la page de détail du flux, vous pouvez voir :\nLes informations détaillées du flux. L\u0026rsquo;historique des exécutions des 28 derniers jours. Les informations sur les connexions utilisées par votre flux. Cliquez sur le bouton Modifier pour modifier votre flux. Vous ouvrez revenir sur l\u0026rsquo;écran précédent en utilisant la flèche de retour. Appuyez sur l\u0026rsquo;étape pour déplier les propriétés de cette dernière. Nous retrouvons notre étape de message dans Teams tel que nous l\u0026rsquo;avions enregistré. Pour l\u0026rsquo;améliorer, nous allons créer un lien propre inclus dans un texte. Pour créer le lien, nous devons lz créer directement en HTML.\nAppuyez sur l\u0026rsquo;icône \u0026lt;/\u0026gt; pour basculer le message en mode HTML. Supprimez la balise URL de vignette en cliquant sur la croix. Ajoutez le code HTML permettant de créer un lien : Lien Le lien doit être placé entre les 2 doubles quotes après href= Le texte apparaissant à la place du lien sera Lien dans notre cas et peut être modifié Positionnez votre curseur entre les 2 doubles quotes. Puis sélectionnez le contenu dynamique dans le menu de droite. Bonus : Vous pouvez utiliser des émoticônes dans vos messages en ajoutant par exemple : ⚠.\nSous Windows 10 la combinaison de touche windows + ; permets d\u0026rsquo;ouvrir un menu pour insérer des émoticônes.\nSecond test du flux Nous allons maintenant retester notre flux. Au lieu d\u0026rsquo;attendre le déclenchement d\u0026rsquo;une alerte, nous allons utiliser la fonction de test qui permet de relancer une exécution avec les paramètres d\u0026rsquo;une précédente exécution.\nAppuyez sur le bouton Tester pour ouvrir le panneau Tester le flux. Sélectionnez Automatiquement. Puis Avec un déclencheur récemment utilisé. Choisissez une exécution précédente. Appuyez sur le bouton Enregistrer et tester. Si vous aviez déjà enregistré votre flux, le bouton indiquera seulement Tester. Bravo, vous pouvez maintenant automatiser les actions sur vos alertes de données Power BI.\nMerci de votre attention.\n","date":"2022-09-25T00:00:00Z","image":"https://blog.ddata.fr/p/power-bi-data-alert-to-teams/brands-people-ZdqSuxl3Lak-unsplash_hua4282981a2eb475f28d312d25bf8595a_2186526_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-bi-data-alert-to-teams/","title":"💡 Power BI : Envoyer une alerte de données dans Teams."},{"content":"Nous allons voir ensemble, comment créer votre premier flux Power Automate.\nContexte Pour cet article nous allons utiliser la persona Violette créatrice Power Apps. Elle va créer un premier flux Power Automate simple.\nPower Automate les bases Commençons par un peu de théorie, je vais vous expliquer rapidement à quoi sert Power Automate et les concepts utiles pour débuter.\nVous utilisez déjà un grand nombre de services disponible sur Internet comme One Drive, Sharepoint, Outlook mais dès que vous souhaitez faire interagir ces services ensemble cela devient rapidement un challenge insurmontable quand on n\u0026rsquo;est pas informaticien.\nPower Automate a pour vocation de vous assister pour que vous puissiez réaliser des tâches complexes entre les différents services que vous utilisez, et cela sans connaissance informatique complexe.\nDéclencheurs et opérations Chaque service et représenter dans Power Automate sous la forme d\u0026rsquo;un connecteur. Un connecteur contient 2 types d\u0026rsquo;éléments :\nLes déclencheurs : ce sont des évènements permettant de démarrer un flux. Des opérations : Ce sont des éléments interagissant avec le service et utilisables comme étape dans un flux. Chaque flux Power Automate est composé :\nd\u0026rsquo;une première étape de type déclencheur d\u0026rsquo;une succession d\u0026rsquo;étape de type opération Un flux Power Automate décrit donc une succession d\u0026rsquo;étapes qui démarre dans des conditions précises définies par un déclencheur.\nIl existe aujourd\u0026rsquo;hui plus de 450 connecteurs différents.\nLes déclencheurs peuvent être classés dans 3 catégories distinctes :\nAutomatique : Pour les déclencheurs démarrés par un évènement dans un service (réception d\u0026rsquo;un mail, création d\u0026rsquo;un fichier, etc\u0026hellip;) Manuel : Pour les déclencheurs démarrés via une action de votre part. Planifié : Pour les déclencheurs dont vous programmez l\u0026rsquo;exécution Votre premier flux Création Pour notre premier flux, je vous propose de créer un flux qui interagit avec votre One Drive et vous permet de transformer un fichier Word en fichier PDF et de l\u0026rsquo;envoyer à une adresse email demandée lors de l\u0026rsquo;exécution du flux.\nUn flux Power Automate ce créer via un site web dédié. Pour créer un nouveau flux, connectez-vous au site de Power Automate.\nAppuyez sur Créer pour créer votre premier flux Power Automate. Appuyer sur Flux de cloud instantané. Saisir un nom pour votre flux, par exemple Convertir en PDF et envoyer par email. Sélectionnez le déclencheur, nous allons choisir Pour un fichier sélectionné - OneDrive Entreprise. Appuyez sur Créer pour créer votre flux. Bienvenue dans l\u0026rsquo;écran de création de votre flux Power Automate. Ici se trouve votre déclencheur que vous pouvez personnaliser. Vous pouvez modifier le nom de votre flux ici. Vous pouvez enregistrer vos modifications ici? L\u0026rsquo;enregistrement ce fais dans le cloud pas sur votre ordinateur. Vous pouvez annuler ou rétablir une action. Ce bouton permet d\u0026rsquo;ajouter des étapes à votre flux. Dans la boite du déclencheur, appuyez sur Ajouter une entrée. Sélectionnez Messagerie. Dans la première zone de texte, vous pouvez définir le nom de la variable, réutilisable dans les étapes suivantes du flux. Dans la seconde zone de texte, vous pouvez définir la question qui sera posée à l\u0026rsquo;utilisateur du flux lors de l\u0026rsquo;exécution. Appuyez sur Nouvelle étape pour ajouter l\u0026rsquo;étape suivante. Filtrer la liste des connecteurs en tapant onedrive. Sélectionnez OneDrive for Business. Sélectionnez Convertir le fichier à l\u0026rsquo;aide d\u0026rsquo;un chemin d\u0026rsquo;accès. Placez votre curseur dans la zone Chemin d\u0026rsquo;accès au fichier et le menu dynamique de droite apparait. Sélectionnez filePath dans le menu de droite, la valeur s\u0026rsquo;ajoute dans la zone (1). Filtrer la liste des connecteurs en tapant outlook. Sélectionnez Office 365 Outlook. Sélectionnez Envoyer un e-mail (V2), le numéro de version peut avoir changé depuis la rédaction de cet article. Le connecteur s\u0026rsquo;initialise automatiquement pour utiliser votre boite e-mail Office 365 Outlook pour se connecter.\nPlacez votre curseur dans la zone À, cette fois le menu dynamique de droite n\u0026rsquo;apparait pas. Cliquez sur Ajouter du contenu dynamique pour ouvrir le menu dynamique de droite. Dans ce menu vous trouverez toutes les variables de sortie disponible pour toutes les étapes précédentes. Positionnez-vous sur la sortie d\u0026rsquo;étape Pour un fichier sélectionné, qui est le nom de la première étape. Sélectionnez Messagerie qui correspond au nom de l\u0026rsquo;entrée que l\u0026rsquo;on a créé au niveau du déclencheur. Cette variable contiendra l\u0026rsquo;adresse e-mail qui sera demandée lors du déclenchement du flux et sera utilisée dans la zone destinataire de notre e-mail. La variable Messagerie est maintenant utilisée dans le champ À de l\u0026rsquo;e-mail, on reconnait une valeur dynamique dans Power Automate car elle est dans un rectangle coloré avec une icône. Tapez un texte pour l\u0026rsquo;objet de l\u0026rsquo;e-mail. Tapez un texte pour le corps de l\u0026rsquo;e-mail. Appuyez sur Afficher les options avancées. Placez votre curseur dans la zone Pièces jointes Nom - 1 et le menu dynamique de droite apparait. Positionnez-vous sur la sortie d\u0026rsquo;étape Convertir le fichier à l\u0026rsquo;aide d\u0026rsquo;un chemin d\u0026rsquo;accès. Sélectionnez Nom de fichier qui correspond au nom du fichier après l\u0026rsquo;opération de conversion du fichier. Placez votre curseur dans la zone Pièces jointes Contenu - 1 et le menu dynamique de droite apparait. Sélectionnez Contenu du fichier qui correspond au contenu du fichier après l\u0026rsquo;opération de conversion du fichier. Enregistrez votre flux via l\u0026rsquo;icône disponible dans le bandeau en haut à droite ou via le bouton enregistrer disponible en bas des étapes du flux.\nTest du flux Rendez-vous dans l\u0026rsquo;interface web de votre OneDrive.\nDisponible via le menu gaufre (9 points en 3x3) en haut à gauche. Sélectionnez OneDrive. Positionnez-vous sur un fichier Word existant ou créez en un pour le test.\nSur la ligne du fichier Word ouvrir le menu via les 3 points verticaux. Sélectionnez Automatiser. Cliquez sur Convertir en PDF et envoyez par email qui correspond au nom du flux que l\u0026rsquo;on a créé, pour exécuter le flux pour le fichier choisi. La fenêtre d\u0026rsquo;exécution du flux apparait et vérifie les autorisations des connecteurs. Cet écran n\u0026rsquo;apparait que lors de la première exécution. Appuyez sur Continuer pour continuer. Le flux vous demande de saisir une adresse e-mail. Vous pouvez utiliser votre adresse pour le test. Appuyez sur Exécuter le flux pour exécuter le flux. Appuyez sur Terminer. Vérifier la boite aux lettres que vous avez indiquée lors de l\u0026rsquo;exécution du flux, vous devez avoir reçu un e-mail avec le fichier PDF en pièce jointe.\nVoilà, vous venez de créer votre premier flux avec Power Automate et vous savez maintenant ajouter des commandes personnalisées dans votre OneDrive.\nMerci de votre attention.\n","date":"2022-09-18T00:00:00Z","image":"https://blog.ddata.fr/p/powerautomate-first-flow/solen-feyissa-IfWFKG3FXE4-unsplash_hu1ee85c87f0bb699a44cee539371079be_1568937_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/powerautomate-first-flow/","title":"Power Automate : Créer votre premier flux."},{"content":"Vous souhaitez créer une alerte sur vos données dans Power BI ?\nSuivez le guide \u0026hellip;\nPréparons-nous Pour créer une alerte sur vos données dans Power BI vous devez le faire dans un tableau de bord. Si ce n\u0026rsquo;est pas déjà fait créer un tableau de bord tel que je le décris dans ce précédent article.\nCréation d\u0026rsquo;une alerte Un des seuls types de visuel qui permet de créer des alertes est la carte de données. Les autres visuels que vous pouvez utiliser sont la jauge et l\u0026rsquo;indicateur de perfomance clé (KPI).\nOuvrez le rapport blog.ddata.fr-simple-report depuis votre espace de travail personnel, comme vu dans cet article.\nSélectionnez la page Information. Cliquez sur le visuel Durée depuis dernier rafraichissement pour faire apparaitre un menu contextuel. Cliquez sur l\u0026rsquo;icône Épingler un élément visuel pour ajouter le visuel. Sélectionnez l\u0026rsquo;action Tableau de bord existant. Choisissez le tableau de bord existant. Appuyez sur Épingler. Appuyez sur Accéder au tableau de bord. Voici donc votre tableau de bord :\nSur la dalle du visuel carte de données, appuyez sur les 3 points en haut à droite pour ouvrir un menu. Sélectionnez Gérer les alertes. Appuyez sur Ajouter une règle d’alerte pour créer une règle. Définir un titre. Définir un seuil, au-dessus de 60 pour notre test, déclenchera l\u0026rsquo;alerte si le jeu de données n\u0026rsquo;est pas mis à jour après plus de 60 minutes. Choisir une fréquence de test de la règle d\u0026rsquo;alerte, toutes les heures pour notre exemple. Vérifier que l\u0026rsquo;envoi de mail est coché. Appuyez sur Enregistrer et fermer. Vous avez créé votre première alerte de données.\nTest de l\u0026rsquo;alerte Pour tester notre alerte, nous allons devoir réinitialiser le délai de rafraichissement de notre jeu de données.\nRendez-vous dans votre espace de travail personnel\nAppuyez sur l\u0026rsquo;icône pour rafraichir le jeu de données. Patientez, vous allez recevoir une alerte par email dans un délai de 2 heures (une heure pour la règle et 60 minutes pour la valeur de l\u0026rsquo;indicateur).\nEn plus de l\u0026rsquo;alerte par email, vous pourrez voir l\u0026rsquo;alerte dans le portail du service Power BI.\nCliquez sur la cloche en haut à droite pour lire les notifications.\nSi la cloche n\u0026rsquo;est pas visible, cliquez sur les 3 points puis choisissez Notifications.\nLe centre de notification apparait à droite.\nNous avons terminé la création de notre première alerte.\nMerci de votre attention.\n","date":"2022-09-11T00:00:00Z","image":"https://blog.ddata.fr/p/power-bi-create-data-alert/mike-meyers--haAxbjiHds-unsplash_hu82c25fa98cd4e9a8103220017be4b8cb_443365_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-bi-create-data-alert/","title":"💡 Power BI : Créer une alerte sur vos données."},{"content":"Vous souhaitez créer un tableau de bord dans Power BI ?\nSuivez le guide \u0026hellip;\nPréparons-nous Je vous propose pour cet article d\u0026rsquo;utiliser un exemple de fichier Power BI disponible sur mon GitHub.\nDans cet exemple vous trouverez un simple jeu de données générer aléatoirement via Power Query, vous aurez donc à chaque rafraichissement des résultats différentss.\nCe jeu de données à l\u0026rsquo;avantage de n’être connecté à aucune source externe et il peut donc être utilisé simplement.\nUne fois le fichier Power BI téléchargé, vous pouvez le publier dans votre espace de travail personnel dans le service Power BI. L\u0026rsquo;utilisation de l\u0026rsquo;espace de travail personnel est recommandée pour faire des tests ou apprendre à utiliser le service, nous l\u0026rsquo;utiliserons donc dans cet article.\nOuvrez le fichier dans Power BI Desktop.\nCliquez sur l\u0026rsquo;icône Publier dans le menu Accueil pour publier le fichier. Choisissez Mon espace de travail comme destination. Appuyez sur le bouton Sélectionner pour publier le fichier dans le service Power BI. Une fois le fichier publier dans Power BI service, ouvrez votre espace de travail personnel.\nCréation d\u0026rsquo;un tableau de bord Un peu de théorie Un tableau de bord permet d\u0026rsquo;afficher des visuels en provenance de différents rapports sur une page écran.\nLes principales différences entre le rapport et le tableau de bord sont :\nUn tableau de bord est mono page alors qu\u0026rsquo;un rapport est multi page. Un tableau de bord peut afficher des visuels de plusieurs rapports différents et donc afficher des données en provenance de plusieurs jeux de données Power BI. Un rapport est lié à un seul jeu de données. Un tableau de bord ne peut pas être filtré et les visuels ajoutés à un tableau de bord le sont dans leur contexte de filtre. Passons à la pratique Nous allons maintenant créer un tableau de bord, pour cela ouvrez le rapport blog.ddata.fr-simple-report depuis votre espace de travail personnel.\nVous devez avoir un rapport simple comme celui-ci :\nCliquez dans le visuel que vous souhaitez épingler dans votre tableau de bord pour faire apparaitre un menu contextuel (au-dessus ou en dessous du visuel généralement). Cliquez sur l\u0026rsquo;icône Épingler un élément visuel pour ajouter le visuel. Sélectionnez l\u0026rsquo;action à faire Tableau de bord existant ou Nouveau tableau de bord, vous avez le choix si il y a déjà un tableau de bord dans l\u0026rsquo;espace de travail. Donnez un nom à votre tableau de bord ou sélectionnez un existant. Appuyez sur Épingler. Appuyez sur Accéder au tableau de bord. Voici donc votre tableau de bord :\nChaque visuel sur le tableau de bord s\u0026rsquo;appelle une dalle. Vous pouvez déplacer une dalle en cliquant dessus, garder le bouton de la sourie appuyée, et en la déplaçant. Si vous cliquez sur la dalle, le rapport d\u0026rsquo;origine du visuel sera ouvert à la page du visuel. Vous pouvez redimensionner une dalle dans le coin en bas à droite. Vous pouvez passer le tableau de bord en plein écran. Vous pouvez le mettre dans vos favoris (ceux du service Power BI pas ceux du navigateur). Bravo vous avez créer votre premier tableau de bord dans le service Power BI. Ces derniers ne sont disponibles que dans le service, pas dans Power BI desktop.\nMerci de votre attention.\n","date":"2022-09-04T00:00:00Z","image":"https://blog.ddata.fr/p/power-bi-create-dashboard/adi-goldstein-Ug1-eXogOS0-unsplash_huf938efc7b2a69af6a2c521a2a8f95bf7_3564155_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-bi-create-dashboard/","title":"💡 Power BI : Créer un tableau de bord Power BI."},{"content":"Vous avez besoin d\u0026rsquo;afficher la date de la dernière mise à jour de votre jeu de données Power BI ?\nSuivez le guide \u0026hellip;\nStocker la date de dernière mise à jour Pour calculer la date de dernière mise à jour de votre jeu de données Power BI, cela se passe au niveau de Power Query.\nOuvrez votre fichier dans Power BI desktop.\nSélectionnez le menu Accueil. Appuyez sur l\u0026rsquo;icône Transformer les données. L\u0026rsquo;éditeur Power Query s\u0026rsquo;ouvre, nous allons créer une requête vide.\nSélectionnez le menu Accueil. Appuyez sur le texte Nouvelle source. Choisissez Requête vide. Dans la barre de formule, entrez le code M ci-dessous. Appuyez sur la coche pour valider. Vérifier le résultat. 1 = #table(type table[Last refresh date=datetimezone], {{DateTimeZone.UtcNow()}}) Si la barre de formule ne s\u0026rsquo;affiche pas :\nSélectionnez le menu Affchage. Cochez Barre de formule. Renommer la requête nouvellement créée en Mise à jour.\nNoter que la formule M donne l\u0026rsquo;heure en UTC et non en heure local de l\u0026rsquo;utilisateur. La raison est simple, si vous prenez l\u0026rsquo;heure local vous aurez votre heure local dans Power BI Desktop mais l\u0026rsquo;heure dans le service sera en UTC. Nous travaillons donc directement en UTC pour ne pas avoir des comportements différents entre Power BI Desktop et Power BI service.\nEnfin appuyer sur Fermer \u0026amp; appliquer dans le menu Accueil pour fermer l\u0026rsquo;éditeur Power Query et charger la nouvelle requête dans le modèle Power BI.\nAfficher la date de dernière mise à jour Maintenant nous allons créer une mesure pour afficher la date de dernière mise à jour en heure local à l\u0026rsquo;utilisateur.\nSélectionnez le menu Modélisation. Appuyez sur l\u0026rsquo;icône Nouvelle mesure. La formule de notre mesure sera :\n1 Date de dernière mise à jour = MAX(\u0026#39;Mise à jour\u0026#39;[Last refresh date]) + (2 / 24) Nous ajoutons 2 heures pour passer en heure d\u0026rsquo;été local de Paris. Nous ne traiterons pas les problèmes d\u0026rsquo;heure d\u0026rsquo;été / heure d\u0026rsquo;hiver dans le cadre de cet article.\nMaintenant il nous suffit de poser la mesure Date de dernière mise à jour dans un visuel pour afficher cette date.\nSi on souhaite afficher le temps passé depuis la dernière mise à jour, nous allons créer une mesure dans notre modèle Power BI.\nLa formule de notre mesure sera :\n1 Durée depuis dernier rafraichissement = DATEDIFF(MAX(\u0026#39;Mise à jour\u0026#39;[Last refresh date]), UTCNOW(),MINUTE) Pour afficher un texte propre à nos utilisateurs, on peut créer une autre mesure DAX avec la formule suivante :\n1 2 3 4 5 6 Texte - Durée depuis dernier rafraichissement = var vMinutes=[Durée depuis dernier rafraichissement] var vHeures=int(vMinutes/60) var vMinutesRestantes=MOD(vMinutes, 60) return If(vHeures \u0026gt; 0, vHeures \u0026amp; \u0026#34; Heures \u0026#34;) \u0026amp; vMinutesRestantes \u0026amp; \u0026#34; Minutes\u0026#34; Formule inspirée de cet article : Calculate Duration in Days Hours Minutes and Seconds Dynamically in Power BI using DAX\nVoici le rendu des trois mesures dans un rapport Power BI :\nVous pouvez trouver un fichier Power BI d\u0026rsquo;exemple sur mon GitHub.\nMerci de votre attention.\n","date":"2022-08-30T00:00:00Z","image":"https://blog.ddata.fr/p/afficher-date-derniere-mise-a-jour/laura-chouette-t6hNUc8vspA-unsplash_hu9a362b859678502b1d25b6ed7a3ae50f_1047477_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/afficher-date-derniere-mise-a-jour/","title":"💡 Power BI : Ajouter la date de la dernière mise à jour de votre jeu de données. 📅"},{"content":"Nous allons voir ensemble, comment obtenir gratuitement un environnement complet Power Apps.\nContexte Pour cet article nous allons utiliser la persona Violette créatrice Power Apps.\nViolette à déjà créer quelques applications Power Apps et quelques flux Power Automate avec sa licence Office 365, en effet dans la plupart des licences Office 365 sont inclues les licences suivantes :\nPower Apps for Office 365 Power Automate for Office 365 C\u0026rsquo;est très bien pour débuter, mais si on souhaite utiliser des connecteurs premium (ceux avec le petit diamant après le nom) on a besoin d\u0026rsquo;une licence payante.\nLa licence payante est indispensable si on souhaite utiliser ses applications en production, mais si l\u0026rsquo;on souhaite juste développer, tester et/ou se former il existe une solution : Power Apps Developer Plan.\nNous allons dans cet article inscrire Violette au plan Power Apps Developer.\nQue contient le plan Power Apps Developer ? Avec le plan Power Apps Developer sont inclues :\nUn accès aux environnements de développement Power Apps et Power Automate. Un environnement développeur dédié. un accès aux plus de 600 connecteurs Power Apps et Power Automate. une plate-forme de données Dataverse dédiée limitée à 2 Go de données. Vous trouverez plus de détails sur la page dédié.\nLes connecteurs premium de Power Automate seront utilisables par les flux déclenchés par des applications Power Apps.\nS\u0026rsquo;inscrire au plan Power Apps Developer Pour vous inscrire au plan Power Apps Developer vous devez vous rendre sur le site dédié au plan.\nVous devez vous connecter avec une adresse e-mail professionnelle ou scolaire associée à Azure Active Directory.\nAppuyez sur Get started free pour commencer le processus d\u0026rsquo;inscription. Renseignez l\u0026rsquo;email associé à l\u0026rsquo;inscription. Appuyer sur Suivant pour continuer. Appuyer sur Se connecter pour vous identifier. Renseignez le pays associé à l\u0026rsquo;inscription. Renseignez le numéro de mobile associé à l\u0026rsquo;inscription. Cochez les éléments qui vous intéressent. Appuyer sur Prise en main pour continuer. Appuyer sur Prise en main pour terminer et être rediriger vers l\u0026rsquo;environnement de développement Power Apps. Découvrir le plan Power Apps Developer Nous allons commencer par vérifier que le plan est actif.\nCliquez sur la roue crantée pour ouvrir le panneau de paramètres. Cliquez sur Plan(s) pour voir la liste des plans affectés à l\u0026rsquo;utilisateur. La licence Power Apps Community correspond au plan Power Apps Developer.\nNous allons maintenant vérifier que l\u0026rsquo;environement développeur dédié est présent.\nCliquez sur l\u0026rsquo;environnement actif pour ouvrir le panneau des environnements. L\u0026rsquo;environnement Developer Environement correspond à votre environement développeur dédié. Vous pouvez cliquer dessus pour passer de l\u0026rsquo;environement par défaut à votre environnement dédié. Pour profiter des avantages de votre plan Power Apps Developer, vous devrez travailler dans votre environement développeur dédié.\nLe plan ce terminant par (default) est le plan par défaut de votre organisation.\nNous avons terminé avec la création du plan Power Apps Developer, il ne vous reste plus qu\u0026rsquo;à jouer avec.\nVoici les liens pour accéder rapidement aux outils de développement, penser à modifier l\u0026rsquo;environnement de travail avant de commencer vos créations.\nPower Apps Power Automate Portail d\u0026rsquo;administration Power Platform ","date":"2022-08-26T00:00:00Z","image":"https://blog.ddata.fr/p/power-apps-developer-plan/dele-oke-MO_oMhIGDHU-unsplash_hua214aac1e1e8841ef50abd9806eb882a_4667015_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/power-apps-developer-plan/","title":"Power Apps Developer Plan"},{"content":"Vous en avez assez d\u0026rsquo;avoir des adresses de tests en onmicrosoft.com ?\nJe vous montre comment obtenir gratuitement un nom de domaine et le configurer dans votre environnement de test.\nContext Par défaut vous avez un nom de domaine en onmicrosoft.com sur votre environnement de test. Vous pouvez bien entendu configurer votre prope nom de domaine acheter chez un fournisseur comme Gandi. Cela à un coût que l\u0026rsquo;on ne veut pas forcement déboursser pour faire des tests.\nIl existe une solution pour obtenir un nom de domaine gratuitement pendant un an : Freenom. Attention après un an pour renouveller ce nom vous devrez payer ou changer de nom de domaine pour conserver la gratuité.\nObtenir votre nom de domaine gratuit Rendez vous sur le site Freenom\nSaisir un nom de domaine. Appuyez sur Vérifier la disponibilité pour commencer. Vérifiez que le nom de domaine est disponible gratuitement. Commandez votre nom de domaine disponbile en appuyant sur Commander!. Si le nom n\u0026rsquo;est pas disponible saisir un nouveau nom de domaine. Appuyez sur Vérifier la disponibilité pour faire une nouvelle recherche. Si lors de l\u0026rsquo;appuie sur Commander!, vous avez le message Not available.\nSaisir le nom de domaine avec l\u0026rsquo;extention dans la zone de recherche. Le nom de domaine est automatiquement ajouté au panier, appuyer sur Checkout pour valider votre panier. Sélectionnez la durée de votre nom de domaine (maximum 12 mois en version gratuite) Appuyez sur Continuer. Enregistrez vous avec votre email. Ou un compte de média social. Suivre ensuite les étapes d\u0026rsquo;enregistrement jusqu\u0026rsquo;à la validation de la commande. Configurer votre environnement de test pour utiliser votre nom de domaine Nous allons maintenant associé le nom de domaine à notre environnement de test.\nPour commencer connectez vous au centre d\u0026rsquo;administration Microsoft 365.\nDans le paneau de gauche, choisissez Configuration. Puis sélectionnez Configurer votre domaine personnalisé. Appuyez sur Prise en main pour démarrer le processus. Entrez votre nom de domaine créer précédement. Appuyez sur le bouton Utiliser ce domaine. Nous devons maintenant prouver que nous sommes le propriétaire du nom de domaine, pour cela nous allons devoir ajouter un enregistrement dans notre nom de doamine pour démontrer que l\u0026rsquo;on a accès à l\u0026rsquo;administration du nom de domaine.\nChoisissez Ajouter un enregistrement TXT aux enregistrements DNS du domaine. Appuyez sur le bouton Continuer. Notez le code que vous devrez saisir dans l\u0026rsquo;interface d\u0026rsquo;administration de Freenom. Conservez cette page ouverte nous allons y revenir plus tard.\nRendez vous maintenant sur l\u0026rsquo;interface d\u0026rsquo;administration de Freenom en cliquant sur Se connecter en haut à droite de la page d\u0026rsquo;accueil de Freenom.\nPour accéder à interface d\u0026rsquo;administration de Freenom, sélection My Domains dans le menu Services en haut à droite. Sur la ligne du domaine que vous voulez associé à l\u0026rsquo;environnement detest, appuyez sur Manage Domain. Choisissez Manage Freenom DNS. Ne rien saisir dans la zone Name. Choisir TXT dans la zone Type. Saisir le code fournit dans l\u0026rsquo;interface d\u0026rsquo;administration de Microsoft 365 dans la zone Target. Appuyez sur Save Changes. Nous allons devoir attendre un peut que la modification ce réplique sur les serveurs de Freenom, généralement entre 5 et 15 minutes.\nNous retournons ensuite sur l\u0026rsquo;écran de vérification du domaine dans l\u0026rsquo;interface d\u0026rsquo;adminitration de Microsoft 365 et appuyer sur le bouton Vérifier.\nSi l\u0026rsquo;enregistrement est trouvé vous obtenez le résultat suivant, sinon patientez et appuyez sur le bouton Réessayer.\nUne fois le domaine validé, nous pouvons configurer des services comme l\u0026rsquo;email pour utiliser ce domaine. Appuyez sur Continuer pour débuter cette configuration. Nous devons de nouveau ajouter des enregistrement DNS dans l\u0026rsquo;interface d\u0026rsquo;administration de Freenom pour activé ces services. Vous trouverez les informations pour 3 nouveaux enregistrements plus bas dans la page, lorsque vous avez un arobase (@) dans le nom d\u0026rsquo;hôte ne rien mettre dans la zone Name de freenom, sinon mettre la valeur du nom d\u0026rsquo;hôte dans la zone Name de freenom. Une fois que vous avez ajouter les enregistrements dans Freenom et patientez, appuyez sur le bouton Continuer. Appuyez sur le bouton Terminé. Bravo vous avez terminé d\u0026rsquo;associer votre nom de domaine à votre environnement de test.\nMaintenant vous pouvez créer des comptes utilisateurs pour votre nouveau nom de domaine.\nAttention seule la première période et gratuite si vous souhaitez conserver le même nom de domaine après la première période vous devrez payer. Sinon vous pourrez créer un nouveau nom de domaine mais vous devrez recréer de nouveau compte pour ce nouveau domaine. Cela n\u0026rsquo;est pas forcément contaiyant pour un environnement de test.\nMerci de votre attention.\n","date":"2022-08-22T00:00:00Z","image":"https://blog.ddata.fr/p/nom-de-domaine-gratuit/amy-shamblen-BV8Ka-JE0Cs-unsplash_hu7cef2e4bd3986451f53f02d4ab92b461_378496_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/nom-de-domaine-gratuit/","title":"💡 Votre nom de domaine gratuit !"},{"content":"Lorsque vous devez tester les services Microsoft 365, vous devez le faire avec plusieurs identités. Pour éviter la schizophrénie, créer des personas et donner leurs vies dans votre navigateur.\nContext Vous avez votre propre espace de test Microsoft 365 et vous souhaitez vous mettre en situation pour tester un service (Power BI, Power Apps, Teams\u0026hellip;), vous allez rapidement avoir besoin de le faire avec plusieurs identités différentes.\nDans cet article je vais vous expliquer comment créer ces nouvelles identités dans votre environnement 365 et comment les utiliser facilement.\nPré requis Vous devez avoir accès à un environnement avec des droits d\u0026rsquo;administrateur Azure Active directory.\nSi vous n\u0026rsquo;avez pas encore votre environnement de test je vous invite à suivre mon article consacré à la création de ce dernier.\nCréer un persona Un persona est une personne fictive qui sert en général pour définir des stratégies marketing. Je vous conseille de créer différents utilisateurs basés sur différentes personnas afin de donner vie à vos différents scénarii et les mettre en scène en conditions réelles.\nPour créer un personna réaliste vous aurez notamment besoin de lui donnée vie au travers photo. Vous pouvez utilisez le site site thispersondoesnotexist pour obtenir des photos de personnes n\u0026rsquo;existant pas et donc de personnifier vos personas sans risque de nuire à autrui.\nPour faire facilement une image en cercle j\u0026rsquo;utilise ce site.\nMes personas Vous trouverez ici les personnas que j\u0026rsquo;utilise, sur chaque fiche vous trouverez une description de la personna :\nAdministrateur Azure Administrateur Power BI Créateur Power BI Manager, utilisateur de Power BI Colaborateur, utilisateur de Power BI Créateur Power PlatForm Création d\u0026rsquo;un profil utilisateur dans son navigateur J\u0026rsquo;utilise Google Chrome pour les exemples, mais vous pouvez vous réaliser ces opérations dans tous les navigateurs modernes.\nL\u0026rsquo;utilisation des profils dans votre navigateur permet d\u0026rsquo;avoir une étanchéité entre les différents profils utilisateurs que vous utilisez. Vous pourrez notamment avoir vos favoris, cookies et mots de passe enregistrés de manière isolés pour chaque profil.\nJe vais créer un profil pour mon utilisateur administrateur, celui créer lors de notre inscription au programme Microsoft 365 Developer.\nPour ajouter un profil utilisateur de Google Chrome, vous devez :\nCliquer sur le cercle représentant le profil en cours. Sélectionner Ajouter. Sélectionner Continuer sans compte pour poursuivre la création. Choisissez un avatar pour le profil. Saisir un nom pour le profile, l\u0026rsquo;email de la personna me semble une bonne option. Choisissez une couleur de thème pour le profil. Ce choix est pour moi important, car il me permet d\u0026rsquo;identifier en un coup d\u0026rsquo;oeil le profil quand plusieurs sessions sont ouvertes en parallèle. En général, j\u0026rsquo;utilise : Le bleu pour l\u0026rsquo;administrateur Azure Le jaune pour l\u0026rsquo;administrateur Power BI Appuyer sur le bouton OK pour terminer la création du profil. Votre navigateur s\u0026rsquo;ouvre avec son nouveau thème :\nVous pouvez passer d\u0026rsquo;un profil à l\u0026rsquo;autre à l\u0026rsquo;aide du cercle représentant les profils. Reproduisaient ces opérations pour créer un profil pour chacune des personnas que vous utilisez.\nCréation d\u0026rsquo;un nouvel utilisateur Pour créer un nouvel utilisateur, nous allons commencer par nous connecter au portail d\u0026rsquo;administration Microsoft 365 avec le profil de notre utilisateur administrateur.\nLors de la première connexion, utilisez le lien Ignorer pour l’instant si vous souhaitez ignorer l\u0026rsquo;authentification multi-facteur (MFA). Utilisez le bouton Suivant pour paramétrer l\u0026rsquo;authentification multi-facteur (MFA). Une fois la procédure d\u0026rsquo;authentification terminée vous arrivez sur la page d\u0026rsquo;accueil du portail d\u0026rsquo;administration Microsoft 365.\nDans le panneau de gauche, ouvrez le menu Utilisateurs Appuyez sur Utilisateurs actifs pour ouvrir l\u0026rsquo;écran de gestion des utilisateurs Appuyez sur Ajouter un utilisateur pour créer un nouvel utilisateur. Un assistant apparait dans le panneau de droite. Nous allons créer le profil de la personna administrateur Power BI\nRenseignez les informations sur l\u0026rsquo;utilisateur Décochez Créer automatiquement le mot de passe puis saisir le mot de passe de l\u0026rsquo;utilisateur. J\u0026rsquo;utilise le même mot de passe que pour les utilisateurs générés par l\u0026rsquo;environnement (voir article sur la création de l\u0026rsquo;environnement) Ne pas demander à l\u0026rsquo;utilisateur de changer son mot de passe. Appuyez sur Suivant pour continuer. Attention, je ne respecte pas les règles élémentaires de sécurité concernant le mot de passe, car je crée un utilisateur factice utilisé dans le cadre de test. Ne faites pas cela avec vos utilisateurs d\u0026rsquo;entreprise.\nRenseignez le pays de l\u0026rsquo;utilisateur. Affecter une licence à l\u0026rsquo;utilisateur. Appuyez sur Suivant pour continuer. Nous allons maintenant affecter le rôle Administrateur Power BI à notre utilisateur.\nSélectionnez Accès au centre d\u0026rsquo;administration. Recherchez et cochez le rôle Administrateur Power BI. Appuyez sur Suivant pour continuer. Le fait d\u0026rsquo;affecter un rôle à l\u0026rsquo;utilisateur dépend de la personna que l\u0026rsquo;on crée et des droits que l\u0026rsquo;on souhaite lui accorder. Sur les fiches de mes personnas, j\u0026rsquo;indique si l\u0026rsquo;utilisateur à un rôle et si oui lequel.\nVérifiez les informations de votre utilisateur. Appuyez sur Terminer l\u0026rsquo;ajout pour terminer. L\u0026rsquo;écran de confirmation de création apparait.\nAppuyez sur Fermer pour revenir à la liste des utilisateurs. Pour terminer, nous allons mettre à jour la photo de notre utilisateur.\nOuvrez la fiche de l\u0026rsquo;utilisateur en cliquant dessus Si la liste n\u0026rsquo;est pas à jour appuyer sur le bouton Actualiser. Si le bouton Actualiser n\u0026rsquo;apparait pas utilisez les 3 points pour le faire apparaitre. Pour trouver rapidement, utilisez le filtre. Appuyer sur Modifier la photo pour utiliser la photo de l\u0026rsquo;utilisateur. Vous avez maintenant un utilisateur utilisable pour vos tests.\nVous pouvez maintenant créer un profil dans votre navigateur pour ce nouvel utilisateur.\nRépéter l\u0026rsquo;opération avec les différentes personnas.\nMerci de votre attention.\n","date":"2022-08-19T00:00:00Z","image":"https://blog.ddata.fr/p/creer-des-personas/hello-i-m-nik-v8pL84kvTTc-unsplash_huc0441eb0c3b34a2bc25f8a0410fc39d0_897394_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/creer-des-personas/","title":"Créer des personas."},{"content":"Vous souhaitez désactiver la demande une double authentification de votre environnement de test ?\nJe vais vous guider pas à pas pour réaliser cela.\nContext Lorsque l\u0026rsquo;on travaille sur un environnement de test, on ne souhaite pas forcément avoir un niveau de sécurité de haut niveau. Par défaut le plan Microsoft 365 developer program active la double authentification (MFA) pour tout le monde.\nVous voyez donc régulièrement cet écran :\nSi vous souhaitez l\u0026rsquo;enlever, suivez ce guide.\nBien entendu il ne faut pas baisser la sécurité d\u0026rsquo;un environnement professionnel.\nDésactivation de la MFA La MFA est activée par défaut, car votre Azure Active Directory est paramétré avec les options de sécurité par défaut. Pour résoudre notre problème, il suffit de les désactiver.\nPour cela nous allons aller sur le centre d\u0026rsquo;administration de l\u0026rsquo;Azure Active Directory avec un utilisateur administrateur global.\nSélectionnez Azure Active Directory pour débuter. Sélectionnez Propriétés dans le panneau de gauche. Cliquez sur le lien Gérer les paramètres de sécurité par défaut pour ouvrir le panneau de gauche. Passer l\u0026rsquo;option Activer les paramètres de sécurité par défaut sur Non Donner une raison. Appuyez sur Enregistrer pour sauvegarder votre modification. Voilà vous n\u0026rsquo;aurez plus d\u0026rsquo;obligation de mettre de la MFA sur vos comptes de test.\nMerci de votre attention.\n","date":"2022-08-15T00:00:00Z","image":"https://blog.ddata.fr/p/desactiver-la-mfa/brooke-lark-fFQ19Vz_Mu8-unsplash_hu9d37c82fe306bfe37bc1c0d5c413c58d_549463_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/desactiver-la-mfa/","title":"💡 Désactiver la double authentification par défaut. 🔒"},{"content":"Dans cet article nous allons créer gratuitement votre propre environnement Microsoft 365.\nContext Je pense que la meilleure manière d\u0026rsquo;apprendre passe par l\u0026rsquo;expérience, pour apprendre à utiliser un outil comme Power BI Service il faut pouvoir le manipuler.\nDans un cadre professionnel, vous ne pouvez pas « jouer » dans votre environnement de production.\nDans un cadre non professionnel, scolaire, autoapprentissage ou autre, vous n’avez pas les moyens financiers pour vous offrir un environnement Power BI Service.\nMicrosoft propose une solution : le programme Microsoft 365 Developer. Ce programme vous permet d’avoir en environnement complet et réel pour faire des tests et apprendre. En aucun cas ce programme ne doit être utilisé en production.\nLe programme se compose de :\nUn tenant Azure Active Directory 25 licences E5 Objectif Je vous propose dans ce premier article de vous accompagner dans la création d\u0026rsquo;un compte dans ce programme.\nCela nous servira de bac à sable pour utiliser l\u0026rsquo;éco système M365.\nPour mes prochains articles utilisant Power BI Service j\u0026rsquo;utiliserais cet environnement.\nC\u0026rsquo;est parti ! Pour commencer utiliser votre navigateur pour vous rendre sur le site du programme Microsoft 365 developer program ou rechercher « Microsoft 365 developer program » dans votre moteur de recherche.\nVous arrivez sur la page d’accueil :\nCliquer sur Sign in pour vous connecter avec un compte Microsoft Vous connectez avec un compte Microsoft existant. Si vous n’en avez pas de compte Microsoft ou si vous voulez utiliser un compte dédié pour la création de cet environnement cliquez sur Create one! et créez un compte Microsoft. Dans les 2 cas, le compte utilisé sera le propriétaire du programme « Microsoft 365 Developer » créé.\nVérifiez que vous êtes connecté avec votre compte Microsoft. Appuyez sur Join now pour commencer la procédure de création. Vérifiez que c\u0026rsquo;est bien votre compte qui est utilisé pour la création. Le site vous demande d’enregistrer quelques informations. Votre pays Votre compagnie Votre langage par défaut Comme d’habitude, lisez bien les conditions générales du site Appuyez sur le bouton Next pour continuer la procédure d’enregistrement. Renseignez le questionnaire Appuyez sur le bouton Next pour continuer la procédure d’enregistrement. Renseignez le questionnaire Appuyez sur le bouton Save pour continuer la procédure d’enregistrement. À cette étape nous pouvons choisir entre 2 types de bacs à sable, nous allons choisir Instant sandbox, ainsi tout un ensemble d’éléments fictifs sera créé automatiquement (Utilisateurs, Email, Teams et Sharepoint) Appuyez sur le bouton Next pour continuer la procédure d’enregistrement. L’étape suivante est très importante pour la suite, nous allons créer ce que l’on appelle un tenant dans un datacenter de Microsoft.\nLors de cette création, nous allons indiquer le nom de l’administrateur de ce tenant. Cet administrateur aura TOUS les droits sur le tenant.\nLe tenant sera composé de :\nUn Azure Active Directory : C’est l’annuaire d’entreprise, il contient notamment les utilisateurs et les groupes. Un environnement Microsoft 365 qui contiendra notamment les boites aux lettres utilisateurs, les sites SharePoint et les équipes Teams. Choisissez le datacenter dans lequel sera votre tenant bac à sable, choisissez la zone géographique la plus proche de vous. Choisir un nom pour l’administrateur du tenant. Choisir et confirmer un mot de passe pour l’administrateur du tenant. Choisir de mettre un mot de passe différent de celui de l’administrateur pour les 16 utilisateurs fictifs Choisir et confirmer un mot de passe pour les 16 utilisateurs fictifs (ils auront tous le même) Appuyez sur le bouton « Continue » pour continuer la procédure d’enregistrement. Afin de valider la procédure d’enregistrement, vous allez devoir indiquer un numéro de téléphone valide pour recevoir un code de confirmation par SMS.\nChoisissez votre pays Indiquez votre numéro de téléphone (sans le premier zéro pour la France) Appuyez sur « Send Code » pour recevoir le code par SMS Une fois le code reçu entrez le dans la zone appropriée et valider cet écran. Bravo vous voilà propriétaire d’un tenant Microsoft 365 complet, l’écran suivant vous redonne la synthèse des informations : Le nom de domaine de votre tenant assigné automatiquement par Microsoft. C’est toujours un nom de domaine sous la forme xyz.onmicrosoft.com. C’est votre nom de domaine par défaut. Un rappel de l’email de connexion de l’administrateur. Le nombre de jours restant pour l’évaluation. Cette période est renouvelée automatiquement à la fin de 90 jours. Nous en avons terminé avec la création de votre bac à sable. Retrouvez-moi dans de prochains articles pour utiliser votre environnement.\nPour plus d\u0026rsquo;information sur le programme Microsoft 365 Developer, retrouvez la FAQ ici.\nMerci de votre attention.\n","date":"2022-08-12T00:00:00Z","image":"https://blog.ddata.fr/p/m365-developer/vadim-babenko-RzVaeI5uLX0-unsplash_hu4c0ac1a5fc3bc62d5287596e79649454_708326_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/m365-developer/","title":"Créer un environnement d'apprentissage Microsoft 365"},{"content":"Cékoidonc Azure ?\nLorsque vous affichez cette page de blog, cela n\u0026rsquo;est ni magique ni divin. Vous faites appel à des services informatiques qui répondent à votre demande.\nCes services peuvent être de nature différente, mais ont tous en commun d\u0026rsquo;être disponible sur Internet.\nIl existe des fournisseurs qui vous louent des services informatiques sur Internet, c\u0026rsquo;est ce que l\u0026rsquo;on appelle le cloud.\nParmi ces fournisseurs on retrouve :\nAmazon avec le service AWS (Amazon Web Services). Microsoft avec le service Azure. Google avec le service GCP (Google Cloud Platform). OVH avec le service OCHcloud. Azure est donc un service de location de ressources informatique sur Internet délivré par Microsoft.\nAzure propose plus de 200 types de services différents à la location.\n","date":"2022-08-11T00:00:00Z","image":"https://blog.ddata.fr/p/c%C3%A9koidonc-azure/adrien-olichon-KzkIVT1HCVY-unsplash_hueac9fc876ba4dc6d3988d92735221d11_1415592_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/c%C3%A9koidonc-azure/","title":"Cékoidonc Azure ?"},{"content":"Cékoidonc Azure Active Directory ?\nLorsque vous souscrivez à des services en ligne, vous ne souhaitez que n\u0026rsquo;importe qui accède à vos ressources, on parle dans ce cas d\u0026rsquo;authentification des utilisateurs.\nParmi vos utilisateurs authentifiés, vous ne voulez pas non plus que tout le monde accède à tout, on parle dans ce cas d\u0026rsquo;autorisations.\nAzure Active Directory est le service Azure en charge de l\u0026rsquo;authentification.\nLa gestion des autorisations des services Azure, sera réalisé sur les utilisateurs authentifiés dans Azure Active Directory.\nOn utilise souvent les acronymes Azure AD ou AAD pour Azure Active Directory.\nEn plus des utilisateurs, AAD permet de gérer des groupes, dans lesquels on ajoute des utilisateurs. Un utilisateur peut être membre de plusieurs groupes.\nLa bonne pratique est d\u0026rsquo;affecter les droits à des groupes et non à des utilisateurs individuellement, ainsi c\u0026rsquo;est plus simple d\u0026rsquo;affecter des droits à un nouvel utilisateur en le rendant simplement membre des groupes correspondants au besoin.\n","date":"2022-08-11T00:00:00Z","image":"https://blog.ddata.fr/p/c%C3%A9koidonc-azure-active-directory/jukebox-print-FUohNQatzVs-unsplash_hu35525c2077e7c31a5937113b838474e5_621905_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/c%C3%A9koidonc-azure-active-directory/","title":"Cékoidonc Azure Active Directory ?"},{"content":"Cékoidonc Microsoft 365 ?\nMicrosoft 365 est un ensemble de service,orienté bureautique, destinée aux entreprises.\nEn plus des classiques de la suite Office (Word, Excel, PowerPoint) on retrouve notamment :\nTeams : l\u0026rsquo;outil de chat et de réunion (écrit, vidéo et audio) Outlook : votre messagerie email. SharePoint : l\u0026rsquo;outil de site intranet et de partage de documents collaboratif. OneDrive : vos documents personnels d\u0026rsquo;entreprise dans le cloud. Power BI : Voir mon cékoidonc ? Power Apps : Voir mon cékoidonc ? Power Automate : Voir mon cékoidonc ? Il existe un grand nombre de licences Microsoft 365 qui intègre plus ou moins de produits de la gamme.\n","date":"2022-08-11T00:00:00Z","image":"https://blog.ddata.fr/p/c%C3%A9koidonc-microsoft-365/joyce-mccown-1Q4TjXPlrVQ-unsplash_hu6f8bd3335d5e0a54aad627c035558e90_1054235_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/c%C3%A9koidonc-microsoft-365/","title":"Cékoidonc Microsoft 365 ?"},{"content":"Cékoidonc Power Apps ?\nPower Apps est un outil disponible au travers de votre navigateur Internet (outil cloud), il vous permet de créer des applications exécutables dans votre navigateur Internet ou sur votre mobile (Android / IPhone).\nLes applications créées sont généralement des applications de collecte de données (texte, nombre, photos, coordonnées GPS).\nL\u0026rsquo;objectif de cet outil est de pouvoir être utilisé par des utilisateurs non technique (citizen developer).\n","date":"2022-08-11T00:00:00Z","image":"https://blog.ddata.fr/p/c%C3%A9koidonc-power-apps/tamanna-rumee-aRlwXNPvLIk-unsplash_hu899d7f1bc2d9696171f2a0270434d38c_7646503_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/c%C3%A9koidonc-power-apps/","title":"Cékoidonc Power Apps ?"},{"content":"Cékoidonc Power Automate ?\nPower Automate est un outil disponible au travers de votre navigateur Internet (outil cloud), il vous permet de créer des traitements qui interagisse entre les différentes briques de Microsoft 365, mais aussi avec des briques extérieures à cet éco système.\nPar exemple, vous pouvez réaliser un traitement automatique qui a chaque réception d\u0026rsquo;un mail avec une pièce jointe, copie cette pièce jointe dans un dossier, créer à la volée, de votre OneDrive.\nL\u0026rsquo;objectif de cet outil est de pouvoir être utilisé par des utilisateurs non technique (citizen developer).\n","date":"2022-08-11T00:00:00Z","image":"https://blog.ddata.fr/p/c%C3%A9koidonc-power-automate/alex-padurariu-ZR48YvUpk04-unsplash_huc237122beeea88f66e1fccb341b7a4d2_2647466_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/c%C3%A9koidonc-power-automate/","title":"Cékoidonc Power Automate ?"},{"content":"Cékoidonc Power BI ?\nPower BI est un ensemble d\u0026rsquo;outils :\nPower BI desktop, disponible sur votre poste de travail, est un outil permettant de réaliser un projet BI. Toutes les étapes d\u0026rsquo;un projet BI sont couvertes : collecte et transformation des données, modélisation et création d\u0026rsquo;indicateurs, et enfin réalisation de vos rapports. L\u0026rsquo;objectif est de rendre aux utilisateurs le contrôle de leurs données. Power BI Service disponible au travers de votre navigateur Internet (outil cloud), il permet la diffusion de vos rapports et tableau de bord. L\u0026rsquo;objectif de cet outil est de pouvoir être utilisé par des utilisateurs non technique (citizen developer).\n","date":"2022-08-11T00:00:00Z","image":"https://blog.ddata.fr/p/c%C3%A9koidonc-power-bi/cedric-stoecklin-3MoMDBHWhnU-unsplash_hu1ca34946bd12f0e64e734615c45e379d_1750064_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.ddata.fr/p/c%C3%A9koidonc-power-bi/","title":"Cékoidonc Power BI ?"}]